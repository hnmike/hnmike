---
title: "DataExpert.io - Week 4 Flink and Kafka - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2024-12-30
source: "https://www.youtube.com/watch?v=KokIFO_k02I&t=4770s"
image: "https://i.ytimg.com/vi/KokIFO_k02I/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgZShlMA8=&rs=AOn4CLBipUQFH1aIcScI5eAUqhD69O1Q3g"
created: 2025-03-23
tags:
  - "youtube"
  - "flink_and_kafka"
  - "data_engineering_bootcamp"
  - "streaming_data_pipelines"
summary: "Week 4 of the Data Engineering Bootcamp covers Flink and Kafka. Explore real-time data processing, streaming pipelines, and key concepts for building efficient data workflows."
---
# DataExpert.io - Week 4 Flink and Kafka - Data Engineering Bootcamp

![DataExpert.io - Week 4 Flink and Kafka - Data Engineering Bootcamp](https://www.youtube.com/embed/KokIFO_k02I&t=4770s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> m there's near real time and then there's micro batch we're going to talk about each one of those and get very clear on the definition of these words because as a data engineer you need to understand the differences between these whereas your stakeholders are not your stakeholders are not going to know the difference like if you if your if your stakeholder says we need the data in real time that might not mean use Flink that might mean something completely different but for the most part what we want to say here is uh streaming am I am I missing something because he's kind of like gone straight into this a I miss like um let me just check his YouTube channel things in data engineering right now in this course we're going to be covering Apache Flink we're going to be covering Apache Kafka we're going to be covering how to use these things to process data in real time you're going to be able to connect to an actual Apache Kafka cluster in the cloud that is attached to the data expert Academy you'll be able to process clickstream data in the labs and you'll be learning a lot about the complex windowing functions that are available in Flink such as count Windows sliding windows and session windows that should be a really fun time as well and uh I really hope you enjoy this many hour course if you want to learn more about how to do this in the cloud check out the data expert Academy in the link in the description below I can get you 20% off \[Music\] well what is a streaming pipeline this is a great question so so we had a little introduction and it's just been cut out of this or like skipped um which that's fun pipelines process data in a low latency way and in this case what I mean by low latency is usually that means that the data is processed on in the order of minutes uh depending on like your windowing function or something like that maybe the order of a couple minutes to half an hour something like that will be kind of your latency for like when the data is generated to when it is processed so that's the low latency way of thinking about it like um another word that I like to use here is intad day because a lot of pipelines in the business world are uh daily pipelines like they actually are um once a day they they kick off at UTC midnight which is like 5:00 p.m. and then they run and then they maybe run for a couple hours they run from like 5 to 7 yeah I think a really good way to think about this is like just say you're looking at YouTube for example and you get like the analytics updated every day so you get like your watch hours updated every day you get your um you get your views like updated every day in your analytic section however you real time you get your views um like updated like quite quickly like on a on a video for example like and and that's the main difference between like streaming and um streaming like pipeline versus a uh versus one that gets updated daily and then that's it and then they're done and so this is different where intra day means that it's going to run more than once a day and that's where this gets complicated and we're going to talk about all the different ways that things can run more than once a day so yeah let's dig into it okay we have a couple words here right we have streaming we have near real time and real time and also microbat which is connected microbatch and mere real time are pretty much exact synonyms so like let's go over this a little bit so streaming which is Flink which is what we're going to do in the lab today is streaming is is also called continuous processing which you can think of as okay every time an event is generated we process it immediately and oh just wanted to um to show you as well like if I go to my analytics on YouTube uh again like this is an example where it's updating things in real time so this is updated in real time then a lot of this is updated like every day once a day so that's that's just something that I found quite interesting like you know when when you looking at stuff like this um you know that that that shows you the different the different differences and how how how these analytics get updated and so if and it's cross like a a river you can think of it as like a river of like literally like a stream or a river like the data just keeps flowing and flowing and flowing and it never stops that is a great way to to think about it um the the best example of a streaming continuous processing engine is Apache Flink which is what we're going to be using in the lab today uh then you have near real time near real time is a little bit different where instead of processing data as it is generated you wait maybe a couple minutes or a minute or two minutes to collect all the data that has happened in that period of time and then you process that data as it comes in and uh the big poster child here is going to be spark structured streaming so spark structured streaming is not yet a continuous processing framework even though data bricks recently announced that they will be committing to making spark structured streaming uh continuous uh continuous processing framework and at that time when when data bricks does announce that it's going to be uh Flink and Spark streaming are going to be essentially the same in that point they're going to kind of converge just like a lot of other Technologies kind of converge these two technologies are going to converge a little bit as well so that's a good thing to remember quick question for anyone who's doing this by the way um I've noticed that when I go to the like data expert like assignment if I go here and I go to my free YouTube um boot camp assignments list um I don't have any any any more assignments so we not having any assignments for week four because uh this is this is the other Book C that I'm doing as well by the way the the one in January so that's why I've got all of those there but are we not have do we not have any homework yet for this this week uh so what that means though is that real time and streaming are sometimes but they're often not synonyms and you want to make sure that you understand the latency requirements of the pipeline and not just take your stakeholders uh words where if your stakeholders is like oh I need it in real time that doesn't mean hey I need to go and bust out Apache Flink and get real technical like I mean it almost never means that actually I think in my career uh when a stakeholder has said real time when they actually meant we need a streaming pipeline that has happened two times two times in my n-year data engineering career when like people have said they need real time at least two dozen times so it's like maybe 10% of the time 10% of the time when a stakeholder says they need real time data they they that means they need a streaming Pipeline and so remember that that like streaming pipelines the actual use cases for them are rare they're like one in 10 of like actual pipelines at least as a kind of a general rule that you can use when navigating relationships with stakeholders and we're going to deter like we're going to talk more about like what what constitutes like a valid use case and what doesn't so yeah don't worry about that yeah let's let's dig a little bit deeper as we kind of like learn more about the I wonder if like things like um when when it comes to like information about stocks and uh information that really is critical for making decisions is if they're like one of the real examples of needing real time because I think I think a lot of cases you don't need that critical that information critically right this second you can have that kind of batch process that feels like it's real time uh oh is a submission not open yet do you know when it is going to be open I mean different verbage I mean I can't really complain like you know it's nice not to have another step what that I need to do just here so another word I said here that's not on the slide is micro batch so you can see like uh near real time data is processed in small batches small batch micro batch near real time there are like all the same that's all the same like very very like those are like exactly the same whereas uh microbatch in real time are not exactly the same they like they're similar but they're different so it's a key thing to remember as you kind of like go through these processes so what does Real Time mean from stakeholders right it it rarely means streaming very rarely um it usually means low latency or predictable refresh rate so I've I've even had stakeholders that say we need this data in real time and what they meant by that is they needed the data up to date like they needed a daily pipeline up to date by 9:00 a.m. every day and it the data actually only needed to refresh once a day so it's funny because as a data engineer your St ER says I need real time but what they actually need is just a batch pipeline they need just a a regular old Plain Jane batch pipeline which hopefully is like a you know an efficient low latency batch pipeline that is regularly refreshed and is reliable and that's a lot of times what stakeholders kind of mistake here is they think uh like what real time means is like reliable and predictable and it's it's like different words and that's where one of the very good things that you can talk about with stakeholders is you can have this thing called the SLA discussion which is the service level agreement discussion which essentially means like you ask them okay like when do you want the data refreshed you can ask them that question and a lot of times they'll be like oh we need it eight hours after midnight or 10 hours after midnight or whatever and then you can make an agreement of like okay our data we can guarantee that the data will be available by then and so this was is um the case in my previous company as well that they they needed a report generated of all the latest transactions by like like I think it was 12:00 p.m. UK time but it but that translated into like 700 a.m. like us time um and as long as we provided it and made the report uh generated by then then it was absolutely fine uh but but that was what they meant by having up to-date predictable data it just needed to be up to dat by that point that conversation will be way better than like just uh not having that conversation and then being like okay we're just going to do everything in streaming and we're going to go the whole other way and just like not talk with people that's why like data engineering is also in some ways more communication than software in is cu you really need to talk with people about like what their actual latency latency requirements are because otherwise like you're going to over complicate things and don't do that don't don't don't build them a Ferrari when a bicycle will work right don't do it don't do it I promise like the tech debt and the pain and the maintenance that you will uh end up creating for yourself is not worth it so yeah completely agree with this I think like there is such a tradeoff when it comes to making uh pipelines more efficient and understanding what what is like the bare minimum that you need to get to make them happy is is is key let's uh let's talk a little bit more there's a perspective of should you use streaming is it like something that uh is worth it is uh like so these these questions here I think are going to be great things that you can ask yourself so I'd say one of the number one things you need to ask is do the members of my team have a streaming skill set I want to give an anecdote here about this one specifically so when I was working at Netflix most of our pipelines uh in the security space were done in batch right batch or micro batch and there was a big Push by the the people in security to be like yo we need lower latency even though we were giving them like data every 15 minutes right we were like that's good enough right we can catch the bad guy if like we might catch him 15 minutes late but like what kind of damage can he do in 15 minutes so I don't know from our perspective as a data engineering team we were like well like do we actually start to implement streaming and change the perspective of it because we have our entire team is very good batch pipelines with a patchy spark and we know those in and out we are like the Rockstar team of batch pipelines so like why would we move to something that's brand new and we're going to have to have the team learn it piece by piece and anyways what happened was the security team was like we have to have it lower latency there's no way around it and so uh one of the members of my team he uh started migrating some of the pipelines from from batch to streaming and one of the things that happened was he became an island where when the streaming jobs broke like he was pretty much the only one who could fix it and then I kind of like started to learn about how to fix it as well that was like where I picked up some Flink skills but then again it was just me and him and we were on a team of 15 and I was always like this is unfair why are we the only ones who have to troubleshoot the streaming pipelines I don't get it and so this is why when you're working on a team you should understand like there should not you like you don't want to be in the worst case scenario where like you build a streaming Pipeline and you're the only one who can troubleshoot it because you're the only one with the skill set because that's terrible it's a terrible horrible no good very bad place to be uh hopefully not too as well because being on call 26 weeks a year is also not fun um so hopefully you have at least uh two like two other people so you have like three people on the team who understand streaming pipeline if you uh that's still going to be a little bit taxing but not as it should still be pretty decent so then you also have to think about like what is the incremental incremental benefit so one of the big things that can happen here is okay we lower latency like in the in this in this Netflix example where we have 15 minutes and maybe we can lower it to like two minutes or three minutes and it's like okay we can stop the bad guy in his tracks a little bit faster we can give him only you know we give him only uh like a couple minutes instead of like 15 minutes that's the main benefit that we would see and uh how are we going to do that and so that could be a win obviously like and and in that case in the security space like you can actually make a case for it whereas like in other cases you can't make the case where it's like okay it's available like the data is available at three so I think the only thing I'm other thing I'm probably going to have to make sure that I understand is like so Kafka um there's very similar Concepts to Kafka in as your vent hubs and so I'm going to have to probably see like what the differences between like what CF calls things versus event hubs because I've worked with a event hubs quite a lot but not not C I've not really worked with CA before 3: a.m. refreshed versus like okay we could have had it available at 1:00 a.m. refreshed and it's like is anyone going to even be querying that data at that time anyways does like like are you going to have some an an analyst this is who's doing an emergency data analytics project at two in the morning like probably not so that's another important case to remember incremental benefit like what is the actual value that we get from reducing the latency important um okay homogeneity of your pipelines uh I use this word in another presentation like there's a couple words that I really like in data engineering and obviously y'all know my my favorite one which is em potent and that's my favorite word I love that word a lot but this is another one that I really like is homogeneity so if you're uh a data engineering team and you I'm surprised his his favorite word isn't data data is it data data data have 99% batch pipelines like okay like why add a streaming pipeline unless there's a the only time that would be the case is like if you have people on the team have the skills and there's a a strong incremental benefit to it then maybe maybe that's the case but generally speaking if you're are if you're a batch team you should stay with batch and I also agree on the flip side like if you are a streaming team you should stick with streaming and that's what Uber does right Uber runs with this thing called Kappa architecture which is a streaming first architecture so for them like it's almost the opposite where they're like okay we have all these streaming pipelines why would we add a batch pipeline that's just going to complicate things and uh red and we're going to have more of a uh you know a heterogenous uh mix mix of pipelines instead of a homogeneous mix of pipelines and so that can just increase the maintenance burden and the on call Overhead like when you are considering whether or not to use streaming and then uh I think this is probably one of the most important points is okay what is the trade-off between these four different options right so you have daily batch hourly batch micro batch and streaming those are going to be your four options of essentially how you can process this data and a lot of times it like I'm going to show on I think the next slide there's a Continuum here of kind of complexity versus latency and the tradeoffs there where again you want to go back to what is the incremental benefit and maybe batch a daily batch is going to be fine and then another important thing to consider is okay where are we going to put data quality because data quality is a lot harder to do in streaming it's a lot harder to do in streaming than it is in batch because in batch you have like these obvious steps right you have a then B then C right and then so you can essentially at any between a and b or between C and D you can put a cut and you can be like okay I'm going to insert quality here and stop the pipeline if there is a problem but with streaming it's like it just runs all the time man it just runs forever there's not a then B then C it's just always on so uh data quality can be a harder thing to do and that uh for streaming so that's another kind of tradeoff it goes in the tradeoff bucket and that's where batch daily batch is going to have the the easier kind of data quality path so hopefully those are some things that you can consider when a state the next time a stakeholder asks you hey we need realtime data these are uh things that you should consider so this is a really random thing and um and I'm just going to show everyone because these are my favorite things ever but um I've got these fizy Sour Apples and my my my partner bought me them for for for for Christmas but they're so good so you should buy some just because um because they're so good uh but yeah no I really really like them like and and and I haven't eaten the entire thing what he's put them in like a jar like of like all these other sour sweets cuz I love sour sweets so I haven't just eaten this entire entire jar since Christmas but yeah they're really good and I I'm I'm probably going to have some soon before using Apache Flink okay so um streaming only use cases so these are some cases where uh I called it I said I said trading I figured trading would be a um streaming only need like absolutely figured that uh there isn't there is a massive trade-off that you can't make uh with like say you use hourly batch or micro batch there's just you can't use those as your uh options because it makes or breaks the use case so the like you have to have extremely low latency for there to be any value at all um uh the number one example I always point to is detecting fraud so like if someone steals my credit card I want to have them not be able to go on a one day spending spree before the fraud is detected the next day I I think that that's kind of a terrible fraud detection system I don't think that that's what they should be doing and uh you should probably be able to to detect fraud in a little bit more of a real time kind of way because that will uh actually make the the product a lot better so yeah I imagine with Live Events especially things like um like sports events where you know you you're viewing them scoring goals and things like that you you want to know when the events going to be there especi this is probably going to make a massive difference for like betting as well for like betting all second imagine yeah um obviously there's other options here like high frequency trading where it's like imagine uh you're like oh my oh man like I I found the perfect setup for a trade yesterday right because the data is on daily batch and it's like okay well like we good for you bro you can't that setup is gone like you can't use it anymore it's like too little too late so you have to be able to process things in a low latency way so you can actually make the trades that you're looking to make another good good example is like sports analytics so if you're like watching a real-time game and like or watching a football game and like doing all sorts of I you ever seen them like draw the circles and like give them give the stats in the game a lot of that's need needs to come from uh real time pipelines because again if it's like oh that play happened even if it's like a near real time and it's micro batch it's like oh here are the stats for the play that happened five minutes ago and people are going to be like what like this is terrible boo so like the key thing that I want you to remember here is there's like for the streaming only use cases there is an obvious signal that we should be using streaming because if we don't use streaming it's terrible it breaks the product it makes it an unusable product here's a good example of where things get a little bit blurry so I would say there's essentially two areas that are kind of gray area use cases for when streaming could be used microbatch could be used etc etc so if you have a master data that is Upstream in your in your data warehouse that is that is like one of the furthest Upstream data sets that is then depended on by a lot of other data sets then streaming could be very powerful because it could make it so that all the downstream data sets can fire sooner because the the daily data will be ready sooner and that could be a massive massive benefit to the pipeline and when things are available and that can cause your whole because one of the other big things that that does is it allows your your Warehouse to be kind of advertised so you can use all of the compute throughout the day as opposed to having some dead compute in the middle of the day and then at midnight there's a big spike which happens a lot by the way in a lot of different warehouses like midnight Midnight UTC to like 2 a.m. UTC is like a spike and then uh it kind of drops down and then there then as people come in to the day and the they come in around 5:00 pm UTC 5:00 pm is when they kind of like come in and then they will and then you get more of those ad hoc queries and it kind of like you get more load as like you have the data analysts and data scientists running their queries that are more at hoc and so if you can actually amortize your compute throughout the day it makes better use of your Cloud resources especially like if you aren't like using like on demand resources and you have like youi renting you're renting out your ec2 instances and you're paying the same no matter what then you want to maximize the compute for each one of those ec2 instances and that can be a very powerful way to do it and anyways Master data latency important so I want to talk about a use case that happened for me uh where I failed at implementing streaming because it was freaking really hard dude it was like so hard and so I was working at notifications and I needed to work on this notifications event fact data so this data set was essentially every row was a notific event you can think of like a notification sent delivered generated you click on it all those different events in the event stream but the thing is is like they can be duplicated because you can click on a notification twice so one of the things that my boss was saying he's like Zack this is a perfect perfect use case for streaming and then I'm like okay and then I like pump it into streaming and no matter what I did it just o out of memory out of memory out of memory like even if I maxed out the memory setting still out of memory and the reason for that is because like the duplicate can happen at any point throughout the day so even if I'm you know it's it's it's it's morning and then I I click on it in the morning and then I click on it at 11 o'clock at night those duplicates still need to be managed and so that means that you have to hold on to every notification ID for the entire day which means that you have to hold on to everything in RAM for like the whole day which means that and that notification data set was like 20 terabytes so it's like okay we just need good old 20 terabytes of RAM and we can just go ahead and do this right and obviously that wasn't in the cards that was just like a lot of ram because if you think about it from the perspective of spark right the what one exeutive needs 16 16 gigs of RAM that's the most that you can give it so imagine 40 terab of ram it's just you can't do it so instead like for me I in that case I tried streaming didn't work and instead I moved everything to micro batch and that worked great uh I think I shared that repo with y'all about how you can kind of do micro batch in like a tree fashion to do dding and there was great and that made it so that instead of Landing uh nine hours so it used to land at 9:00 A.M UTC and now and then after I migrated it it landed at 1: a.m. UTC which allowed all the downstream pipelines to pick up and run a lot sooner and that gave all like made all the notification data sets way more up to-date and uh analysts love me for it because then they didn't have to like be like oh wow we have to wait for notifications to update because that nine hours after um midnight meant that like some days it was going to be it's going to bleed into the afternoon of the next day so my whole point here is microbat can work streaming can work in these cases and the last point I have on the slide is like if you're serving data to customers it should probably be pretty up to date because customers have a high expectation on latency they expect your data that you're serving them to be as up to date as possible because if you serve them like data from a week ago they're like what like why are you doing this so you can get some good things with that I I found that like when you're serving data to customers one of the things that you want to be careful about is you have that trade-off right where it's harder to implement data quality so a lot of times like streaming still might not be a good fit and microbatch still could be the better fit because microbatch is easier to implement like full data set data quality and and even for customers if it's an hour behind depending on the use case as long as it's not one of those streaming only use cases I was talking about in the last slide they might be okay with it being an hour behind and uh so that can be good just many many days behind is when customers are going to get upset so there are some gray areas here where streaming could be used and streaming might not be used uh I've generally found most of these gray areas are going to be solved with microbatch but I have seen some of these gray areas be implemented with streaming as well a big one here is if the analyst is just upset that the data isn't up to date and uh and those cases where he's quering it early in the morning or something like that like just let it be let it be like doesn't matter like uh like because what are they going to do with it anyways because that's the big thing to remember about like if an analyst is complaining that the data is not up to date like you need to ask yourself okay if it was up to date what would change in the business and most of the time nothing and so it's probably a waste of your time to reduce the latency and so in those cases because you think about like the the velocity that business decisions are made at most business decisions aren't made within hours they're not like oh man like like we're looking at this data and we need to immediately act we need to immediately solve this problem that never happens like business does not move that quickly like ever and so that like that's a big thing to remember when your analytics partners are like complaining about latency is that like so what man so what if it's data from yesterday like so what like it can be data from yesterday and that's fine it's most of the time it's fine that's just something that you find annoying and not actually impacting the business that much and that you get the same results anyways even if you're making decisions based on data that's one day late reminds me of like being just genuinely been been a developer and like really wanting to refractor something because you don't like the look of it but you know that you don't need to refractor it because there's no business reason to refractor it because it's unlikely to be changed and is it reminds me a bit of that basically um so you got to think about it in those cases it's like okay even if the data was immediately available and they could watch it update in real time how would that impact their decisions I mean in some cases I I got I got some news for you you can actually potentially even be uh a net uh you know negative to having real- time data the most upto-date data because like then people like might be just stuck on the screen and mesmerized by the fact that the numbers keep updating and they're like wow this is so cool like sometimes like we don't want we as humans don't need to know that we don't need to know how the data is coming in like and we can just have data once a day have you ever seen those people who are like addicted to the stock market when they're like looking on on Robin Hood or something like that and they're watching the stock change the value change and they're like wow this is so cool but then they get so myopic on like the minute to minute changes of the data that they waste a bunch of time and it's like it doesn't matter it's just a minute to- minute fluctuation and maybe just looking at like the daily open and close of the stock market is probably going to give you maybe even you might even make better decisions based off of that because the the minute to- minute fluctuations are just noise and it causes you to kind of overly focus on noise over uh signal and so if that's happening like like you want to be careful be real care real careful when you're working with that kind of stuff so key thing here analytics Partners business move slowly most of the time if they're complaining the DAT is not up to date just tell them what would be different if it was up to date and the answer most of the time is nothing so and it's beautiful that's like one of those things that should be very relieving as a data engineer is that like if like if business was moving quicker right and then they were like yeah we got to solve our problems faster and we have to have everything in real time I think being a data engineer would be even more stressful than it already is and so that's a good thing so impatient analytic stakeholders can definitely be annoying but that doesn't mean you should you should use Flink let's talk a littleit about how streaming and batch pipelines are different because they're very very different they're like almost the opposite so a streaming pipeline runs all the time it runs every second of every day all the time batch pipelines run for a small percentage of the day they usually run even the beefiest of pipelines will run like four hours four out of 24 so one sixth of the day and that's going to be those are those Bera big giant Behemoth whale pipelines a lot of Y y'all pipelines if you aren't in big Tech they run for like five minutes so like five minutes out of a day is like not even 1% whereas your streaming pipeline will run all the time so one of the things that's kind of bad about that though is did y'all know that uh um Men actually pay more uh for insurance on their car than women do and one of the reasons for that one of the biggest reasons for that is men drive cars more and it's like if you drive enough miles in your car you will crash your car guaranteed 100% guaranteed if you drive like you're statistically guaranteed to crash your car if you drive enough I mean that's that's true but I'm pretty sure that also statistics that suggest that like men are more likely to buy um sports cars and they're more likely to do racing so I don't think it's just the fact that more men drive more cars even if you're the safest driver in the world because you're going to get ran into because there's just all that Randomness that can happen so my whole point there is you have a streaming pipeline that's running all the time the probability that it breaks is higher than a batch pipeline that's running for five minutes a day because the rest of the time the batch pipel is just vibing and it's just like sitting on the couch just waiting waiting to to run for the next day so in that way like streaming pipelines require a completely different skill set they require because they are they they act streaming pipelines act like a web server almost they act like a a server that is like serving data and moving data and is acting like it's very software engineering very software engineering and I think that's one of those things that uh is why another reason why streaming isn't as implemented because it's not better it's not better in some regards as well if you have something that's running all the time it's not better it's definitely not better so um it's that's where you can get a lot more failures and like it's different so you also for streaming uh it's kind of like spark in this way but you need unit test and integration tests for your streaming jobs even more than for your spark jobs because for two reasons one is that it acts more like a server it runs all the time so you're going to need to have like you need to have that software engineering mindset when working with streaming pipelines but also it's harder to implement data quality like in the data quality checks which the batch pipelines in spark they get that they get the batch pip they get the the quality checks on the batch pipelines so and so you get that second fail safe with batch pipeline so it's like even if you're like wow even if I push a bad build or I push a an incorrect query and Spark at least like uh the data quality checks and Great Expectations is going to save my ass but like you don't get that luxury in streaming so you need to have more unit test and integration test than streaming otherwise you're going to have a you're gonna have a hard time so remember that like that streaming is it's a lot like there's a lot of things that go on it's a lot more complex oh yeah here's that here's that Continuum I was talking about in the earlier slide so you essentially have a a Continuum here from not complex to very complex and also from high latency to low latency so you see on the left side here we have SIMPLE High latency daily batch and then on the right side we have like Flink low latency and high engineering complexity so remember that like when you're working with latency stuff that there's a Continuum here where the lower the latency the solution the higher the complexity of the engineering it's not like Flink and Spark are equal in terms of complexity because they're not and so like as you reduce the latency of your solution the complexity goes up so that's a good thing to remember as like you are thinking about oh should I use bat should I use streaming is send people this send people this screenshot real time is actually a myth because even even with link even with Flink it's a myth because of the fact that there is Network time so a lot of times how it works like for example for the lab today I have this thing called an Interceptor that what it does is every time my website gets a web request it intercepts it and logs it to Kafka and then the average latency for that step right is about 150 milliseconds and then from there kofka has it and then Flink needs to pick it up from there and then Flink picking it up from kka so there are a few people watching and I did ask a question but how did everyone find the break did anyone have did it does anyone not celebrate Christmas does anyone celebrate anything different did you spend the last week with family even if you say didn't celebrate but then like had colleagues who who wen't in the office how how has your last week been the latency there is anything from like uh 100 milliseconds to like a couple seconds and then Flink then needs to also write that out to a sync and that really depends because Flink can use windowing and if there's windowing involved the latency could be multiple minutes but if it's just like uh an enrichment where there's you're just uh like write reading and writing rows and maybe adding columns which is what we're doing in the first lab here by the way the um if it's an enrichment then we need to uh like the the sync can be written out too almost immediately but then there's going to be the latency on like post dress right or a latency on uh riding out to another kofka Cube so what I'm trying to say here is that this step from event generation to kofka to Flink to sync like I remember when uh um I was working at Netflix and we were trying to detect security anomalies and uh they have this thing called a red team testing at Netflix which is where they have people try to hack Netflix and then they inject bad data or messy data into the environments and try to hack stuff and uh they essentially did a red team test and we had all of our streaming not necessarily just Netflix that does this I'm pretty sure this is like a standard like thing in cyber security where you have like a blue team which is your defense team and your red team which is like your hacking team pipelines ready to go and for the red team test to go from like hack compromise to cough to Flink to sink to uh alert that process which should be you know our promise was hey real time so when a compromise happens we will immediately know about it that wasn't the case it wasn't the case at all it still took a couple minutes so it's like I don't know like obviously there's going to be time in between just because you have all the other technical steps that can happen in between when something actually happened and when something is processed and discovered and generated so there's going to be all those different pieces right and so we already talked about the four categories here of daily bad hourly B micro batch and continuous processing so I don't need to really talk about that as much but like the whole idea here is real time isn't it's not like immediate real time A lot of times people think of there's a synonym of real time means instantaneous and no because technical things take time oh that's awesome how did the um J into go were they good and like if we could get there I mean I think at smaller scale you can actually get things to be close like you can get things to happen and you can get uh alerting and all that stuff on the order of um you know a single digit seconds but as you scale up and everything things can take a little bit longer because it's like you have to sift through stuff and like there's more data to look at and like there there's more resource contention and there's like all these other things that can happen that make things a little bit slower that make the the real time part of it take a little bit longer so there's a Continuum here that's the whole point here and uh just remember real time does not mean instantly let's talk about the structure talk about the structure of streaming pipelines um there's a couple things here that we want to talk about um so you have the sources the poster child of streaming sources is Kafka kofka is um been around for a long time like I even used kofka like way back in the day like back when like Obama was President I used kofka back in like 2014 and so kofka has been around for a long time same with rabbit so kofka and rabbit are like the two big competitors uh Rabbit doesn't have as much throughput so like it doesn't scale quite as well as kofka does uh but rabbit has more complex routing mechanisms so like for example rabbit can be like a message broker where you can do like a pub sub a lot easier with rabbit than you can with kofka whereas kofka is more like a fire hose in One Direction and it's like more of like a big old river that like you can't really like divert it's just like here's my stream of events now process them and but in because of that architecture it's really fast and you can handle a lot of data lot of did you enjoying last week and spent Christmas with her family and watch squid games with the wife I am watching squid games at the moment too I've only watched like the last the first two epes I think maybe the first three um but yeah don't don't spoil it for me it's been really good hopefully you um found Christmas good have you been doing the DAT engineering course as well and yasine that's awesome uh accepted as a data engineer two days ago super happy that's really really cool hopefully like um hopefully like you enjoy where where is that you're going to go uh when do you when do you end up starting so you have the sources you can have another source called a a dimensional Source like maybe what you're trying to do here is you have your events coming in and you want to have that event bring in some dimensional sources like maybe you want to do some of that like denormalization of your fact data where you want to bring in some different columns from uh like your STD table or whatever type of table you want to bring in and these are called side inputs so if you want to learn more about this like I would Google Flink side in inputs and that's going to be a way to look at how you can kind of marry these two worlds so you can have a side input of data that's coming from uh like a a regular data at rest Source whether whether that be like Iceberg or postgress or any Source that's considered like a table like a data table instead of a data queue where Flink and kofka and rabbit are like cues and and they're another word for that is like data in motion so you can have dimensional sources that enrich your event data that can be very powerful as well and then these things will refresh on a Cadence so like what can happen is you can set up your Flint job that will have like a dimensional data that that enriches the events and then that dimensional data will Refresh on a Cadence maybe every three hours or six hours you get to pick how frequently that dimensional data refreshes so that you get the right dimension for that event and obviously like if your dimensions are changing a lot then that Cadence needs to refresh very often but you don't want to have it refresh too much because then like you're going to like waste all this quer and compute for data that didn't even really change but those are the that is so cool well congratulations do you think the um the course was Zack he at all or do you feel like by that point you already had a good um a good breth of knowledge and dat engineering two big Source inputs for your streaming pipeline uh and that's where your data comes from um these are you think of this as like the spark of a streaming Pipeline and it's how we actually make sense of our pipelines and like what's going on with them uh your two options here are going to be mostly Flink and spar streaming there's other options out there as well but these are the two state-ofthe-art ones and um these are the ones that actually do like the sequel or the crunching of the data and you can learn a lot about like how this works with these things Flink is very powerful and like how it manages like Windows and uh sessionization and watermarking and out of order events there's a lot of things that Flink does well that we're going to talk about here in the next couple slides but keep in mind so you have the source then you have the compute then you have the destination which is where is the data going so you process your data in real time like you your data lands in real time in kfka Flink picks it up in real time and then it dumps it somewhere and generally speaking there's three choices you either dump it to another C kofka topic or you dump it to something like the data Lake in this case like iceberg is probably a good example I want to give a little aide about iceberg real quick so iceberg is one of the reasons why it was created was to solve this streaming problem where before you had Hive the hive metast and if you wanted to add data to The Hive metast store you had to overwrite the old partition so you had to like essentially take all of the data and overwrite it again all the data that was already there so there was no way to like add new data to a partition that already existed that just didn't work Hive didn't let you do that and Iceberg was like okay we we want to let people do that I think that that's a very important thing to do is let people append new data to partitions as opposed to having to overwrite because Hive metastore was very batch oriented it was a very big batch oriented uh meta store whereas iceberg was like we need to be more streaming friendly and so that's why Netflix moved to Iceberg is so that they could have data tables that could be populated by streaming and then easily queried by batch pipelines as well so you get both if you use Iceberg which is so cool that's like like of everything that's like the number one reason to switch to Iceberg is like you have streaming events coming in and you want to append and then obviously you have postgress as another sync that can be another very powerful way to uh set up um a sync talk about streaming ch this stuff is always really interesting so one of the things remember I was talking about how there's latency between when uh data is generated and when it lands in kofka so that is totally true and one of the things that can happen when that when data is generated when it lands in kfka is because there's latency you can actually have data that was generated before other data land after it so you have like older data uh that is or you're going to have like newer data that is ahead of older data in the data set and so the data is not in the right order and that can cause a lot of problems that's a big issue for streaming pipelines that you want to worry about is how do you manage out of order events especially like in cases of like if you're looking at a funnel analysis or you're looking at uh all sorts of different uh event streams that like can have out of order events is something that I think would like can can be a challenge for streaming and we're going to talk more about like what what is done to kind of minimize the impact of that then you also have late arriving data so late arriving data generally speaking in the batch world doesn't have as big of an impact and the main reason it doesn't have as big of an impact is because of the fact that you only have to worry about it at midnight like maybe like two minutes after midnight and like a lot of times like the the batch job doesn't even fire until 10 minutes after midnight or even 15 20 minutes after midnight and it's like okay then you have the entire set for batch like you don't have to really worry about late arriving data unless it's like extremely extremely extremely late and in those cases like most of the time people are like well that data is gone forever and we're just going to not process it and so uh you want to think about late arriving data Flink has some interesting ways to manage that as well and another big one is recovering from failures so one of the things that's tricky about streaming pipelines is it's a little bit different from batch pipelines in that when you uh when it fails you need to like reset it and then have it run again like and the longer you wait to fix a streaming pipeline failure the more and more data Gets behind it and it gets like backed up more and more and more and more whereas whereas bath pipelines that's not the case right you just have your one batch of data and that's it so there's not as much pressure to necessarily recover from failures when you are in the batch World versus the streaming World there are ways to fix this problem in some regards in the streaming environment as well and we'll talk a little bit more about those as well but these are the three big things that you want to be more aware of when you're working in the streaming environment where like most of these things like in the batch environment like they just don't exist or they don't matter so like these are different problems that you w't need to be aware of so let's talk about uh out of order events so Flink can deal with out of order event so I'm getting I'm getting the impression that like a lot of this is very similar to um in in a you have like the different options or you have a event hubs which allows you to just ingest data quickly uh like in its raw form so like you could like ingest it and just throw it into a data Lake and then you've got um like Azure stream analytics which allows you to do the transformation in real time as well uh so it feels very similar to that but it's just like the Apache version of it for example I've been learning dat engineering two years now and I graduate September 2024 and then I started the boot camp it helped me for the technical interviews a lot oh that is so cool I'm so glad uh and yeah congratulations I really hope that you enjoy your your new world it's going to be exciting so how it works is when you have an event stream in Flink you can specify this thing called a watermark and a watermark says okay all of the events after there's no events that are newer uh than than the watermark and and essentially like once that Watermark is hit we can guarantee that all the events inside that Watermark are going to be older than the events after the watermark and so how the watermark works is it looks at uh the event time of the events coming in and usually there's some buffer so like maybe it's like a 15-second buffer so like you have an event time and then you you say okay everything within the next 15 seconds could be out of order but something 16 seconds away is not out of order we can we're essentially guaranteeing that all of the the orderedness is going to happen in the next 15 seconds and so that's how uh water marking works is you give yourself a window for each event and then you have like the next end number of seconds that is considered okay there's a possibility in this um uh in this window that things could be out of order and then Flink will fix the ordering in that in that window automatically for you which is cool that's one of the things I like about Flink it does some really cool things for you so that's water marking so uh we're going to cover watermarking a little bit in today's lab as well it's one of the pieces of the puzzle here and hopefully y'all will find that interesting okay let's talk about uh recovering from failures so Flink manages there's so many different ways that you can manage recovering from failures so uh the big one is going to be uh actually checkpoints checkpoints is going to be the main way and you can tell Flink to checkpoint every um n number of seconds and it essentially the state of the job at that moment in time and then it knows like where to read from and where to write to if things fail so that when you when you start it back up again it doesn't like read everything again and that kind of leads it to a segue around what are called offsets so kka has this this notion of what's called an offset and essentially when Flink starts up you have to tell it whether to do earliest offset latest offset or a specific Moment In Time those are the three choices that you have or it can pick up from a checkpoint or a save point those are all those things that are kind of different ways that uh this stuff can can work out so earliest offset means read in everything that's in cka read in all the way back as far back as we can go and so that's a lot of data latest offset means only read in new data after the job starts so that it will only read in any new incoming data and then you have a specific time stamp which is like okay only read offsets that are at this time stamp or newer and then okay one of the things I want talk about here real quick is the difference between save points and checkpoints so checkpoints are they're internal to Flink and they work uh kind of in a way that Flink can manage things like kind of internally and it's kind of the the data that it recovers from is like this internal Flink binary whereas safe points are actually um more agnostic they're they are more like you could think of it as like okay it's going to be more like a CSV file of like okay we processed this data and we got this far because sometimes if Flink fails we want other systems to be aware of like hey this this failed and this is where we got to and so so save points are more used for like other systems than for Flink whereas checkpoints are going to be used for Flink itself so those are kind of like the main ways that Flink recovers from failures we're going to talk in this case we're going to talk more about offsets and checkpoints in the lab today okay late arriving data there's uh this is another thing that Flink can do uh which is where okay how late is too late and you do have to pick the time there there's actually a time that you have to pick around like okay uh is it freaking 5 minutes that's too late 10 minutes that's too late like how far away is it that things need to be before um how late is too late and keeping in mind that there are two different uh we're going to talk more about this on Thursday but late arriving data and watermarking are related in some regards right because you could think about waterm marking as being like okay there's 15 seconds and it can be out of order and it's like in some regards out of order also is kind of like late arriving because it it it came in in the wrong time because other data was able to sneak in before it so watermarket and late arriving data are similar Concepts but watermarking is more for like the 99% of data and then the late arriving data is for the very the long taale of like the small amount of data that might come in like exceptionally late whereas watermarking is more for the the data that arrives out of order in the window so yeah late arriving data is not really a problem for batch because um the only time it's a problem for batch is around UTC midnight and that's only if your jobs fire really quickly so uh yeah that's essentially it for late arriving data congrats on getting to the end of the streaming data pipelines lecture day one if you're taking this class for credit make sure to switch over to the next tab so that you can get credit to be certified and I'm excited to see you in the lab hey welcome to the Flink well so I I'm just going to switch over to the lab now I sent y'all a a copy of this Flink envir data pipelines lecture day one if you're taking this class for credit make sure to switch over to the next tab so that you can get credit to be certified and I'm excited to see you in the lab hey welcome to the Flink lab setup so to get started here we're going to want to clone the data engineer handbook so you can do that with either code here you can do it with SSH or you can download the zip doesn't really matter uh make sure that you have it cloned though uh that's going to be the first step and once you have the handbook open you want to you'll see there's a bunch of materials here so you want to do boot camp materials for Apache Flink training and then example m.m and in here you'll see it says a couple things here like Kafka web traffic get from I'm just going to push this here and then we want to go to the this one is that right website kka web trffic key get from website so what that means is we you want to go back to the Flink lab setup uh this link here boot camp. Tech creator. lons Flink laab setup okay maybe I'm just going to see if there's another one that I need to go to less okay there it is okay just need to TI that let me copy this into a end file first so if anyone who isn't fully familiar withn files basically um what this does is it allows you to set environment variables for your machine uh so so then when you do like a dock compose it will take these environment variables by default you can also specify like in a Docker compose you can also say like specify like just say you had a dot in for development um you can also create like a in for development and specify the file name or the environment variables that you want to use uh so but by default it uses n ah this is not very nice what I want to do is copy that you're not making it easy for \[Music\] me and then you take this one \[Music\] data pipelines lecture day one if you're taking this class for credit make sure to switch over to the next tab so that you can get credit to be certified and I'm excited to see you in the lab hey welcome to the Flink lab setup so to get started here we're going to want to clone the data engineer handbook so you can do that with uh either code here you can do it with SSH or you can download the zip doesn't really matter uh make sure that you have it cloned though uh that's going to be the first step and then once you have the handbook open you want to you'll see there's a bunch of materials here so you want to do boot camp materials for Apache Flink training and then example m.m and in here you'll see it says a couple things here like Kafka web traffic get from website Kafka web traffic key get from website so what that means is we want to go back to the Flink lab setup uh this link here boot camp. tecre doio lons Flink laab setup up and then here you'll see there's this your Kafka credentials in this PDF so what we want to do is we want to copy uh each one of these over so we're going to copy this guy over into our web traffic secret we're going to paste him in there and then we're going to copy this guy over here this web traffic key and then we're going to paste them in here and um please make sure to not publish any of these keys in any public repo because otherwise confluent will get very angry at me so then you'll see we need to get this IP to location so the IP to location you just go to this website and you'll be able to get uh the IP to location um API key it's very easy very clear uh thing to do there I'm not going to do that in this call because like uh I don't want to share my key with y'all but make sure to do that as well and then everything else is just set and you don't have to worry about it so then what you want to do is you want to um essentially copy this file and paste them in again and make it I'm just going to log in right I'm going to put my IP in here I'm going to copy it like that so hopefully um no one sees that well they probably have seen it you could take it if you want but it's not free it's free so you can't do much with it Flink m.m and we're going to say okay and we should be able to we want to overwrite if you have anyone there so now we have this overwritten now this is our Flink m.m this is how we're setting up our Flink environment so the next thing we want to do is we want to go to that spot one second let's um let's go there real quick let's go to the data engineer handbook boot camp materials for Apache Flink training so that's where we're at so let's make this a little bit bigger so in here right you'll see uh there's a couple different ways that we can do this so I'm going to do this all with Docker compose and not use the make so we have these nice little uh make commands that can make things a little bit easier but what we want to do here is we want to do this Docker compose command here uh this is going to set up Docker to run so this is going to build our Docker image so we're doing uh Docker compose M vile Flink M you know I was just saying a minute ago how you can specify the the end file that's what that what what what they're doing there so yeah let's just rename this the Flink end not sure why um we do that for this one but not for the previous but I am not too bothered for I need to do the n file before the upd and so you only really need to do the build if you want to rebuild stuff um but if you haven't built it before it's going to build it anyway um but I would avoid doing this every time because you don't need to do that every time it's just going to build stuff unnecessarily if you don't need to if you don't need to build it unless you've made changes to the docker compos like not the docker compos the the docker file so if you make changes to this then yeah you want to rebuild it but if you haven't then you don't need to build remove orphans right so then this is this a the first time for me a lot of this was already built and cached so it was faster for you it might be significantly slower because you have to download all the link dependencies but so what's happening right now is we are doing a pip install for all the rest of our dependencies that we need in order for our Flink uh container to get running uh and this takes a little bit of time so just just be patient with it and uh we'll be able to get through this together I I Believe in Us uh so all this is doing is installing all the PIP libraries that we need to go through so what we're going to be doing today is we have we're going to be checking out this thing called start job so startart job what it does is it looks at a Kafka topic so the Kafka topic that we're going to be listening on is actually the one for data expert so every time you do a page load for data expert it puts a record into kfka right so it puts a record in the Kafka and then what we want to do is uh and you'll see in the Kafka topic the Kafka topic name is boot camp events prod and um that's where it's going to be putting its values and uh it should be pretty awesome and then we have the start job where you'll see one so I'm just going to look that pause for now just because I want this to finish first for I made a joke the other day about like um um waiting for terraform to deploy things to a j and this is exactly what it feels like doing that as well so painful just waiting for a Docker far to build so painful what is in this Docker file there's a lot in it oh not too much we're just installing Python and Java th for this is so boring W that's uh G so for those who are like doing this course again how have you found it so far like how have you found like the previous weeks how have you found this week how have you found the homework are you finding are you enjoying it are you finding it f okay we're currently on this step now so not too many more steps to go e this is what I don't like about like a lot of these open source things you got to like download M and HST them and try them out on your machine and I just want to like HST in a y and not have to worry about it bogging down my machine let it bog down some other one person's machine Okay cool so that's running now good news Okay so let's play again one of the things that has is this IP address so what we want to do is this job let's just we go back to go into here look at our job we see there in here let me just make this a bit smaller um okay what it does is it actually takes the data from Kafka then what it does is it hits it actually hits a freaking um this API location API and it geocodes the IP address so we can see where the traffic is coming from should be pretty cool so uh we're almost done here it's uh probably needs about another like maybe 10 15 seconds here to uh be fully installed and ready to go but this is uh this is really good because this is um like and if you're on Mac you can just say make up you don't have to have all of this freaking fancy stuff uh but like the docker compos stuff is the one that's more Universal okay so you see we have our job manager and task manager sorry I'm just curious about how the make p works the make p just runs the docker compos anyway so this should work on Linux as well started so what we want to you can make m files work on Windows as well it just doesn't work out of the box do is you want to go to Local Host 8081 and you'll see here is our freaking Flink job manager this is where we can see our jobs running and uh there's nothing running right now so uh uh one of the other things that we want to do here oh my bad one of the other things that we want to do here is we need to make sure that our other um from week one there is um a postgress container that you need to be have running so make sure you have that running follow the instructions on week one to have that running and then what you want to do is you want to go into postgress and then you you need to run a small script first so you'll see there's this init sequel soorry I'm eating I'm eating sweets at the moment like I said they're really good let's go to our postgress instance so local need to go to PG admin it's going to take a bit of time to fire it because it hasn't hasn't been loaded in well for so I'm going to ask the serious questions now I'm going to ask in the chat as well are you a gummies person a chocolates person or a sour gummies person for me I love sours sour sour gummies are my favorite okay so I've just created the table that he's mentioned to create so you need to take this SQL and then run this SQL okay this SQL just ran uh you need to do this in order to get postgress to actually be able to collect the data so now what we want to do is we want to go back into our Flink um uh make file so you'll see with this you have this make job command this is going what's going to kick off the the Flink job command so what we want to do is we want to paste that in and run it here Docker compose and um I have it running right here you'll see like it ends up kicking off this Docker compose command and then okay so go to the m file we have a look at what it's doing basically what that's doing is it's running it's going into your your um so let me have a look let me just show you it's running this command here in your in in in your job manager basically so if we go we've got if we go in here and we go in here this container here is basically running that command inside of this container here so if I run that now for example if I run this that is the one you ask me to one isn't it um do exit I should start probably seeing some logs in here maybe not yeah you can see there's a lot of jobs that are now running a lot of things that are now running inside of that container don't want to install the M tools I want to I would have done that for e okay and it will create our kofka stream with Flink and then it says loading in the postgress and then job has been submitted with job ID so you'll know it's running when you go into here and you'll see processed events you should see a okay so what's that part number there let me just change this toast 720 nice little running thing here at Local Host 8081 if you have everything set up correctly there's one final check to make sure you have everything set up correctly and that is going to be you want to go and say select star from processed data from processed events and if this oh Local Host \[Music\] 8081 one jobs processed events okay this say now make sure to put in the password there but like you'll notice that this will have data and um if there's actual data in here then that is a good sign that you did the right thing so yeah uh I hope you enjoy the lab this is going to be really fun cool so we go into here and we \[Music\] do SCT stuff from processed events okay yeah so why is that so slow um I'm just going to exit that but yeah that's the is good cool so that is working as I want it to and then I can now go on to the we're going to okay I can now go on to the love the lecture I sent y'all a a copy of this Flink environment so let's go look at this uh thing here this uh Flink M so you see this like uh copy example M to Flink M so definitely do that there's going to be a um a lot of these values will already be covered and so you don't have to really worry about them um make sure you have uh the week one and two database running in postgress otherwise is this is not going to work and so hi how you doing you want to make sure that that's running um and you don't like also make sure to pull for the latest version of this repo because this repo used to be coupled with its own um postgress image but I don't think that that's important I think that like just having this be just Flink is the way to go and so then what you want to do is after you have that Flink M kind of put here right you'll have see here's Flink M with all of the super secret password stuff right kofka password right there all these different values here make sure you have your IP coding key you got this from the ip2 location website that I Linked In the Discord make sure you get your API key and put it here um the code will work without this but uh it will be a lot cooler if you put it in and then uh obviously you have like the cop thank you nice to see you joining me as well how are you doing today you URLs these are all the Kafka Brokers for my Kafka cluster and then obviously you have like your post address username password all these things like the how to connect to postgress it's not going to work without this stuff so you want to make sure to have that set up and then after you have that set up you want to go to SQL and then there's going to be this in it. SQL and you want to get this processed events table and go ahead and we're going to copy this table over into our uh kind of editor here and then we will um we can run this oh my God so good literally I've just been eating like these phys Sour Apples I love them so much but I'm quite a boring chocolate person like I like actual just chocolate rather than do you know like um masas and like Snickers and things like that where there's chocolate and something else with it I I I much prefer just they're like plain chocolates so boring right obviously it's going to fail for me or or create table if not exists right so it will work either way if it and then you'll have your processed events table so hopefully if you do everything right here um we will see data in this um in this table uh after we kind of walk through our Flink job so before we set that stuff up and we go into Docker I actually want to walk through the Flink code and then we can go from there so in uh in the repo you'll see uh there's a SRC folder SRC job and then there's two jobs there's aggregation job and start job so we're going to go into start job which is going to be the one that we go with first and what we're going to want to look at here is if you go all the way down there's this thing called log processing that's going to be this is essentially like the the start of the job this is where um all of our definitions and stuff are going to go from and we can look and see how this job is actually set up so one one of the things that I uh I noticed um when I was working through this the very first thing you want to do is you want to get this stream execution environment so you need to give Flink the ability to say okay we're doing a stream environment because Flink can also do like uh you can say like enable um batch mode this actually works thank you I'm good I write my thesis report and it's been hectic lately oh nice what's your thesis on works right you can actually throw that on or I think it might be is batch mode or something like that this is like you can definitely Flink actually understands that and then it will process things a bit differently but the default is streaming so we don't have to worry about it um then oh yeah we have enable checkpointing let's change this because this 10 here is actually in milliseconds so let's go ahead and multiply this by a th so that we do every 10 seconds not every 10 milliseconds because every 10 milliseconds is a lot of checkpointing that's like too much checkpointing and um then okay then we have oh yeah here it is it's in streaming mode right here that's okay we do put we do set it here so first we have our execution environment right then we have our settings so these are going to be the settings that will uh that we want to enable which in this case we're just saying okay use streaming then this is what we call call our table environment which is where we can Define our sources which is going to be which this the table environment is what we're going to use for the rest of the time so these are these higher level environments we only use at the beginning you can kind of think of them as like the kind of like the spark context and then later on you just work with like data frame oh did you refer when I had the round one um I can change it it doesn't it takes um it takes a few seconds let me do that now actually um I'll quickly change that for you what that you prefer it like that I don't mind having a having a circle just like to switch up really oh no one second my headphones have died this is a disaster one second frames going forward and that's essentially what we're doing here is this T environment allows us to kind of work more with like uh what the what the kind of the spark equivalent of a data frame is and then the F the first thing we do is we we do this create temporary function this is very similar to registering a UDF in um spark if you know a UDF UDF stands for a user defined function and in this case uh we're doing git location uh let's go ahead and look at that function when command click and you'll see here what this does it's on generate in cular data using diffusion models in simple way this is the topic sounds way above me right now but I hope it's going well and you're enjoying it and I'm glad I'm glad you like the um Circle yeah I just I don't know I quite like the um maybe you'll have to put a vote for it I quite like the like slightly um cved Square this does is get location what it does is you have this IP to location uh URL this is from that website that we were talking about and then what this does is it takes in an IP address and then it takes it uses the the API key that you uh provided and then it goes ahead and it it tries to get the country region and city of the of the data provided and you'll see that like um if you don't give it the key the request will fail and if the request fails it will just return an empty dictionary so if you don't want to make an account it's fine and we'll still get data it just won't be as cool um and so that's what this UDF does is it takes in an IP address and it returns a Json object of country state and city that's what we're going to do with uh this uh UDF function and you'll see it's called a scaler function the reason why it's a scaler function is scaler functions take in one row or one column and they return one row or one column so in this case we're taking in one column IP address and we're returning one column this Json object so that's what scaler means there's other ones like aggregating function and these other types of udfs as well that you can work with but in our case we are just using scalar function and if you're not familiar with python this is actually how you do inheritance in Python so our class is get location but it it inherits the scaler fun function uh functionality and then all we need to do is implement this and that allows us to use all the other functions in Flink that are associated with scaler function with this without having to actually Implement them ourselves okay cool that's the git location UDF this is what we're going to use to enrich our data with Lo or enrich our event data with location data based on IP address so we go back down here let's go back to that git location call right right so this is where we're registering git location great so then what we want to do is we're going to create a couple different uh tables so we're going to create a source table and then we're going to create two syns we're going to create a postgress sync and a Kafka sync and our source table is actually a Kafka source so you see here we say create events Source Kafka so if we go into here this is where uh we actually go and Define our table and you'll see okay our table name is events and this is something that is kind of we have to call it events because that's what I called it uh when I set everything up and then uh you'll see in here we have our um what we try to say here this is okay so then we have um a pattern so I need this pattern to like essentially parse the time stamp because uh in Kafka the event time is actually a string but we want to turn it into a time stamp and this is the pattern that the time stamp needs and then what we do is you'll see here you see this Watermark uh code this is where it diff this is like you'll see that this like initially you're like wow this is squel and then you're like wait a minute this isn't squel what the hell is going on with this four keyword right this is like a real really weird one so what this is saying is the watermark essentially this is giving it that 15c window I was talking about right and that like so this will fix the ordering of events that are within a 15sec window um this code is slightly different to the code in here so I'm a little bit confused are we meant to change it are we is it just that he's updated it since um like for example this is that create ENT Source C okay uh and uh yeah it's a little bit odd um don't understand why he's isn't here stamp and this is the pattern with the time yeah needs and then what we do is you'll see here you see this Watermark uh code this is where it diff this is like you'll see that this like initially you're like wow this is SQL and then you're like wait a minute this isn't squel what the hell is going on with this four keyword right this is like a real really weird one so what this is saying is the watermark essentially this is giving it that 15 second window I was talking about right and that like so this will fix the ordering of events that are within a 15 second so for the start job the waterm Mark's not there but in the aggregation job it is there and hey Jade will this recording stay even at the 31st of JN uh I don't know I'll have to check with Zach if he wants me to remove them if he does then I'll have to remove them if he's pretty happy with me keeping them then I'll keep them but um I'm I've not covered everything in all of my like stream so uh and I think I think whilst yes you you have the labs there it's not like um you won't be able to get like the uh the certificate you won't be able to get the certification you won't be able to complete the homeworks so you'll still be able to learn but you won't get the the benefits of of of completing the course other than obviously the learnings which is a great benefit to be fair and so that is uh what this water mark is doing and then this part is so nasty this took me so long to figure out how to do but anyways what is going on with this let's talk about each step each one of these configurations because uh I don't want to just gloss over this and be like yeah this is how you connect to kofka so um so you know how someone was saying at the beginning of this lecture or the beginning of this lab are kofka and Flink connected they are not and you'll see this because you see the connector here is kofka and if say we were using rabbit you could also do rabbit mq here if you were using a rabbit connector or this can also be jdbc which is uh the Java database connector so you can actually have Flink also read from postgress or from another relational database so like Flink can pretty much read from wherever the hell you want it to read from it's not that big of a deal and then you have the bootstrap servers so these are going to be all of the the the servers that are running kofka this is usually like a cluster of servers that runs kofka together and then you have the topic so you can think of a kofka topic is very similar to like a database table and um if you look in the Flink environment in this case our kopka topic is boot camp events well technically it's porcupine 78 436 do boot camp events but uh I I couldn't get rid of the porcupine stuff because uh so I use Heroku Heroku kofka that's how I set up kofka because I tried to set it up on AWS and do it all myself but then I was like wow this is a lot of work so I just decided to use heroku's kka which worked a lot easier and so then we have the cka topic which you think of a cka topic and a database table are kind of similar then you have uh this SL SSL endpoint identification algorithm this can be https and other things as well but um Heroku uses uh um key stores so it uses these four to do the algorithm to identify that you are someone who has authorized access so we just have to essentially say this is nothing because there's like this weird bug that happens so then you have the cka group I'm presume that that BG's been fixed cuz it's no longer needed to be added so kka can have a group so you can have um you can think of this as like a schema so you know how like a lot of times in in the database world you have like a P.T name or dev. name or uh you know the different uh it's kind of like that higher level grouping so this can be that's what this is in kofka and you'll see that that's in this case this is called a porcupine 78 436 boot camp so that's going to be our uh Kafka group that's what we're going to be using for that and then we're connecting over SSL so this is actually secure kofka and then uh these ones get kind of gnarly this is essentially the password and the the the the different trust certificates if you want to look at these files they're actually in this uh you'll see there's this folder called keys and you'll see it's like super nasty and encrypted files right they're both like really crazy there encrypted files that you could decrypt with the kofka password and then that gives you access to the kofka um database or to the kofka you know cluster I mean not database then after that we have a couple other things here these are the last three and then we have got over the the all the million things that you need to connect to cof so then remember how I was talking earlier about how there's latest offset and earliest offset so what I'm going to do here is I'm just going to sneak in here and I'm actually going to change this to uh we're going to change this to latest offset and then this can also be latest here so what this one does uh scan startup mode so this is going to be when you first kick off your Flink job is this going to read from the first record in kofka or the last record in kofka right and then this uh property's Auto off reset this is if it fails and restarts do we uh read from the the first offset or the last offset and that's what uh this is going to do or it can also pick up from a checkpoint so that's a separate thing so it but checkpointing is optional so this is but this is required so um even if checkpointing ends up failing then it will this will end up picking up where it left off last thing is like okay how is this data stored and then in Kafka this is stored as Json you can also put like CSV and tsv and other values in here as well but um the data here is stored in Json and that's how my Interceptor dumps the data to kopka so that's why we have to use Json here so this essentially gives us a schema that we can work with and we can play around with so then what we want to do here is we want to execute the the table environment so the table environment is saying okay this table exists and this is how we connect to it so then this gives us access to this cka CU great that's how water marking works and this number could be bigger or smaller I chose 15 seconds and it's probably it is a little bit arbitrary you can play around with it to kind of understand like how ordering works but yeah watermarking is is is important for out of ordered events though so yeah yeah no problem thanks that was a really great question so um so we've defined our source which is cof now let's look at uh let's look at our Sync here which is we're going to have create process events sync postest in this case right um we uh we actually don't necessarily need this Watermark here because we we only really need it in one spot but in this case we're making another uh table here so in this case what we are doing is we have a table that has a different schema right you'll see it has this new Geo dat column which is also varar but what this does is you'll see the connector here is different in this case we are connecting via jdbc our URL is our post address URL that's defined in that Flink environment and then we say our table name table name in this case is processed events and then we have our username and password and then we have the driver so this is going to be our driver code and this is something like when you're deploying Flink you have to include this uh this Java code in your deployment if you want to be able to connect the postgress you have to include the driver code as well otherwise this doesn't work so that is essenti how the SE works for post is this I'm pretty sure that this feels like just standard event handling but with Flink and Kafka um just looks very similar to to what I've done before with um both rabbit mq and vent Hub and retrieving and posting events onto event hub this allows Flink to directly access this postgress table processed events and keep it in mind though that this create table statement does not actually create a table in postest this is just to have Flink be aware of the schema so that's why you actually have to run that you know how I said you had to create this create table at the beginning of the lab you have to run this create table it's because you they're different right and so uh this is just to let Flink know what the schema looks like um so then you have uh so that's our post sync let's look at the last sync and then we talk about how uh each of these things actually works so then we have a cka syn so here's a cka syn we have process events kofka and then you'll see in this case we don't use this pattern because we don't need to because it's a sync um so then we have uh we have our um sync ddl which is all this is the same you'll see this ddl and the postgress one are the same because these are literally the same I just wanted to show people how you can write data both the postgress and the Kafka um and you'll see in this case all of this is exactly the same as um in the source data because it's the same cluster you'll see the only difference here is actually the topic you see the topic here is a little bit different it it's funky do I offer mentorship opportunities so I did it in 2024 um but I've put a pause on it for now other than my current mentees that I do have and that's just because um I really want to focus on the breadth of impact for people so I want to focus on helping more people but like but like using like YouTube and things like that rather than like one-on-one help which I think is is really really helpful and beneficial but it just helps less people so I have put a bit of a pause on accepting new new mentees and for the mentees that I do help they typically they're typically getting their first role as well so so so so that's been the aim for me to help people like all the criteria I guess you would say to help people is are they currently looking for a job and is it this their first job whereas um so I've put a bit of a pause on that and then I am as well as that focusing entirely on on growing YouTube so that I can help more people on YouTube and impact more people that way and then uh what else get my headphones in and then um I am yeah yeah so so i' no no I'm not I'm not mentoring new people at the moment I have still got a few mentees that I've got but but they'll be my last ones basically until until they found a job uh and then yeah I've only got like one or two two left now uh and who are currently actively looking so hopefully they'll find a job in the next couple months one's just about to finish University and then the other one is is got a couple of in UPC coming as well so fingers crossed for both of them and D Tim Hello nice to meet you how it works right now like I'm doing it this way where I do this like weird split logic so that we can get that weird porcupine uh thing out of it that like Heroku gives me but then I give it the new table name keeping in mind that like uh like and I'm sad about this because this was something I thought heroki would let you do but you can't change this name and create a new topic Heroku does not let you create new topics on the Fly and then in here you then can create your topic name and then add it to this big old thing and then this is then our sync and this is where we can write data out and so we now have our sync and we have our two syns and we have our um source so we have now gotten through the first five lines of code of this freaking Flink job and now it's uh let's kind of like actually go to where some of this transformation logic is happening okay so this gives us our Flink postgress and Source table and you'll see that like one of the things that's really cool is this actually you'll see this um actually just returns table name so after you create this table you can actually reference refer to this table just by its table name just like you would in a normal sequel environment and I like Flink sequel for that reason and that's one of the things that's really cool about Flink sequel because Flink Sequel and one thing I always never get about like companies is like why don't they just call things what they are like source and destination why do we need to add things like sink in there I'm pretty sure in in in um in aour we just say destination well I do anyway yeah and uh link Java are going to have like the same performance um and then in this case what we want to do is we're saying okay we're going to add to the cus Inc insert into and then this is what we're going to do is we're going to look at our IP address right and we pull the IP address this way because it's like kind of in a nested field because Heroku actually does this weird like IP address forwarding thing that you need to uh kind of extract it out and then we want to do a a date field here um so this is essentially moving this back into like a normal time stamp field because um it's it was having a hard time serialize C was having a hard time serializing this to Json because it was like a timestamp column and time stamp has a harder time like serializing to string but this serializes it back to string and then we have the referrer which is like in terms of web traffic this is like where they came from like LinkedIn or Twitter or wherever right and then you have the host in this case this is like Zack wilson. Tech or exactly.com or the all there's like there's like a bunch of them and then you have the URL which is like back signup or back login or back boot camp or whatever and then you have uh this this is you'll see this is actually the git location call so this is actually okay so that's actually been deleted for some reason I go to the aggregation job it's in there though isn't it okay so for some reason it's been deleted from the job the start job but it's in the aggregated events sauce Kafka yeah no it's just they don't have a CF one for some that we can then uh call you see how we we we created the git location up here and then we are calling it in um SQL right here and then we are reading from kofka that's essentially what source table here is and then this is inserting into the kofka sync which is the new topic so we're moving we're we're enriching it with data and then we're moving it so then we have uh the second one here it's doing literally the same thing where we are taking it and we are uh instead moving it into postgress and this is going to write records out to post and you want to add a DOT weight here on the last line of your code uh in blink otherwise this will just uh will run uh for every record that's in the current cka que and then it will end the job and this this makes it run continuously this weight so um then we have a little try catch here to in case there's a failure for postris and uh this could also be a failure for kofka but like it's kofka is probably not going to fail because it's like kka to Kafka which is going to be like less of an less of a bug so then at the bottom here obviously you have the actual um like the entry point for the job where we actually call the job so this is the job that we're going to be working with today to actually kind of go over what's going on here to show like hey look we're so um yeah I don't know why but the CFA bit is not there it's creating events Source from C CA but then it's putting everything into the sink so which is fine but it's just just not the same as what's on here basically I really appreciate your efforts for mentoring program I also jump to the rescue of new H's devs it seems that senior devs care a lot less about the people in the wellbeing as they should also saw some of the efforts on keep going it's an important task they say that that because I think we appreciate we have to appreciate it more now we appreciate not enough anymore well thank you yeah I I do agree that like especially in some places where they a lot of juniors are just kind of left to defend for their own devices and feel like they don't have adequate support uh I think I think quite lucky at Microsoft in that you've got a good um you've got a lot of people who want to help so that's been a really really refreshing thing thing and we work with a lot of customers who need our help as well um because a lot of the time we're training the development team to use a your so it's it's kind of hopefully it's it's trying to spread that kind of impact just outside of work as well we we're doing stuff so then what you want to do now that we have uh that working we can actually go into our messy Docker environment now so what we want to do here is say make down I'm going to do make down first which is going to we want to kill all of the um the the images that we have up and we say make up okay so now our job or our um our environment is up and running so let me show you that environment real quick so if we go to Local Host 8081 you'll see okay so this is our task manager and you'll see uh we have no jobs so this is um and here's our task manager this is where it's running and you'll see all sorts of different things about like Flink the memory you can see all sorts of crazy stuff but the the key thing here is you see there's no running jobs because I just started up the task manager and let's kind of go over like what's going on here with our slots right so we have three available slots so that means I can run three jobs at once but if I have more than three then uh like it's going to wait for one of them to finish or something like that but our jobs are actually designed to never finish they're supposed to run 247 because they're streaming drops but um Flink drops can also be batch drops that's why they can finish that's why some of these have finished right so flink's kind of a wild one where it can do both so then um what I want do here make job so this is going to call up uh the docker container again and it's going to see how it's saying okay Docker compos exact we're calling the job manager and we're like okay we're calling Flink directly and then we're calling the the start job file just to make it clear like I'm not using a Mac but I'm using WSL which is why my make is working um but yeah you can use make in more than just map environments and then uh you'll see okay okay now we have two jobs that have been submitted these are the jobs to write to kopka and the job to write to postgress so Flink is similar to spark in this way where you know how spark they say spark is like lazily evaluated and it only actually fires up a job when uh like there's like an insert statement or a collect statement or there's like a movement of data Flink is very similar like that and that's why there's two jobs that end up getting fired here and that's because of these insert statements here these insert statements cause two jobs to be fired and uh that's what's going on if y'all like are curious about like why there's two jobs instead of like one big job so you'll see here now we have uh we have two jobs here that are running and if we let's let's go into this processed events so if we look at this guy this is going to be our um our code here and you'll see it's it's going pretty well back pressure zero busy zero because if you remember I changed the source to be uh you see how I changed the source to be latest offset so it's only it's only reading a new data so like let's let's let's like load up a row okay so that's going to cause it to have at least one of data that um let me go into here real quick I need to let me delete the data here first so that we can see the new data so okay so I deleted the data from uh from processed events and let's uh add one more record to the queue and then what we want to do here is if we go back here we say select star from processed events okay so you see I have one record here and see here is US California City San Francisco right so and if any of y'all freaking want to like go to Zack wilson. Tech you'll see that your data and location will show up here and um and obviously I live in San Francisco so this is actually pretty good and kind of creepy I was able to do this and um and so idea here is if you see I can go to other Pages too so if I go to like boot camp right oh let me go to ex look at that dude I got some freaking people in here he got some Texas right so um Sugarland Texas interesting it's a cool place so anyways you'll see uh like but you see how all this data was just available immediately because that's what real time's all about is it's immediately available that's what it's all about and you'll see um this guy here actually um you can see some of these people are actually coming from different spots like this guy came from my substack so this is probably an event that was not from the boot camp because you see like there's a referrer here that is um my substack so they came from my substack newsletter and um you can see oh that's kind of like how all this is working right and this is the idea behind uh how this works but like what you can do right is you'll see with the Geo dat so if we like geodata and we Json and then we say uh country right then we can like um there we go and then we can also then uh count one and we can say uh what group by one there we go then you can see all the different uh countries so we got us as uh leading we got Nigeria we got Canada and we got Brazil right we got we got a couple of people showing up from all over the place here right and uh this is essentially what's going on but it's in real time right so like as more data comes in see there's another one see now we have another one V like Venezuela I have no idea um uh but like you'll see like this is a live data feed right of this processed events because my sorry I'm just doing this thing where job is just processing new data as it comes in and so this is pretty cool um let me just uh cancel this right this is cool so I'm just going to show you this so this is me I've hidden my IP because I don't want anyone to know what my IP is but um how cool is this it yes so thank you sorry I just I had a bit of a moment um so how cool is this so I've got my this this yeah it's the refer the different things I've been on it's it's noted that I've got I've done these few things \[Music\] and it's got the country GB the state England and the city Elon which isn't where I'm from but it's pretty cool that it's it's near where I am where how far is it's about 7 17 minute drive I don't think that's that bad is it it's like the city is not too far from me so I think that's pretty cool so one of the other things this job is also doing right because there's two jobs here right so if we saw here so you see processed events kofka you see it was here at this like 39.67 megabytes and you see how like now it's a little bit higher it got moved up to this 40 megabytes and now it's like slowly growing and like so that's what's going on with this process events kofka because we are writing more data to this cof Q as it comes in as like more and more people like you know spam my website and stuff like that and so this is kind of the idea behind how uh Flink works is you have um kind of different values that can come into place here so one of the things I want to show real quick though is I'm going to kill these jobs real quick so just gonna I'm say so the jobs are killed and I'm gonna say make down I'm just going to kill the whole I'm going to start over here and what I want to do here is I'm going to change this to earliest just to kind of show you the difference right so you'll see here when we ran that what there there was like see how there's like I don't know like 50 records because that's like how many people hit the website in the last like five minutes but now when I run it this time with earliest offset if we go look at the the the Kafka dashboard so ours is boot camp events so this is going to be our um actual data set that we are working with right so this data set is like about 15 megabytes so I would hope like if we use earliest offset we're going to process all of this data again it's going to process all the way from the beginning and that's like what we're going to do when we redeploy this job here so in this Cas we're going to say make up okay so now what we want to do is we want to then say make job and then I'm going to show this is going to create some interesting things in the UI that I want to also uh illustrate to y'all so that you can understand some other kind of pieces of this puzzle so there we go we have our two jobs running so now let's go back into the Flink UI while like we can still uh show what's going on here so let's go look at this process events job process events job and you'll see one of the things you'll see is like you see how it's like red right now and it says it's very busy a 60 64% red and so what's going on here is um it's busy because of the fact that it's oh there we go now it's really red right so now it's like really busy now it's like essentially maxed out so what happens here is you have um you see how there's like busy and then there's back pressured so these are two different kind of things that can happen because so busy means okay we are at Max compute like this this Flink job cannot process any more data and um if it processes more data it gets moved into back pressure and essentially back pressure is the second layer where back pressure essentially says it tells Kafka Slow Down slow down because we we we need to essentially add a little bit more latency to the data so that we don't out of memory and that so that we can process all the data that's coming in and this going to happen like you know if your data set goes so this is my machine having a bit of a meltdown I'm just going to stop that Viral or if your dat set is like um like if you get a lot of hits it's very spiked you get some spiky data sets because you can't necessarily manage that um yourself because you can't just be like I'm going to stand up more Flink machines obviously you can set the parallelism and have like do distributed compute with Flink as well but you have to pick the number of machines ahead of time and you don't know how much data is going to come in you could have more could have less right so that's essentially what's going on here right is now we have our data and it's um essentially running right you'll see uh now I bet it's back to okay so yeah it's still processing stuff it's still dumping everything out so um this is kind of the idea behind how this stuff works is we have our um back pressure and uh our uh busy busyness of our job and you'll see the kofka one is the same way right it's just maxed out like trying to just trying to struggle through it and then eventually it will uh it will like be essentially go from like 100% to zero percent because it's just going to finish everything and uh it will have processed everything but now if we look at like this query here right so remember like we have like 50 rows here right I bet there is a lot more than 50 now right oh yeah look at that look at that there's a lot more than 50 now so now we have like uh okay see we have like we have our us we have India Great Britain Canada whatever IE is uh Japan right all these different countries here even some that didn't even uh you have null here so that means that some of these uh IP addresses didn't geoc code correctly or they like they have like a there's like a null right but you'll see like I don't know there's 44 different countries so people in 44 different countries have visited my website in the last day and that doesn't I mean it might not even be done right let's uh we put like a like put a it should um keep processing there we go so you see how it's still it's still struggling through right all of the data here and that's one of the things that is tricky about postgress syns so kopka sinks are generally a little bit more performant because of the fact that they um are meant for high throughput because you have like the uh the queue can just be dumped in and it it's fine whereas postgress is not right postgress is more meant for uh like transactional data that is like kind of not large volume right it has like a and so you want to be aware like when you're building out your Flink jobs like where is the data going and like what this is wild this is not supposed to be so busy why is this so busy all the time like y'all need to like quit visiting my website like last time that just shows you like because I did this same presentation back in May and like it was busy for like it was busy for like a minute and now we're like we're rocking almost four minutes now guys like so this is this is madness but like this is the idea behind like how um you kind of build Flink jobs but the key thing I was trying to say here is back to the syns right so Kafka is going to be a very high throughput sync that like can manage things a lot more effectively so I imagine that the the kofka dump here is going to end up being not busy a lot sooner than the postgress one just because of the throughput of kfka and then uh that's kind of the idea behind how these jobs work congrats on getting to the end of streaming data pipelines lab Day One streaming pipelines are crazy right if you're taking this class for credit make sure to switch over to the next tab so you can get credit for day two I hope you enjoy it I wanted to talk oh I love this this is cool this is the stuff that I really really enjoy though like like when you get to this kind of software engineering type stuff cuz it's a lot it's a a good mixture of both data and software I really like it it's cool um yeah but it's very very similar to event hubs to me it's basically event hops but just a different flavor so who knows I'm just going to do another uh make down and yeah I think I'm going to end it there because I've done the first lecture and this first lab I've got the lab completed I assume that they just removed the capka um sink for some reason uh but everything that I needed to do was in there so I'm pretty happy with that so yeah uh yeah one one thing I think you start realizing as a software engineer is that like everything is just the same just with different names like like all these things that you end up doing is just the same with different names like like just different products so for example AAR is just the is literally just the same as AWS like it offers the same stuff like you can do the same things in it but it's a different name and they call things different so like you know AWS has ec2 instances which is B basically just a fancy word for VMS and stuff like that and it's like and then you know you've got um security groups which I'm pretty sure like is I don't know what they're called in um in AWS but they they called something similar and then you've got like uh Lambda which is I think pretty much just a short functions and so you got all these different things that just are the same and so like for the example this you've got Kafka which is basically like event UPS in in aure so yeah that's cool um yeah I like that that was fun I like that one so I'm going to end here thank you for those who have watched it was great to talk with you great to speak with you hopefully we'll see you next time but yeah uh goodbye thank you for watching see you next time


 > [!info]
> - **Understanding Streaming Pipeline Latency (0:42):** Differentiate between streaming, near real-time (micro-batch), and real-time processing. Stakeholder definition of 'real-time' rarely means a true streaming pipeline.
> - **Stakeholder Communication (8:05):** Don't just take stakeholder's words for it. Understand the actual latency requirements of the pipeline. An SLA (Service Level Agreement) discussion is crucial.
> - **Team Skillset (14:03):** Assess if your team has the necessary streaming skills before implementing a streaming pipeline. Avoid creating a situation where only one person can troubleshoot.
> - **Incremental Benefit (16:36):** Evaluate the incremental benefit of reducing latency. Is the reduction in latency worth the increased complexity?
> - **Homogeneity of Pipelines (18:18):** Consider the homogeneity of your pipelines. If you're primarily a batch team, sticking with batch might be better.
> - **Data Quality Considerations (20:21):** Data quality is more challenging to implement in streaming pipelines compared to batch pipelines.
> - **Streaming Only Use Cases (22:14):** Streaming is essential for use cases where extremely low latency is critical, such as fraud detection, high-frequency trading, and real-time sports analytics.
> - **Master Data Latency (24:39):** Streaming can be powerful for master data that is upstream, as it can make downstream datasets fire sooner.
> - **Tradeoffs and Challenges (26:11):** Be aware of potential memory issues when de-duplicating data in streaming pipelines. Micro-batching might be a more viable solution.
> - **Impatient Analyst Stakeholders (29:15):** If analysts complain about latency, determine if faster data would actually change business decisions.
> - **Streaming Pipeline vs Batch Pipeline (32:28):** Understand the different requirements of streaming and batch pipelines. Streaming pipelines require a software engineering mindset and more unit/integration tests.
> - **Complexity vs Latency Continuum (35:26):** Recognize the complexity increases as latency decreases. Consider the trade-offs between daily batch, hourly batch, micro-batch, and streaming.
> - **Real-Time Myth (36:07):** Real-time isn't instantaneous due to network time and technical steps involved.
> - **Streaming Pipeline Structure (39:56):** The structure consists of sources (Kafka, RabbitMQ), compute (Flink, Spark Streaming), and destinations (Kafka topic, Data Lake, Postgres).
> - **Watermarking (47:55):** Use watermarking to handle out-of-order events in streaming pipelines.
> - **Failure Recovery (50:13):** Flink offers checkpoints and offsets to manage failure recovery. Save points are used for other systems, checkpoints for Flink itself.
> - **Late Arriving Data (52:15):** Understand how to handle late arriving data.