---
title: "DataExpert.io - Week 4 Homework & Week 5 Lecture 1 - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2025-01-15
source: "https://www.youtube.com/watch?v=el8PzdD7zqM&t=3467s"
image: "https://i.ytimg.com/vi/el8PzdD7zqM/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCMgZShGMA8=&rs=AOn4CLCtDOFCwgAuLRy83WhUT4QllnWsxQ"
created: 2025-03-23
tags:
  - "youtube"
  - "Data_Engineering"
  - "Metrics_and_KPIs"
  - "Experimentation_Frameworks"
summary: "DataExpert.io week 4 homework and week 5 lecture 1 from the Data Engineering Bootcamp. Key topics include metrics, KPIs, and experimentation frameworks."
---
# DataExpert.io - Week 4 Homework & Week 5 Lecture 1 - Data Engineering Bootcamp

![DataExpert.io - Week 4 Homework & Week 5 Lecture 1 - Data Engineering Bootcamp](https://www.youtube.com/embed/el8PzdD7zqM&t=3467s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> e hi everyone and welcome back to the channel uh so today I'll be just going over the changes that I've made so I'll quickly be going over the changes that I've made since the previous episode so if so if anyone's not seen that then I suggest you go ah and have a look at what I did there but um there were a few changes that I need to make in order to make the llm happy so I got like a be initially like with the changes that I had and then there were a few things I needed to change to make it get a a so the first thing was that I just added needed to add loads of comments everywhere and I just I just made the llm that I was working with do that so I yeah I just asked um chat gbt to to document it basically um the second thing that I did was uh previously so I didn't make any changes to anything like to two three uh or three B so all of those were the same so so other than the comments that I'm talking about everything else was the same as the previous episode the only changes that I made was in the first one so for the first one uh let me just get the let me see just open up a new query I can't remember I saved it in here okay I'm just going to copy this refresh this I'm just going to refresh and make show you the previous okay so we'll go through what the changes are so as you can see I've added a lots of comments here um but I didn't change any of these ones down here I only changed the top ones up here so the first thing that I did was um I created a the thing that the llm marked me down for was it wasn't like Dynamic so you can see here there is um hardcoded values for current year uh so so so what we needed to do was make it Dynamic and so what I did was I I created a way to get the latest year that hasn't yet been um been like been changed so so I I got the like last year by basically saying okay well I either want to get the max year of players growth accounting so if players growth accounting um has a value then get the max current year because we know that that's going to be the year that was last changed um if not what we want to do is we want to get the first year that we're going to start with so so just say like the last year in so just say in in um in play as the first year that has any data is 2006 what this is going to do is it's going to get the minimum value of um like of the current season so say the current season is 2006 and then it's going to minus one from it and then so we're going to get 2005 so that means we can start from that like value uh where where um where we starting from the beginning so so that's that's just the edge case for when you don't have any data in uh in the players growth accounting which is what we we had before so so that was the thing that I did to get the last change year so we could get that dynamically rather than get that um get that uh static and and hardcoded in and then the second thing that I did was I just changed the uh I just changed the like naming so instead of being yesterday I called it last season and this season and then what I did as well was I had to do a join on that current year because um because because we then had this value here of when the last change value was um and then and then I did the same thing for this season so I just changed it to be join but instead of um just being last change year I had a last change year plus one so that will literally do the exact same thing as this but it'll just do it dynamically so instead of having to put the values in here like whenever you like insert into this table with this these changes it'll just get the last year so just say you know it's 2006 and 2007 and 2008 and 2009 Etc so it's it it saves a lot of time um so so that's what I did there and then the only other thing that I did other than this um like Dynamic way of getting the years the only other thing that I did was I did change this here to be a bit less um to be a bit to be a bit more concrete so before um I didn't store whether or not the current year is active um but this is a really useful tool to to use because it means that instead of having to like do uh do maths in logic within your casing you can just use that is active value uh and that is active value is already on the players table so we can just use the players table for that to um to detm whether or not it it's needed so um so as you can see here so as you can see here we have this when do T do is active is null then false else T do is active so we're basically saying just set is active and then what we can say is we can do some logic here where we can say um if yesterday is null and t. active is true then new um when um and then we can do some other other ones um so we can do when y do is active equals true and T do is active equals true then continued playing um and and you can basically do all all of the valid states that you need to do rather than like having these various States here which can um can I don't believe that it it would it it it it had any edge cases but um I think the llm just wanted it in a very specific format so I just gave it the format that it was as asking for which is this um and I've noticed as well like you probably need no um I was just thinking what if y do player is null and then T do is active is false but um you you I don't think you'd have a scenario where like you the first year that you're in there would be tired um so so yeah that's that's all I did uh those are the only changes that I made really and then I got the air um everything else stayed the same so query 2 query 3 stayed exactly the same it was just this one here um so yeah just keep that in there slowly just scrolling down so if anyone wants to have a look at these changes in fav detail that they're more than welcome to okay so what we're going to do now is we're going to go into the lessons basically other tab so that you get credit for it exit of you check out the lab what I want you to do is make sure you're on the latest version of the code base um oh no you have to go to the lecture first actually thinking like a product manager is critical for being a good data engineer and what does that look like it looks like you build pipelines that actually impact the business that actually change business decision and that can be in a couple different ways one is you build good metrics that change business decision making so learning how to develop good kpis and good metrics is a very powerful way to do that another side of it is you build metrics that are impacted by experiments so if you can learn how to do both of these things build good kpis and understand how experiments work and how to plug your metrics into experiments you will become a much more impactful data engineer and in this two-hour course we are going to be covering all of that so I'm excited for you to check this out and make sure to like follow And subscribe so I think this is an important question why do metrics matter at all it really depends on the company and all sorts of different things on like how much metrics matter and how much they actually impact things I know for example like when I like when I worked at Airbnb for example uh metric mattered less there uh than at Facebook for example like Facebook was a very very very data driven company they were all about that data and I'm not saying Airbnb wasn't I'm just saying that like a lot of times Airbnb they'd be like well the data is pointing this way but we have a design kind of situation where we think this is a better design anyways and so one of the things that can happen there is it really does depend on who the founder of the company is of of like how much metrics actually matter because the founder is going to set the culture of the company and if you look at the difference between Mark Zuckerberg and Brian chesky then it becomes very clear why there's a difference in the importance of metrics where you know Zuckerberg Computer Science Guy very like nerdy you know he's like I am a robot right Zuckerberg gets that Vibe right it's like if he didn't care about like like metrics that would be like very out of character for Zuckerberg right but then Brian chesk is more of like a design guy he's more of like an artsy fartsy guy like I feel like if chesky didn't found Airbnb he totally would have been like a starving artist type and so like very very different vibes from those guys and then like so how they lead can make a big difference and so keeping in keeping that in mind when you're like working with metrics and working with things like depending on the company you might need to have another layer there where at for example at Airbnb one of the big things was it was it wasn't just about like this metric went up this metric went down it was more like you needed to come up with a story behind it like oh like we made these changes and this happened and then maybe it's like a a six-month Arc or a one-ear Arc and you talk about like all the buildup to these changes and these differences as opposed to like at Facebook you could just be like this number went down and that's bad or this number went up and that's good right and that's where you can have depending on the company how much they put in stock in metrics versus storytelling can can tell you a lot about the company but also metrics are important metrics provide visibility metrics they they explain the I also Imagine though that like this isn't necessarily just company based but um it's team based as well because like some teams probably will do a lot more with metrics than other teams uh and it really is like when you have a big company it is it is so like different for each of the the teams the world especially as you get more and more and more of them the the visibility that you get into your data is just better so uh and just into your business because if you have different types of metrics that are measuring different types of things you can make more clear decisions because you have more visibility into the business environment and the more clarity and visibility you have the fewer icebergs you're going to crash into because you can see a little bit further into the future and we we're going to talk a little bit more about that as we kind of go into more metrics but keeping in mind metrics are very important so what are we talking about today well we have about five things that we're gonna talk about uh so have metrics play a huge part in data modeling um they in a lot of ways they kind of inform data modeling I'm sure if y'all uh finished the homework uh from week three on the spec one of the things you'll notice is that a lot of the metrics that you defined in the spec and the aggregations that you were looking for really did inform what the data table should look like and that's a big deal because sometimes you can have really funky metrics that people really want and they're just very hard to get or they're very weird to like create a data table for especially like if you are getting if you're getting Beyond like simple aggregations and counts and stuff like that it it can get very interesting to get the right data model for that kind of stuff especially like if you're doing like ratios and denominators where you have a over b or A over B plus C or like any any other like anything that's more than just like a simple count or aggregation metrics can get kind of funky so also metrics uh make sure uh metrics can't be gamed uh we'll talk a little bit more in details there uh I I I ran into a couple different cases where I've seen metrics be gamed at Big in big tech companies and how you can avoid that uh there's definitely a lot of different examples there because metric metric go up does not mean good like it it's more complicated than that and that's why you need this it's one of those things as well like when people are aware of the metric that's when they can game it so typically from from what I understood of like kpis you kind of need conflicting kpis so that you don't um only focus on one uh one area to to hit those metrics so I'm trying to think of an example like um so say uh you know you want to um hit uh you want to to uh like hit hit your sales figures for like and get so much in revenue and then a opposite metric to that might be uh that you um want clear scoping and um for for 90% of projects or something like that so so you have one metric of you know okay we're being Rue Focus but actually we also need to make sure that we have clear scopes of what it is that we're delivering so that when um when you're then been measured on these metrics you're not like just you know selling selling selling without making sure that you've got like the the um the revenue that is like is without making sure that you haven't got the clear scope in there as well so uh so that's that can be really important because if you don't have if you don't think of those like conflicts and things then people will just maybe maybe they'll just start selling loads and then then you end up with loads of um projects that you can't deliver need to have uh some balancing that needs to be put in place we're also going to be talking about the the inner workings of experimentation uh so how does an experiment work how do you you know specify test and control and we're also going to talk about the difference between feature Gates and experimentation those are slight they are very related but also different so we'll we'll talk a bit more about those and then to wrap it up we're going to talk about how to plug metrics into experimentation Frameworks so uh for example um we're going to be using stat Zig for the lab today and in stat Zig you can put web loggers in your API and stuff like that you can definitely do that with stattic um but the problem is is like you might need other metrics that are not easily logged like uh they might be more from like a database or more from like state or something like that they need to be more like eted and we're going to talk more about how to get those metrics into your experimentation Frameworks as well so yeah let's uh let's let's get into this all right so key thing here uh when you're doing that spec the spec stuff that we were talking about in week three and you're having back and forth with your stakeholders on what are the business metrics that you should be looking at this is something that you should be considering at all times is if they are asking for something that's more than like a count or a sum or something that's you know really just like a group bu like simple thing like that's going to be mostly something that you should push back on at least in my experience like whenever the data scientist is like hey we need this funky rolling average sum uh percentile thing uh generally speaking as a data engineer you should push back on that that's like not your job that should be their job and you should just give them the raw Aggregates and let them let the analytics people give do whatever sort of analytics and numbers that they want on top of it so key thing here don't let the data scientist dictate your data models with extremely complex metrics to make you think like you have to add all these other crazy metrics into your data model because that's not the case that's not the case and nor should it be that's just going to like it's going to make your life miserable so let's yeah let's go to the next slide okay so what are the types of metrics so you know in the last slide I was talking about there's Aggregates and counts those metrics are great uh there's also ratios you can think of like percent of users in different countries talk about like compositions before in in a previous lecture uh you can also have ratio which is like percent still active if you remember from last week we talked a lot about the uh retention metrics right or it's like percent users still active like the survivorship bias and those numbers those ones are interesting and like because that's going to be like you have A over B in those cases and so those can be other metrics generally speaking as a data engineer you should only be supplying Aggregates and counts and what grain you supply those at uh is usually at like the entity grain you know in like uh week one and two we talked about daily metrics where you have like user ID and then you have metric name and then you have the value of the metric those are going to be generally speaking the metrics that you're going to supply to data science and if they're asking for ratios or they're asking for percentiles they're asking for anything like that like they're just trying to give your job to you give their job to you and that's not cool I mean I used to like as a data engineer that was a big mistake that I made was I felt like a very confident data engineer where I was like I can do data science too so I'll just do it because they asked me to and I'm a good I can do data science and data engineering I'm a freaking Wizard and uh for the most part like I learned that one you don't get recognition for that in your performance reviews for the most part because your performance reviews are based on how well you do data engineering not based on how well you do data science so that is a big minus uh and also you're going to be ask a lot of weird statistical questions around the metrics that you supply that you probably don't want to answer because you aren't a data scientist and that's more their wheelhouse and they and I don't know like I'm fine with answering those questions because I have the statistical foundations to like do data science but like I don't enjoy those questions I don't it's not like I'm like oh my God like I just I'm so excited for you to ask about freaking whether or not this is like a normally distributed freaking metric or like all the different things that can happen when they're talking about metrics and definitions and stuff like that like so the key thing to remember is that if you define a metric you should be able to ask answer any question about that metric and so that's why you want to keep them simple because that's not like data Engineers aren't meant to be statistical Wizards it's not our job like like I look at statistics as something that a data engineer needs to be like two out of 10 or maybe three out of 10 at to be successful on the job so you don't have to be a crazy wizard at it but this is the main idea you have these three types of metrics and we're going to talk more about like what did anyone else find statistics in school so boring like so boring like it was just the most boring part of of math think why we have these types of metrics because they're all pretty useful um so Aggregates accounts they're the Swiss army knife uh you should be using these a lot as a data engineer you can think of like the number it's like counting events like you know fact data is a very common one like the number of times someone's logged in the number of friends someone has added the number of notifications that someone has responded to like a lot of those are really powerful metrics but also other ones are like more like they're more they're like in the middle between like a and a dimension like are is the user active and like did did they send any messages today did they like any posts today so it's more like a daily active thing like a one or a zero kind of thing and those can be very useful especially when uh you have uh a skewed demographic like for example uh with notifications if you make an extremely viral post the number of notifications that you'll receive is a lot higher like an order of magnitude or two orders of magnitude higher than the average user but if you have just the notification sent or notification received yes or no it doesn't matter if you went super viral or not because whether you got one or a million it doesn't matter and we're counting people not actions and that's a big thing to remember here is that there's the two different types right you can have uh metrics that are based on the number of people and then you have also ones that are based on the number of actions and those two different types of Aggregates can be very very important and we're going to go more in depth into those different types of Aggregates uh later later in the lab and in this presentation so keeping in mind uh you're gon to have a lot of Aggy here um another thing to remember is uh you're going to want to bring in Dimensions here as well there will probably be some specific cuts that people care about I know uh at Facebook they really cared about the like the Young gener trying to get them off of Snapchat and Tik Tok and getting them on Facebook obviously given the bring that doesn't seem like they were very successful but that was something that they definitely tried to do and then you have uh other other ones like there's a big strong push to grow in places like India and uh Brazil and other countries so there's probably a geographic Dimension that you want to add and sometimes uh when you run experiments they can be statistically significant overall but not statistically significant in some countries so we're we'll go a little bit more into detail there uh as we kind of go through this presentation but the key thing to remember about this slide is when you're working with metrics uh as data engineer you generally want to give your analytics Partners simple metrics okay then you have ratios ratios are important uh and they are uh something that mostly data scientists should work with so that because you don't want to have to be a data engineer and have to think about like the numerators and denominators and the inclusions and exclusions of different users in the numerators and denominators of these uh functions that's stuff that should be done by the data scientist because they're going to be better at that like statistical knowledge than uh you are as a data engineer unless you're a super awesome data science data engineer person I don't know then maybe do both but even then I I'm more of the opinion of having that be owned by the analytics people just so that you aren't burdened I mean in my case when I've owned metrics like this over my career the problem that happens is that like I end up doing both roles and I'm like answering business questions about these metrics and I'm like why am I doing this I want to just write schola and do ETL and stuff like that and then it's like then I just do both and get kind of burnt out obviously doing both is a good way to get impact but uh we can talk about that more later but anyways these ratio metrics you like clickthrough rate the number of people who saw your rent page versus the number of people who sign up you also have a purchase rate the number of people who signed up to the number of people who gave you money and then you also have things like cost to acquire a customer one thing to note though as well is just like um that what I agree that you know that try and make it so if you have someone there who can do the job to to to help them do the job but don't just be like oh that's not my remit because um you know I don't want to do that and I just want to do the cool stuff that I like to do uh I think I think that's it's really important to to obviously not fall into the Trap of doing someone else's job but like help them if they they're stuck or whatever rather than just lobbing over the fence because I think as a as an engineer for example as a software engineer like I definitely seen developers in the past where they've just kind of lobbed testing over the fence and I don't think that that's the right thing to do I think it's a it's a joint thing it's a joint responsibility so uh making sure that you can help your analyst and things like that get the answers that they need I think is really important like you might spend you know a thousand doll on Google ads and then that gives you 10 users and then it's $100 a user to get to acquire a user so you could think of like there's a bunch of different ratio metrics that you can have that uh like are very common and ratio metrics generally speaking they measure not the like amount of something but they usually measure the quality of something like the conversion rate is the quality of your web page you know if you only convert 0. 1% of your users that visit your web page it's probably terrible right but if you convert 100% like which doesn't happen but if you converted 100% of the users that probably means that you had one traffic and it was you and you signed up and that was it right because 100% also is like you should be skeptical uh but generally speaking a good conversion rate is like like more than three or 4% if you can get that many people to sign up that's great so that's kind of the idea behind ratio metrics and these metrics are going to be also they can also be cut by different dimensions and this is where it's important to be careful because what dimensions you cut by makes a difference in how aggregate uh because if you have for example uh say you wanted to look at the number of the number of unique users or like like so you're looking at like unique website user hits div or like it's a conversion rate but like the bottom the denominator is the number of unique users who visited and the numerator is the number of unique users who was there were there right and then if you look at it from if you break it out by uh say like device operating system then the numbers aren't going to necessarily add up to the right things because those like you can have someone who's on Android and iPhone at the same time and so that's where you get this non additive sort of thing where it's like if you look at the conversion rate of all the operating systems and like try to average it out to the conversion rate like and you do like a weighted conversion rate of all the operating systems it might not equal the overall because of this additive versus non- additive property of ratio metrics and like of certain Dimensions like operating system like interface there's there's a couple of them that are like that we talked about that earlier in uh previous lecture but so ratio metrics are all about you know indicating quality um okay so percentile metrics percentile metrics are very useful so when I worked at Netflix uh one of the things that I worked on was reliability of like up time one of the things we we prided ourselves in Netflix was we wanted to have 99.99% uptime so there was only like there was like 25 minutes a year that you get where Netflix can be down and we were trying our best to reach that and we did a pretty good job so let's talk about the the differences here of like what okay so if you had a metric that's called P99 latency what that means is okay when our website loads for the slowest 1% how fast is the website so it's like so it's like essentially it's it's asking like how fast is our worst or how fast is our worst experience and so it's really important if you're trying to optimize the tals your distribution and capture more audience and capture more share I know that this these types of metrics were also very very important at Facebook so one of the things when I was working at Facebook was there was a big Push To Go Global a big push to get people across the entire world and one of the things that we realized was across the entire world like people not everyone had iPhones not everyone had or Samsung Galaxy s37 or whatever like they all had like most people in the world like their phones are kind of low budget and one of the things that we realized looking at some of these latency metrics and some of these uh engagement metrics for people in these developing countries was that they their phones couldn't handle Facebook because Facebook was like either too data intensive too Ram intensive or like there was just so many different like things like the the hardware of their phones just couldn't handle it so what these metrics helped leadership understand was that they needed to make another version of Facebook and that's what they did they made another version of Facebook called Facebook light that was uh it took it used I think like 20 or 20% the same amount of data as regular Facebook and the install used like it was a 90% smaller install and it also used a lot less Ram it was also stripped down obviously it's funny because like I remember at least like Facebook light when I was like I've actually used FS but like to be fair and I actually quite like to um I think I prer it as well like I remember like because it it was a lot bit more basic and it's a lot less like um am I am yeah I I actually I forgot about Facebook like but I used to I used to use it on my old pH and then I got a new phone and and when I parted the apps over I didn't part that one over uh so I might I might get that back because I actually preferred it like using Facebook light for a while I'm not sure if they have it still i' have to check I shifted to using Facebook light for a while because it was like a worse experience and I'm like I you know I I'm already chronically on social media and I want to like not be on social media so much so I like to downgrade my experience to make it a little bit less addictive yeah that's the the same reason that I started using it to be fair like because they make the experience really enjoyable and really easy to use on regular Facebook and then um yeah you you could you can just end up Doom scrolling and stuff like that so anything that stops you doing that it's why I um for for a long time I uninstalled things like uh Instagram as well but uh anyways it's funny because like Facebook light back then really reminds me of like threads now I'm like are they just making the same app again like like what is going on with Facebook but anyways back to percentile metrics like because Facebook light was a massive success like it got like hundreds of millions of users and it was able to it was able to get that tail end of users who were not on good enough Hardware to enjoy regular Facebook app so you can have you can have a you can look at the tail on that side but you can also look at the tail on the other side of like okay P10 engagement of active users so in this case like what are like what are we doing for people who are like lurking on Facebook for the people who are like they show up they scroll the feed but they never send any messages they never like any posts they just like scroll Facebook and one of the things that uh there's been studies that show this that like the how you use social media makes a big difference on how it impacts your mental health and for example if you're like a a lurker and all you do is passively consume then it impacts your mental health a lot more than if you're a Creator and engaging and being and being more social like if you if you're more social on social media it's not as negative to your mental health so that's another great example of like okay what about uh if we're looking at the engagement metrics for those bottom like lurker people like what's going on with them and like how how can we make maybe make those metrics better what what experiments could we run to try to engage the least engaged people so that's interesting that um just thinking about some some um so want to know um and I can definitely see that these metrics matter a lot because they they are looking at the tals this is where you're going to have a lot of your incremental impact because of the fact that you're I feel like I'm getting sucked in into like story here rather than the actual like content of of um of of the examples he's given me uh so I'm like getting sucked into the fact that oh yeah you know that's that's what happens at Facebook and oh that's really interesting interesting and now I really want to Google um the the the history of that and the information about that uh yeah average user like like for example Facebook's average user like is not going to leave they're just gonna stay because of the fact that they are no lul come here and uh they're just going to stay and because like they're in the medy part of the bail curve and they like their needs are already being satisfied and they already are having a pretty good experience and so that's where you really do want to look at these kind of tale metrics because that's where you're going to find people who you can actually make their experence and uh help them come back and engage more so that's how percentile metrics work as a DAT engineer like you should probably not be passing these metrics to your data scientist you should be passing them the the daily Aggregates and then the data scientist people can do their percentile magic and stuff like that on the data on top of that uh they can plug it into their experimentation Frameworks All That Jazz this like this makes me really torn as well like because there's um there's a there's a DAT a platform track on the uh the paid version of of the boot camp and there's a a non um there's an analytics track as well but and I always thought okay I'll probably go the DAT platform uh track because I find that a lot more interesting but actually you know doing that kind of stuff is also very interesting to me okay shifting gears here a little bit so metrics can be gained especially with experimentation so experiments can move metrics up shortterm but down longterm I remember one of the things that happened when I first worked at Facebook there was an experiment that ran that was like what happens if we 10x the notification volume in a small subset of the Facebook users and then they noticed like oh wow if we 10x the volume of notifications we get more growth and they called it like blasts like notification blasts and then they were like wow we should probably do this more often and I'm I'm sure a lot of y'all on this call are like no please don't like please I I will turn you off I will silence you don't do it and so like and that could be a big thing that can happen where you can do these short-term like gimmicks and games and all sorts of things that can uh still like make your numbers and make your metrics go up because of like some sort of like shortterm for some shortterm reason but then it's not like helping you look at the long term so the big one for me right was in notifications you send more you get more us you get more users in the short term in the long term you lose that handle you lose that lever because what happens is more people turn off settings and then you can't access those users anymore so what you need to do is you need to create other metrics that can help you figure this stuff out like for example um like notifications I created the metric called reachability which was the percent of users that we can send a notification to so then when they run experiments where they blast all these users they can see that it's actually like detrimental so that made it so that and then we had a rule in notifications after that metric came online where the only experiments that got shipped were experiments that increased growth and either were neutral or increased reachability they had to have both of them going up otherwise like it was going to be an experiment that was uh considered not successful so that's a great point when I was talking about earlier like why do metrics matter didn't those cases like it makes it so that you're not flying in the dark where you're like wow uh like you see now we can see that we're we're making notifications better increasing growth and they're more relevant because fewer people are turning off their settings so then you can actually get like a a win-win you can set up your experiments to be win-win not just like gimmicks and games and so other things you can do right with metrics right is you can you can fiddle with stuff you can fiddle with the numerator or the the denominator where it's like oh what metrics are you looking at um for example like you could say like I know a lot of them was like uh engagement per uh daily active user or engagement per month active user and you can look at these different kind of uh crosssections of different users and I I always thought it was interesting because like that was one of the one of our goals uh when I was working there was I worked a little bit more on like email and SMS notifications which are used more to bring people back on like a monthly basis than on a daily basis for Facebook and so it's like okay but if we look at the engagement per daily active user then the metric the experiment might look like a success but that wasn't our goal to begin with because we're looking we're trying to boost the engagement of monthly active users where the effect might be uh more diminished because of which people are in which populations and so you can you can fiddle with the numerator and denominator of your metrics and like you can essentially like get experiments to tell you whatever you want so uh that's where you want to be very careful with doing stuff like that and not going too deep into like fiddling with stuff and like really having clear hypotheses when you are on the out like when you're starting your experiment and that you can test those specifically and look at those metrics specifically so that you aren't like trying just trying to I don't know if youall ever heard of packing packing is a a great term that I recommend y'all look up and it's it's essentially a way to make experiments kind of show uh significance in whichever way you want them to show significance or not and so then other things um you have a novelty effects so for example if you introduce a new feature or you introduce new things then users will uh generally speaking they're gonna uh be excited by this difference if you add a new tab you freaking uh change your notifications I know this was an interesting one that uh that I I saw was like if you added an emoji to Notifications uh there was like this like at the beginning of the experiment there was this massive lift it was like a 15 or 20% lift in a notification conversion rate if you added an emoji and people were like in the notification team like that first week we thought we had like we were all going to get promoted and like oh my God like we just we just struggled we just had to put a freaking smiley face in the notification and who would have thought right oh my God right but then like as the experiment kept running uh we noticed that that uh the the lift that we saw at the beginning ended up starting to die down as the novelty of the new feature wore off so and emojis were not the slam dunk to notification conversion that we thought they were and so that can happen though and that's why like you need to be aware when you're running experiments for how long your experiment should run um another sorry this this is just um this I'm just thinking about features like new features that that um that companies bring in and you know what I'm really really happy about that got removed you remember like in LinkedIn when you had those bloody AI summaries and yeah you just got AI summaries on every single person on your feed I'm so happy that they got rid of that that was the most annoying thing in the world it just yeah I don't know why it just came into my head when I was thinking about features here I was thinking about notifications notifications then went into thinking about uh yeah LinkedIn um notifications are really bad and then that went into oh yeah and other LinkedIn bad bad features which made me think about LinkedIn um AI movies another example I had was when I was working at Netflix they were doing these feed refreshes where they were trying to see like if we refresh the uh the movie feed uh uh in the background while someone's not using the app and we do a background refresh does it increase retention because then the movie feed is as fresh as possible and they tested it out like at different intervals like do we refresh every eight hours 12 hours 24 hours right different like intervals and they noticed like the more often they refreshed uh the more retention they got which was great but like what about the downside of that like if you're only caring about retention then like obviously you would pick the the the most frequently refreshing uh experience but then it's like if that adds millions and millions of dollars to your AWS bill because you're just you know causing all all sorts of additional requests then that might not be the right play so another thing to think about uh that's why you need counter metrics and that's what I did at at Netflix was I added another counter metric which was the AWS cost of uh of an AB test so then they could look and they could see like then they could balance it out they can then do an ROI analysis of like we get this much increased retention at this cost because a lot of times like you get this thing called diminishing returns where like you do get a little bit more value as like you keep uh increasing that refresh but it costs substantially more for every uh interval at some point you get this like nice little Parabola effect and um and so well no I mean a logistic effect where you hit that kind of like carrying Capac cap where you have to increase the cost a lot more to get a smaller and smaller benefit so metric be game we're going to talk a lot more about this on Wednesday as well okay so now we're g yeah so um so I think we mentioned that earlier like with the scenario of okay you could have kpi that says you want to get loads of sales like you know you want to hit sales targets but then okay well you have a counter metric on making sure that things are um all all your projects have detailed Scopes like you know bit of a bit of a bad example because you know how what what do you define as a as a as a detailed scope um but um but you get the gist off like they have very different um they have they counter each other and so you end up someone not gaming the system by doing only focusing on one kind of go back to the fundamentals here of like how does an experiment work so there's really four pieces to uh an experiment this is very similar to the scientific method if yall remember that from school uh your first step is make a hypothesis then you want to do group assignment you can think of that as like test control you might have multiple test groups there could be two or three or four test groups as well and then you want to collect data and then you want to look at the differences between the groups so that's going to be the main things and we're going to go over each one of these in detail so that y'all can be ready when you are making your own experiments okay so hypothesis testing uh the null hypo so hypo hypothesis testing is where you have What's called the null hypothesis and and then you have the alternative hypothesis and the null hypothesis is always the same which essentially says there is no difference between test and control there's no statistical difference like making this change will have no statistical difference and the AL the alternative hypothesis is there is a significant difference from the changes that you introduce like the test and control cells so those are the two um types of um ways that or two hypothesis and these are like you can see how these are like only one can be accepted or and and so let's go into a little bit more details around this because this is an important kind of nuance so you never prove the alternative hypothesis right so in this case you can reject you reject or fail to reject the null so in the case when there is like there isn't a statistical difference you fail to reject the null hypothesis which says that there is none but if there is a statistically significant difference you reject the null hypothesis because your alternative hypothesis might not be right because it might have more details in like what you think is like the reasoning behind it and it's it's similar in science how like you know you have like hypotheses and then you have like theorems and you only really get to that proof level when you have like a massive body of work right and so you don't really prove hypothesis you just support them and that's going to be a key thing to remember here because of the fact that you can have all sorts of other things that could be going on so you want to always be that kind of like hesitant kind of handwavy data scientist in that case Okay so group testing uh this is an important Point uh this is essentially how we assign test and control or uh to our group members or and in this case our group members are so I'd say like hypothesis testing is probably like one of my like most difficult areas like or things that I struggle with like we had to do we had to do in hypothesis before for one of our customers um which was to prove whether it was it was to do with semantic analysis and whether or not we could determine um like whether or not we could get like we could get the level of like semantic analysis that they need about using open AI um and and because it's it's all experimentation because there's no clear outcome yeah I find that that's so quite difficult because you don't have a clear goal in mind you are just trying things and seeing what the results are and then observing them our users and then we got to ask a bunch of questions so one of the big questions to ask is are these users in a long-term holdout like for example at Facebook when I worked in notifications they had this thing called long-term holdouts which is they wanted to measure the effectiveness of notifications as a product and the way they did that was they took a small percentage of Facebook users who we just never sent them notifications ever they got no notifications it's like a small percentage of users get nothing and the reason why we do that is that we can compare the growth of that group to the grow the growth of everybody else and then we can see the impact that notification has on a percentage basis and that like incremental growth comparing those two groups was a very critical metric for Facebook but with that being said if the user is in a long-term holdout and I'm doing a notification experiment that user is not eligible for that experiment because they can't get notifications so you can have these long-term holdouts that are very important because they're like long running experiments and you want long-term holdouts because they measure the effectiveness of various parts of your application and and the health of those parts of the application because you'll have users who don't have that feature and then you can see like oh are these users like like how many more users are we getting back because of this feature and that could be huge as well and so like make sure that like that's the first thing you want to test is like are they in a long-term holdout another thing to test is like you might be doing an experiment that is geographic so maybe I'm doing like a sale in the US and then it's like okay are the are are my users in India eligible no because it's a sale in the US so like uh a lot of times your experiment groups are not 100% of your users it can be some fraction of your users based on different criteria that you have for them another one might be like oh I want to do an experiment on Chrome to see uh if these users have this bookmark installed or to have this thing installed on or like a an extension installed or different things like that like and then then then your population is just people who use Chrome not everybody so you can have experiments that have all sorts of different like criteria to be involved in and we're going to go over how to set some of that stuff up in stat Sig uh today in the lab and then another one is like what percentage of users do you want to experiment on because another thing is is like what if your experiment sucks what if your experiment has a lot of downside to it you don't know and that's where it's like if you're a big company like Facebook and Google like you can actually really limit the the downsize impact where it's like you might start with like one% of users or half a percent of users and then if the experiment shows promise then you open it up to a wider uh number of users to see like how the power and like the statistical trends that they'll probably stay the same because you have a a strong enough statistical sample but they they could shift as well so so that's where like what percent of users do you want to experiment on uh for for my uh experiments that I'm running on my website I just do 100% because I don't have 10 trillion users like Facebook and Google so I for me to get enough statistical power out of my tests and out of my experiments I need pretty much every user I can get so that's where it's important to look at what the differences are there and like how big Tech versus a smaller company is going to do experiments and a big one here is going to be the percent allocation because big Tech gets that luxury where they can experiment on a smaller percentage of users and see what happens before they open it up to everybody else whereas at a smaller company if you did that you're going to get like two users and it's like okay um you don't have enough statistical power to prove that there was an impact or not so you want to make sure that your tests are powered we're going to talk more about that in another slide here okay so let's talk about group assignment so there's uh two ways that you can yeah I think we stuff like that like um you you can you can do a little bit of testing in terms of um in in Zach example for example you could do like testing on um on sign up like if if the like way that the the like marketing like if if he did ad adverse for example um if the way that like he did the marketing affected the um conversion rate but um but yeah probably not directly on the website possibly but uh though I do know people I do know of like companies who um kind of star like where where are people getting to before they drop off like if there's like a particular page and how do they make it more seamless to get them to sign up to second headphones have decided to disconnect they just decided to turn off like in uh in an experiment the first question is are you logged in and if you're logged in you can have a logged in experience and uh and your your user ID for your app will be the identifier for that user and logged in exper experiments are more powerful than logged out ones in some regards because of the fact that you have a lot more information on your logged in users right like for example pretty much everyone in this call filled out that intake form so I know all like the time zone that y'all are all in I know all sorts of different things about y'all right and so if I run experiments on exactly.com that are logged in experim experiments I can use that and assign groups and bring in more data and richer data about y'all because I have so many more like mions and different things on like how y'all interact with my website but on the flip side like uh you also probably want to do loged out experiments to see how people convert so that you can get customers and usually speaking if you're going to be doing a logged out experiment you're going to want to use one of two identifiers like you can either use uh an IP identifier but you want to Hash it you hash your IP identifier then that will uh be a way to get someone to kind of have a stable identification the problem there though is like if someone's on like they're visiting your website and they're on mobile and then they go from uh mobile to Wi-Fi back to mobile then that's going to be uh treat as two different users even though it was one user that's where stat Sig is pretty cool they have this other thing called a stable device ID which is going to be a more consistent way to hold on to users that you can also use they uh they have their different trade-offs and benefits right and the stable device ID can also be pretty powerful as well so uh and also know that like the if you use IP another thing that can happen is if you have two people who are two people on the same Wi-Fi that are visiting uh a certain website they're going to have the same they're going to be the same user so you also get the you can have one user become two but you can also have two users become one and so you can actually have both experiences happen like when if you use just IP and so that's where you can that being said IP is also very simple to use and actually in the lab that's what we're going to be using so what happens is you pass your identifiers to stattic stat Sig will then say okay the server use the blue send the blue environment or the red environment so you can think of like the different colors as different experiences that users can uh get and statti will essentially tell the server that and then the server will send to the client the the the changed view like whether it's a blue button or a red button or you you you say like data engineering Academy versus data engineering course or different things like that you change the words up so then you can measure the impact of your experiment experiments over the long term and another big important thing that you have to do is you need to track your events so if I send a client with a blue button I need to track all the events like so that I know that this like they got the blue button and then they signed up and then they bought from me right I have the whole funnel of events that they did uh and so those events can happen either on the client or on the server depending on where you want to do your logging and Stat Sig offers apis for both right you can have a client you're going to do client Side Event tracking or server side um for the lab today we're going to do all server side because it's simpler um so this is kind of the idea behind how to run experiments there is one one piece in here that is missing and that is what about other metrics that that aren't logged on stat Sig but you want to add like for example you might have a user pipeline that you want to add a bunch of other extra Dimensions to so you can have like you can imagine like a separate ETL process that dumps into St stat as well that gives you like user level dimensions and metrics as well that you can also add so uh and we'll talk a little bit more about at the very end of this presentation okay so that's the idea behind group assignment that's how they get into tested control it's based on those identifiers then you collect data you see okay you collect data for a while until you get a a statistically significant result generally speaking in big Tech that meant you waited at least a month so you held you ran your experiment for a month to see what was going to happen so and what I like what like I was saying in the previous slide you want to use stable identifiers uh whether that be your IP address IP address or a stable ID uh in statti and then there's also um you can also use uh your user ID like your logged in user ID if you're doing a logged in experiment we're going to be doing a logged out experiment in the lab today because setting up logged in would be just a lot of work so collect a lot of data until you get a until you get a statistically significant result but keep in mind you might not especially if uh you're looking for either a very high level of significance or the change is very nuanced and there actually isn't a statistically significant result generally speaking the longer you collect data the more likely you're going to get a statistically significant result okay so how it works right is the smaller the effect the longer you're going to have to wait so if there's a big change and like one one like uh one version of your test is just dramatically better than other versions then great awesome and then uh you'll your statistically significant result very quickly and that could be a very it can be a good feeling too because you're like oh awesome like my hypothesis was correct and I got my data really quickly and that can often times be a sign that you have like a slam dunk experiment and uh on the flip side like if it's been a month and you still don't have a statistically significant result it might not ever be right or it might it might the effect might be so small that like it doesn't matter so keep it keep that in mind when you're like collecting data uh another big thing do not underpower your experiments So today we're going to be talking about how to have multiple test cells so you can have test cells one two and three and then your control cell you have different test cells and uh the problem is like if you have too many test cells then you're going to run into problems because you're going to have underpowered because imagine if you had 10 test cells and that means that like each cell is only going to get like 10% of the data and if each cell only gets 10% of the data then in order to get enough data to to measure if there's a statistically significant result is very high and you don't want to do that because you if you're working in a startup you definitely don't want to do that because you don't have enough data Google is a little bit different right they they they ran an experiment where they tested 41 different shades of blue and they were able to send like 2 and a half% of the traffic to each shade of blue and then they were able to get enough data back on that and they didn't have to wait a million years and the reason why they didn't have to wait a million years is because they just have an insane amount of data at their disposal and they can do a lot more experiments that's one of the things that's beautiful about having a lot of data is that you can run experiments a lot quicker because you have like just more incoming uh input from people and remember this this is critical is that like when before you start your experiment you want to make sure that you have logging set up for all of your events and dimensions you want to measure so that you can check and like you don't have to be like oh halfway through the experiment be like oh man we forgot log this other metric and now we and then because then you're going to miss out on a lot and then it's going to mess up the the differences because like you're going to have like a skewed experiment where half the people were in the experiment and you didn't log the metric and then the other half you loged it halfway through and it's a problem so that's a big important thing to remember before launching experiments is make sure you have all your logging in place okay now we're getting into the cool parts of this so you want to look at the data and here's an example of an experiment that I'm running on my website on exactly.com and you're gonna when you're running an experiment you're going to look and you're going to get charts like this that are different metrics and you can see like the statistical significance oh great I'm glad I'm glad they helped um yeah hopefully um they continue to help like I think a lot of people are getting value from the homework ones so uh and I'm glad I'm glad that they're getting value from the homework ones because uh the the homeworks can be quite difficult so hopefully um people are finding them you still uh like kind of day over day or like the different this is not this is just like comparing test to control so in this experiment uh I'm doing Red versus Blue buttons and you'll see if if there's if there's a bar that overlaps zero so if you have negative and positive options then it's not a statistically significant result so the only like the only metric that actually makes a difference here is whether or not someone visits the signup page and you'll see that I mean there's not a lot of data here yet and that's why these uh the air bars are so freaking huge so you'll see in this case this uh the impact here is 21% plus or minus 20% so that means that like the Red versus Blue Button could be it could impact signups between one you get impact sign between 1% and 41% that's like the the range of like what could possibly be here keeping in mind that I've only been wunning this experiment for two days and the confidence interval right you see the confidence interval here's only 80% which is a pretty weak confidence interval that's not the industry standard in data science I would like so technically like if you use industry standard this experiment shows that like whether I use a red button or a blue button there's no statistically uh significant difference so uh let me show you that like as we talk more about P values so in this case the P value would be2 which is a very high P value um that's what we want to look at so values so generally speaking P values are uh the the standard one is .5 so what that says is there's a 90 if the P value is less than .5 that means that there's a 95% chance that the effect that we're looking at is not due to chance and is due to some other Factor right and it's not due to Randomness and uh so in the last slide you see when it's like this 80% confidence interval that means that for for this metric this visited sign up the one that's statistically significant down here that means that there's an 80% chance that this is not random but a 20% chance that it is random so keeping that in mind when you're like looking at confidence intervals and the the lower the P value the higher certainty you have that the randomness is uh it's not due to Randomness but due to some sort of underlying change so um P values are very important if you haven't looked at them I highly recommend looking into them they're a statistical tool that are used to pick out change and they're one of the most important things when running an experiment to determine whether or not we can say that there was an actual change or not okay H but just because something is statistically significant doesn't mean that it's it's valuable uh and one of the reasons that is the case down here that means that there's an 80% chance that this is not random but a 20% chance that it is random so keeping that in mind when you're like looking at competence intervals and the the lower the P value the higher certainty you have that the randomness is uh it's not due to Randomness but due to some sort of underlying change so um P values are very important if you haven't looked at them I highly recommend looking into them they're a statistical tool that are used to pick out change and they're one of the most important things when running an experiment to determine whether or not we can say that there was an actual change or not okay H but just because something is statistically significant doesn't mean that it's a it's valuable uh and one of the reasons that is the case is the statistically significant change might be tiny like for example in that last slide you saw how like I was like oh there might be a 1% lift if I use a red button instead of a blue button it's like it's like a 1% lift is that even like is that I mean it's valuable but like what if it's like a 0.1% lift then it's like okay that's essentially the same even though there is a a statistically significant difference it's the same and that can happen especially for experiments that are running for a long time because as you collect more and more data more and more uh metrics the differences in them will be become statistically significant but the the Deltas might be tiny and if they're tiny like they're probably worthless so that's a key thing to remember about statistical significance okay so I want to talk about some other important gotas with statistical significance um if you have tested control uh here's an example for you like so say you're measuring notifications received and am I also doing the paid boot camp I am but I am like I'm not really started a lot of it yet so I've started like watching the labs um so um I'm on the first lab lecture at the moment uh but it's really hard to do it because I'm doing this as well and so it's a bit disorientating going from this to going to um to the other one I found it quite hard to um when I'm working in the same area to to do that kind of Conta switching uh but yeah I'm doing that in the day though because I can't exactly film that so yeah like that's your uh that's the metric you're looking at in your experiment and uh in the test group you have a bunch of average people and then in the control group you have like Beyonce and you have uh Justin Bieber and uh and then a bunch of also a bunch of you know average people if you look at notifications received Beyonce and Bieber are probably GNA have a few more notifications than everybody else and that's going to really skew the average up a lot where it's kind of like how you know in a room of me uh and uh Warren Buffett the average between us is a billionaire even though I'm pulling him down a lot he just has so much money that it doesn't matter and so it's the same idea here where if you have extreme outliers you want to be careful and uh one of the things you can do is it's a technique called winsorization so if the outliers are so extreme what you can do is you can clip them to a less extreme value and generally speaking that's like the 99.9th percentile is what you want that's what the like the standard kind of winsorization is so that like if you have like just very very extreme outliers that will make it so that you don't have as much skew but another thing you can do is instead of looking at um event counts look at user counts because that can uh but it depends on like what you're trying to measure but like a lot of times user counts is going to give you a better view because in that case it's like okay uh Bieber and uh Beyonce both received notification so they count as one and I reive that notification so I'm at the same level as Bieber and Beyonce right so um anyways that's kind of the idea when you're looking at statistical significance remember that skew can cause problems uh winsorization is often built into different experimentation platforms to help uh avoid this problem but it is a technique that you should be aware of okay we are getting real close so so statsd can create metrics we're going to go over how they're logged and all sorts of things um in uh in the lab today and uh how about adding your own so this is a very common problem and this is a big problem that I did at Facebook was like uh as a data engineer you often are going to be adding new metrics into experimentation platforms so that data scientists can look at how they change via experimentation and so that's a big thing to remember as like uh you're building stuff out is that like a lot of these daily metrics and daily Dimensions or slowly changing dimensions are going to be added into your experimentation platform so that data scientists can go nuts to see like if there's like uh any dimensional impact on an experiment or like if this metric is important or what counter metrics you need so very common very common thing I've done this that literally every company I've worked for in Big Tech congrats on getting to the end of the day one lecture if you're taking this class for credit make sure to switch to the other tabs so you get credit for inside of you check out the lab hi Carlos how you doing oh nice I'll um I'll check that out but awesome that's great that's great to see um uh yeah I've paused there because I think I'm going to end here tonight it's quite late um and I had to start a little bit later than I initially planned because my little boy decided that he didn't want to go to bed uh and he decided to steal a pack of Kelly Welles and eat them in bed whilst we were in the living room and he just decided to to munch on them turn the TV on and St trying to watch the TV um living his best life so so had to start a bit later because he wouldn't go to sleep he's so funny he's so funny um but yeah but yeah that was um that that's been my night but um hopefully he will go to bed earlier tomorrow so we'll see it's one of those things as well like you like he's done some like what he shouldn't do like stealing a bag of sweets and eating them in in and just trying to watch the TV in our room while we're downstairs but like because it's just so funny you can't like be you can't be mad at him because you're just like but yeah anyway um I'm going to end that because it's been a long day but I think tomorrow is going to be more productive I've got good I've got good amount of time tomorrow so um Yeah tomorrow's going to be a good day right I'm going to end there thank you for those who have watched and I'll see you next time


 > [!info]
> - **0:43** Changes made to homework week 4, added comments, made it dynamic (not hardcoded).
> - **3:52** The LLM marked down for not being dynamic, added a way to get the latest year that hasn't yet been changed.
> - **7:27** Storing whether or not the current year is active is a useful tool for logic within casing.
> - **11:30** Thinking like a product manager is critical for being a good data engineer, build pipelines that impact the business, build good metrics, understand how experiments work.
> - **12:10** Metrics are important, but it depends on the company culture set by the founder (e.g., data-driven vs. design-focused).
> - **13:57** Depending on the company, metrics vs. storytelling can tell you a lot about the company.
> - **15:15** Metrics inform data modeling, funky metrics can be hard to create a data table for.
> - **16:02** Metrics can't be gamed, conflicting KPIs are needed so people don't only focus on one area.
> - **19:15** Push back on complex metrics requested by data scientists; give raw aggregates instead.
> - **20:38** Supply aggregates and counts, usually at the entity grain.
> - **21:31** If you define a metric, be able to answer questions about it; keep them simple.
> - **23:32** Metrics based on number of people vs. number of actions are important.
> - **24:30** Give analytics partners simple metrics.
> - **25:37** Important to help analysts get answers and not just lob things over the fence
> - **33:34** As a data engineer, pass daily aggregates to data scientists for percentile magic.
> - **34:23** Metrics can be gamed, experiments can move metrics up short term but down long term.
> - **35:20** Create other metrics to help figure things out, otherwise, you lose that lever.
> - **37:11** Have clear hypotheses when starting experiments.
> - **37:28** Be aware of novelty effects; run experiments for the appropriate length of time.
> - **39:56** Need counter metrics, do ROI analysis.
> - **41:22** How does an experiment work? Make a hypothesis, group assignment, collect data, look at the differences between the groups.
> - **41:52** Hypothesis testing: Null hypothesis (no difference), alternative hypothesis (significant difference).
> - **42:24** You never prove the alternative hypothesis, you reject or fail to reject the null.
> - **44:11** Are users in a long-term holdout, is the experiment geographic, what percentage of users do you want to experiment on?
> - **47:03** Group assignment: Logged in vs. logged out experiments.
> - **51:01** You need to track your events.
> - **52:24** Collect data until you get a statistically significant result.
> - **52:36** May not get a statistically significant result, the effect might be so small that it doesn't matter.
> - **53:16** Do not underpower your experiments, before you start the experiment make sure you have logging set up for all of your events and dimensions you want to measure.
> - **57:14** P values are important, statistical tool that are used to pick out change and they're one of the most important things when running an experiment to determine whether or not we can say that there was an actual change or not.
> - **57:56** Just because something is statistically significant doesn't mean that it's valuable.
> - **59:38** If you have extreme outliers, you want to be careful, winsorization is a technique to clip them to a less extreme value.
> - **1:00:49** As a data engineer, you're often going to be adding new metrics into experimentation platforms.
> 