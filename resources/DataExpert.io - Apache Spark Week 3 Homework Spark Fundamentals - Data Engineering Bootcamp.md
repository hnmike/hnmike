---
title: "DataExpert.io - Apache Spark Week 3 Homework Spark Fundamentals - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2024-12-17
source: "https://www.youtube.com/watch?v=wNQHWdCC6O4&t=681s"
image: "https://i.ytimg.com/vi/wNQHWdCC6O4/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGDcgUChyMA8=&rs=AOn4CLCRBQCoH59Hx7CaRcBxJKCOEJewBw"
created: 2025-03-23
tags:
  - "youtube"
  - "Data_Engineering"
  - "Apache_Spark"
  - "Data_Engineering_Bootcamp"
summary: "Apache Spark Week 3 Homework: Spark Fundamentals from Data Engineering Bootcamp. Learn to disable broadcast joins, use bucket joins, and optimize data size."
---
# DataExpert.io - Apache Spark Week 3 Homework Spark Fundamentals - Data Engineering Bootcamp

![DataExpert.io - Apache Spark Week 3 Homework Spark Fundamentals - Data Engineering Bootcamp](https://www.youtube.com/embed/wNQHWdCC6O4&t=681s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> for for e for for everyone how's everyone doing today well welcome to week three of the homework this SP fundamentals homework funny story I um I got this homework completed like Friday but I only got B and I'm I'm one of those people who I hit getting like anything less than an A so like I was like right well I can't I can't do it until I've got an A so um I've spent like like the week weekend like optimizing what I did so that I could get an A um it was uh yeah it was a few different challenges that I'll go through but um one of the ones that I found quite um difficult was the fact that or like one of the things I thought was quite odd was um a lot of the learning was done in Java in in in SC Scala and then all of this was done in has to be done in Python and so that was one of the things so I initially did some of this I initially did it in Scala I know it says python here but then like loads of other people do were saying to do it in Scala so I started doing it in Scala and then when I submitted it it was like nope you can't do it in Scala it needs to be in Python and so um I had to switch to python but yeah it's been an interesting one this week um I feel like um it was yeah it was it was an odd it was an odd week but um yeah i' I've only done the first homework for this week I've got the homework testing to do still as well but I think that's going to be a little bit easier for me just because I'm a bit more if if there's anything that like I'd says as my skill set tends to be um the the testing so that's that's what I did so yeah you'll see I went to if we go to dat expert here into my assignments you can see I got my little a took off so so I like my little a there makes me happy yeah so okay why is why is my screen so dark like let me put let me make myself let there be light one two three \[Music\] go that's better so what we're going to do is we are going to have a look at what the things are that we need to do so I'm just going to delete my little Warehouse here because that was from before and then um yeah so basically um first thing that we need to do is uh have a look at our little tables that we've got so we have uh if we go into our data folder we've got um these these tables here so match details matches got matches which is match ID map ID is Team game playlist ID Etc completion date then we've got match details which is match Ed play game tag gamer tag um and a few other ones so uh player total um kills Etc so there's a few different options in there and then these are the different these are the rows that we need to basically are the tables that we need to change so we've got medals matches players Maps matches and match details so these are the ones that we're going to be working with today so the first um the first part of this question is uh we need to build a spark job that disables automatic broadcast joins with the spark conf do set to be um minus one uh and that's basically just going to disable the broadcast join automatically and that means that we have to then manually specify the broadcast join um so that's the third thing that that they want us to do the second thing that they want us to do is explicitly broadcast join medals and medals and maps and so um what that means is it wants us to do uh a broadcast join on for the maps we want to match the matches to Maps so if we go into matches here we've got this map ID that we can map to we also in our um in our medals we can map this medal to the medal matches players here um which is is here here uh and then we can also do uh match ID we need to then do a bucket join on those things to do match details matches metal matches players match ID with 16 buckets and so um they're the like first things that we need to do we need to do broadcast joins I booket joins um and we need to disable the broadcast joint for the initial one so the first thing that I did was did the disabling so um I'm just going to I just created a file um match job Umi you can like um separate this out into separate ones if you want to because I was just going to be submitting um like all together I just decided to do it all together for mine but um the first thing I did I was just going to define a main um like folder and within that I specify my spark config session another them and within this we um we need to import spark as well so the command for spark is these two things so um I don't know why this does run like this does recognize p p SP fine but like for some reason it just decides that it it doesn't recognize it I think it's because I'm using my I am using a a virtual environment and because vs code isn't inside that virtual environment I um I it doesn't recognize this but because everything that I want is in my my virtual environment it it recognizes it so I don't need to you don't need to worry about that um it it's fine um so if we just go to um the boot camp then now in our virtual environment and this will allow us to inspark offline um so um I'll I'll do the commands basically to to just get our spark winning and and make sure that it is all happy um but the first thing that I did is just get the config set up so um call it uh thingy so we got our SP session built and the next thing we want to do is do our um we want to um set our broadcast join special to be minus one so you can do it like you can do that you can just copy this and do that you can also do this like here you can do do config do config and then you can do um spark do SQL do auto broadcast join threshold to make it happy there and that will like um that will give you a much it just I think it looks neater anyway I think it looks nicer so so you can do that basically and then once we've done that we've got this is all all all set up we can then just do our initial like um like broadcast join so I just created a separate method here for doing my broadcast join for the um match details so that that's the first one that I uh no that's not what I do explicitly broadcast join medals and Maps so um broadcast join and then we can what we can do is we can read through our like csb files so we can do um like and you can see like coil is giving me some bad suggestions here so um so my data is here so this is up to level so job source to data um so that's why I've done the um this here just so that we're going up that level to to get to the previous level and then we can do the same thing for matches um for maps sorry and that that'll give us that same um like it'll read it in and so if we just do here if we do matches uh yeah do show um and the same thing for maps as well and we do uh just go into jobs \[Music\] here and then we do Python 3 um match stat job oh we need to actually call this method as well so you can see now it's initial initi initiating Spar and you can see here that this is Reading in our data absolutely fine so we've got our um like match data and we've got our um map data and so once we've done that like you can see here that we've got our map ID there and our map ID there so we just need to join on those two Val just so that we can then say we've now got the broadcast join done and so you can see that like if we do um match M let's just do that let's count the number of maps and and count the number of matches you can see here like the number of matches is significantly higher than the number of M Maps which is why we do which is what like the broadcast J is doing basically so um so when we do a broadcast join we're basically joining a smaller table so we're just going to do a broadcast join on those two tables um now now that we've like read them both in so I'm just going to remove these dot shows as well because we don't need them anymore and then you can see here like um the um there's a you you can specify this thing called broadcast which allows you to do that broadcast join that we talked about so I'm just going to move inner as well let's do that so um this is a really annoying thing about this um course um like some of the ideas have um underscore and then some of them don't so you've got match ID which has an underscore but then Maps doesn't have an underscore same with match it like map look at that um so that's yeah yeah it's it's not fun so you can see now that that that's um like ring our tables and join them together um and so we're aome and so we can just return that now and we're going to do the exact same thing um so like one of the things that I had to like um do is part of the like feedback from the chat gbt was to write comments everywhere and I really don't like doing that because I think comments just make things a lot more like messy but that's what chat gbt wanted so is what it is and then we can do the same thing with um uh what was the next one that we needed to do medals so there's a slight difference here because um we don't want to be joining this too matches DF we want going to be joining this to the medal um matches players uh here cuz that's um that's the one with like um the relationship with medals so like if you go to matches for example you can see that there's nothing in here that mentions anything to do with medals so um that's only included in the middle matches here uh and so the I based on the limited information that we got on on this homework I assumed that we wanted to do the broadcast join with medal matches players instead of um with with matches so then you can do broadcast join and I love um I love um Co palette for stuff like this cuz it's so easy and you just want to match on um I'm going to do that way um and then I'm going to just verify that that's the correct yeah midal ID um I love like copilot and stuff like this CU it's just so um helpful because it just gets rid of so much BL Oiler plate also like I don't know if anyone's noticed but I've got like like area of like um of Parcels behind me because it's Christmas and they're all just like stuck in my office uh I need to call it again forgetting to do that that was another thing it's like some things are like yeah so like match details so that's like not got an s on it but then medals matches players has s's on them so I'm like okay well that's just me being again complaining about inconsistencies I'm such a um stickler for stuff like this again so you can see here that this is like happily and successfully join those two um two files and again I'm just going to do like a um printing of the count as well because that's always good to kind of validate like um know um one of the things that I found here was as well like it's so important just to like inspect the data as you're doing it like just to keep make sure that like you're getting the values that you want to to be getting again you can see like this is so small in comparison to this table okay and so we can do a return of this again so now we can basically do um broadcast that's got I don't know how to call these like I don't know what's the nicest way to call these either because you don't want to give them too big of names don't do you okay so that's this one done okay so the next one is um doing the bucket join for match details matches medal matches players on map ID with 16 buckets um and so this one was a fun one to figure out uh so I initially um submitted it with some values and and I and I added the sort within partition and the repartitioning in there like zck had done in his lectures and then I did the bucket Buy on the next line and the llm told me I shouldn't do that and I don't need to do that and then I resubmitted it without it doing those things and then it told me that I should do it so I was like what um but yeah um so from from from what I understand you should do it because you then sought the data in the partitions that you s you put it into but the llm was kind of having a bit of a moment and decided that I did not need to do that so yeah that was that was a an interesting one but yeah okay so death um and then we can do a bucket join um and then we can just look at join um for matches and then uh just going to get rid of that because lik to hallucinate a little bit then we can do a create a bucketed data frame um okay so the first thing I did was I specified the number of buckets that I wanted to create so the number of buckets that I wanted to create was 16 and then once I specified the number of um buckets I wanted to create I um what does it say create maps and matches create matches data frame um we partition and sort with partitions and then we can do that um I just did with match I do initially and then once we do that we can then do um matches um the right and I just did it as packet just because that's quite a common file format to do and then once I did that I did in here you can do um after you do the format you can do this bucket by which allows you to put specify the number of buckets that you want to book it by and then you can do um once once that's been done I then read the table so I can do um um uh either that I don't know I did um SQL let's call this matches \[Music\] bucketed and then let's do that do matches DF show I need to call it again I keep forgetting to do that one of the things that I found um was that it was sometimes diff like difficult to like um one of the things that I found was like this would sometimes error like saying that that the um the files already exist so I needed to I need to investigate a little bit more on why like that um that's not working this is where you have to like you can't just look at like what um co-pilot's given you and go on face value because it gets things wrong so many times so like that should be underscore match ID it's definitely useful though because like it can be really boring to have to like type everything out so again um so what we're saying is we're getting our matches dat frame and then we are repartitioning that that dat frame by the number of buckets on Match ID and then we're sorting within our partitions by match ID and then we also doing um a bucket by here which is number of buckets match ID mode overr right saice table um you could also Partition by um like completed date as well uh if you wanted to do it on a day byday thing that's generally recommended for um vol when you're working with a lot of data so that was another option that you could do so as you can see here that that's um that okay uh is that okay yeah okay so that's created the bucket tables and what I can show you in here as well is you can see all these files here that have been created based on the um on on this line here um so so that's just something to to be aware of that that's what this is doing \[Music\] um so once we do that we can do the same thing for booket join uh match details we don't care about that and then I'm just going to return this as well okay so let's just make sure that this is right so we're reading in our match details CSV file with then rep Partition by match ID we're sorting within our partitions by match ID we're doing a bucket by number of buckets which is match ID then we're doing mode over right savs table uh match match details book I did um then we can call that and so we read in our booket join for our match details for data frame yeah so this so this is what um I found before where like if it's already there it um doesn't like that it's already there what I'm going to do is I'm just going to Del it but I am going to Google um how to get this I'm doing that because it's very annoying for so what I can do is I can just do a drop table if exist there then let's do that now one thing I'm going to try as well so ideally if you're doing this in terms of um you already have the table there it's already existing then you don't need to worry about this because what you can do is you can do um an insert into instead uh so instead of doing save as table you can do insert into and then overwrite equals true um but because we not doing that in this particular one like we could probably create the table separately so um let me just try this now just going to figure out still need to figure this out basically but um I'm hoping that like Forever on in the other lessons we'll figure out more efficient ways to kind of insert data because this is really just this is this is okay for like oneoff data like for doing it for doing it in bulk but this isn't going to work when you need to do it like iteratively um but I suppose we've already been doing that in a separate thing so um so I I guess I can't to worry too much right now but um it's just something to think about that this isn't this is okay for like when you're literally doing a lift and shift from one area to another but this is not going to be able to be used like on a cumulative basis basically and we do the same thing for bucket join medal match choose players e you can see here that that's all fine um so that's done then the next thing we do once we've done all of that we then need to create our Aggregates of different data um so um I'm going to create a couple of methods def um get of it I'll just I'll just do it all in one for now so you can see what it is one thing that you might want to do as well is like split this out into Separate Tables so you'd have an aggregate table and you would have a buckets table and you would have a um you'd have a broadcast joins table just to make it all like separate but I'm just showing you the code of what I've done so um uh and to be fair I submitted it all in one file and and I still got like an NA so I think as long as the as long as it's readable as long as you're commenting fine then then I wouldn't be too worried so I got get average kills and match per player and the way that you want to do this is um if we look in our matches. CSV file you basically want to group by match ID and then byy uh so there isn't a player ID in here so what you need to do is um we first need to do our booking um which I just realized we haven't actually bucketed so um let we go here uh how did you approach Justus in just out of curiosity for then so before I um before I actually do the aggregation I need to do I need to booket join everything so what I did was um I read everything in so um and then I just did uh oh nice using clustered by match ID into 16 buckets yeah I um I tried to use the spark functions as much as I could so we can do a so this just matches this just joins all of the tables together basically the only other thing that I did as well actually is um I joined the medals matches players on Match ID and on player gag so that we didn't get duplicates too bad I couldn't use CA I know to be fair though I use Python quite a lot these days so um I guess I can't go play too much I'm wondering if um this is not completing quick enough for I know why I've removed the um SQL thing how did I move how did I remove that for that is the same name isn't it for for just trying something here very weird still need to figure out how to nicely remove that this these tables you can you kind of see they're all here but I'll just delete them manually for now okay if anyone's found a nice way to do that let me know but um yeah I'm not too worried I keep misspelling this this thing keep putting an e on the end of it I don't know why like I don't know why like nothing ever is spelled gamer tag with an e at the end get a little heat memory here so that's all bucketed and now we can get our Aggregates so so the first thing that I want to do is um is um get the average players the most kills per game so the first thing I can do is uh most bu yeah then we can do d i play a game Agate and then this is actually called player so if you're look in here in the um table and this is where you need to start looking at the table and yeah it's just so easy to miss spell it isn't it um oh my God don't know why like I just could my spell on it so much um uh let's look at this table yeah this is the one I ended up using player total kills and then um the llm told me I should give this a nice name so I gave it an earlier of total kills or average kills um that average yeah it was average I didn't want Total kills average kills and so we could do not it's not EG it's average and then you can do um this average kills per game show and then uh get the most common playlist which was um so this is what Co this is what Co palette suggested but actually what um what um the LM suggested was to do most common playlist and then do um the uh by playlist yeah do the order by but instead of doing first you take one that's what um the LM suggested to do and \[Music\] then which map gets played the most get the most common map and then very similarly do the same thing but this was map ID I use map ID not map name and then what was the next one which map do players get the most killing SP medals on and so this was a little bit different so um we actually had to do a join with um the uh we had to do a broadcast join so we had to do something very similar to this basically because um basically we look on the killing speed medals if you look here in The Killing spr medals there's a classification like thing and so there's a classification and if you do Killing Spree you can see that there's a killing spree classification in here so we needed to do a bucket join on the medals. CSV um which you could have done it up here but because um because this wasn't included in the book ADF um we do it I did it again um just with the booket DF at this this time I'm not sure if you wanted to um do the podcast join and then join everything but I decided to keep them separate and only join them when I needed them and I did uh so I add that in and then I did a join um so booked so D equals and then I did a broadcast join on the middle ID and then I did a killing spree um like filter so um you can \[Music\] do what did I do to equals yeah and so you want to do a filter um on where like goodbye yeah that's what I did basically so we join the medals table we do a filter on classification equals Killing Spree we Group by the map ID and the count and then we take the first one and that is pretty much what I did for that one and that was all the these ones here so I'm just going to check that this works fine oh wait that's not going to work I need to Del my warehouse first okay yeah so again um so this is playlist ID not playlist for you can see here that it gives you that information about the most common map for King spre is that um and so this is pretty much done e okay so once that is done \[Music\] so with the agre with the aggregated data set try different sort within partitions to see which has the smallest data size hint playlists and maps are both very low cardinality so what I did next was um these bucket joints here for this match is booked I did that but with um with the um so I just did a a get file sizes DAT file what I did was um I created two new files with this data here is \[Music\] and then with the bucket join matches V2 what I did was I sorted with imp itions of match ID and map ID and then for book it and I called the table that as well and then I also created another table be free and I did uh what was the other one playlist ID May and then um once I did that I got the and then I compared the vile sizes of each one so um and so once the files were created I um did uh what I did once I got the different I notic on Discord everyone's getting different answers for each of these think as do the analytics assumptions you make yeah let me do for table and table um file then um there's this thing that you can use yeah it's so weird like got an A from the alarm it's so confusing like but I think I think a big part of that was because I think the homework instructions were very very loose do that make sense like there's no there's very little scope here on what's correct and what's not it's very hard to be like well what do you want from this data and what's like if that makes sense I don't want to remove it I want to compare file sizes um I actually did a file and then that was all I did um then I called this I'm just doing the Aggregates for each one of these and then what I'm going to do is in my aggregated function I'm going to do a start a Time start and a start a Time end because it says to um like have a look at um the different side like the llm suggested like comparing the different times and stuff like that so um let's do in here where's my get I could get this so I created a start time for the start of all my my calls and then I did an end time and then I printed end time on a start time um just to for each one to get the times for each one um so I'm going to remove the booket do \[Music\] show file sizes and then just going to delete this yes it is interesting wasn't it let me just remove this cuz it's doing for all oops my SPO one thing that has been good about this though actually is like like I remember before when I used P Spar I always defaulted to SQL whereas I've really tried to utilize the P back methods in this um in this homework and it's been quite good to be fair e I'm try am for this this is stupid I am didn't mean to do matches matches details for some reason it did matches details for some reason how odd how bizar How Bizarre so I'm just removing the P list the map ID there all right better yeah I imagine that's probably quite common isn't it okay what's wrun Oh wait need stop swearing the children think of the children to be fair I don't think any children watch my channel like if I look at the stats for example the majority of people watch my channel like my age so yeah this reminds me of like when I'm writing terraform and I just have to wait and watch it run and then you're like is it going to fail is it going to fail for this is why likeing Bloody Co pilot never trust co-pilot P palal get you tricks you into a bolt sens of security then it takes everything away e yeah so you can kind of see like if you look at here like if we look at the first one the file sizes are so different than this one what say that sizes are actually the same one thing that is interesting though is just how like the seconds do differ so like you can kind of see here the um the seconds are vastly different between each one um so that was interesting observation I don't know where the file sizes are pretty much the same just in a different order um I do have like um my little snow filter but I figured like people might find it a bit distracting for the homework one so I I decided to disable it for today but um is there anything that you did in particular to like verify like the the file sizes because like this this doesn't actually all the file sizes are the same for me even though like um I've Chang the partitioning of them um it's weird because I still got an A on this but if I look at here the F sizes are definitely all the same sizing for last thing I did as well very interesting um let's try you did don't say needed it all together did you say oh you did by gamertag as well let me try by for I'm just going to put that in here e do one with brows e e e e just oh let's see if I don't do the bucket by in these ones if that changes things yeah definitely like is a lot slower than the other two methods like if you compare them like I think that was taking like double the time than the other two um in terms of all the aggregated queries we just taking double the amount time yeah you can kind of see it's 13 seconds for the first one and then it's 7 seconds for the second one right for me go back to how it was for no worries have a enjoy enjoy your go simulator with your nephews for okay so I'm just going to try and remove for these two this bucket by for going to win this now e okay so you can see that there is slight differences in at least this one and this one these two like those are is here um I'm just looking at what just ined then this is the last thing I'm going to try and if this isn't it I'm just going to do leave it how I did it because that's what I did do did get did did do well so um e yeah so this is very obviously very different to these ones but these ones don't really differ that much um I wonder I I'll probably I'll play around a it bit more I wonder if it's just because any if I do start within partitions and just do on playlist ID that changes things but um but yeah uh that's pretty much all I did the last thing I did was um I just explained each uh each like uh verified table cutness basically and for that I just um each of the tables ass sh partition information that's all I did really that was the last thing I did for this homework e but yeah you can kind of see like each time like when all you do is partition on um match ID it's so much faster like slower than when you do like sort within partitions like on different values um um yeah that was it really that's all I did um I definitely want to investigate a little bit more about this um but I'll need to do that another day um because uh I'm not quite sure what's going on there but um everything I did submit was this I did submit this as this is and so I need I just need to figure out why these values are all the same versus the first one um and I'll just what I'll do is I'll probably just investigate like different types of partitioning um because at the moment a lot of partitions are very similar so I might just like remove match ID and see if that makes a difference um but other than that I I did submit this and if I go to my feedback this was the feedback so it said that I did the test one correct it said that I did this correctly it said that I did test three correctly it said that I calculated the average kills I basically did all these um optimize data and size measurements we calculate the file size and measure the we times for each table this shows good understanding of how partitioning affects performance and then you provided a function to verify the partitioning correctness using its plan um so yeah that's what I is and now I'm on to the homework submission that's the next one yay um so I'll be on with that like probably tomorrow I guess um I've got a couple of nights where I can do streaming CU um my partner's away for well he's not away but he's going out for a couple of nights and so I get to be all antisocial um for those few nights um yeah uh te caros oh cool nice to what video you plan on Posting and what what video editing software do I use I use OBS Studio I'll type it in uh what what video editing software do I use um just clip Chum to be fair um or dentu all they're the two ones but to be honest um some of my videos have been edited by someone else so um I can't help you with those ones you're probably best off reaching out to my editor if if if I've used an editor I've linked them in um if I have used an editor I've linked I've linked his profile into um into my channel so um he's really good to be fair but um but I've started doing more technical content so I don't really edit much of my technical trained content just because I don't think you need to uh but he's he is good so um he might he might be able to tell you what he uses right okay so I'm going to end there because I think this has been a long live stream and got to the end but yeah uh thank you for watching with me um if anyone manages to figure out why my let me go back if anyone figures manages to figure out why these are the same then let me know but um this is what I've done and I'm I'm pretty happy with what I did um pretty happy with how it got there but yeah uh that's it for me thank you for watching and I'll see you next time and good luck with the editing caros I hope it all goes well


 > [!info]
> - **Disable automatic broadcast joins:** Use `spark.conf.set` to set `spark.sql.autoBroadcastJoinThreshold` to -1 to manually specify broadcast joins (7:30).
> - **Explicitly broadcast join medals and maps:** Join smaller tables using `broadcast` to optimize performance (7:56, 16:39).
> - **Bucket join match details, matches, and medal matches players:** Use `bucketBy` with 16 buckets on `match_id` (8:37).
> - **Sort within partitions:** After repartitioning, sort data within partitions for better organization (28:04).
> - **Inspect data:** Validate data throughout the process to ensure correct values (25:21).
> - **Consider partitioning by date:** For large datasets, partition by date for efficient querying (33:47).
> - **Measure file sizes and execution times:** Compare different partitioning strategies to optimize performance (1:07:53).
> - **Verify partitioning correctness:** Use `explain` to check the partitioning plan (1:47:11).
> - **Use PySpark methods:** Utilize PySpark methods instead of defaulting to SQL for better performance (1:19:51).
> 