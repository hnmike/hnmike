---
title: "DataExpert.io - Applying Analytical Patterns Lecture & Lab 2 - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2025-01-08
source: "https://www.youtube.com/watch?v=biaaA9GfNPw&t=4749s"
image: "https://i.ytimg.com/vi/biaaA9GfNPw/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgVChOMA8=&rs=AOn4CLDRFdgE-3yiUQDUAPumiTuXtm9RAQ"
created: 2025-03-23
tags:
  - "youtube"
  - "data_engineering_bootcamp"
  - "analytical_patterns"
  - "data_modeling"
summary: "Data Engineering Bootcamp Lecture & Lab 2 focuses on analytical patterns, SQL optimization, and the importance of data modeling for efficient analytics. "
---
# DataExpert.io - Applying Analytical Patterns Lecture & Lab 2 - Data Engineering Bootcamp

![DataExpert.io - Applying Analytical Patterns Lecture & Lab 2 - Data Engineering Bootcamp](https://www.youtube.com/embed/biaaA9GfNPw&t=4749s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> e e funnels are a mass massive part of analytics and data engineering and what does that mean so you have a funnel is something where you have a lot of people at the top and you have not as many people at the bottom and so you want to push people through funnels to make more money to get more growth to get more engagement there's so many different paths that funnels are necessary for and the data engineering behind it is somewhat complicated in this course we're going to be covering how to do good funnel analysis and uh I hope you enjoy it because we're going to do funnel analysis just like they do at meta \[Music\] so I'm sure y'all have seen this meme this meme is uh I don't know I've probably seen this meme at least like once a month on LinkedIn where the the technical interview for data engineering the SQL interview they're going to ask you all sorts of crazy stuff and you might get like these crazy like self-join questions or ranking questions or like and then like and then when you actually do the job like the odds that you actually use those functions are a little bit lower I found that like that there's more overlap between the data engineering SQL interview and the data engineering job than there is between like the software engineering leak code interview and the software engineering job so in that way like I can't complain too much about the data engineering squl interview but it's you know like there is definitely still like they they they try to test you try to grill you pretty hard so things you'll see in the data engineering SQL interview that you'll almost never do on the J so um I've been in this situation a couple times where I've been interviewing at companies and I will come up with a solution that uses a window function and then they ask me to rewrite it without using a window function and like I don't know man like what like i' I've been there like more than once in my career and when they asked me to do that I'm always like makes me want to like throw a chair because it's like I solved the problem bro like like why are you making me solve the problem again and some companies have this like absurd obsession with like an SQL but not even like an SQL because a lot of window functions are an SQL now but like more like 2003 ansy SQL they're all about that right and in this case though like when if you were trying to solve a problem with a window function and you do want to convert it to a problem that is not using a window function uh Sometimes It's Tricky can be actually a very tricky problem especially like with like ranking and stuff like that some it can actually get really gnarly but at least for the case of um like uh a lot of times you use like lead and lag and lead and lag can easily be uh replicated with a self join where you do a self join on like date plus one like date equals date plus one and then you get the two records on the same r and then you can essentially do that that's what they're usually looking for when they're asking you to write the query again without a window function a lot of times when uh they're asking you these kind of questions as a data engineer they're mostly just trying to see um and make you squirm a little bit and see like oh like how much depth does this guy actually have can he solve the question in a completely different way can he give me can he can he say tomato and tomato because if he can't say both he's obviously not qualified for job and it's kind of absurd it's kind of absurd I found that that that part of the sequel interview to be kind of absurd um another one that I've been in i' I've I've had to do a couple times is uh using recursive CTE I don't know if y'all have ever used like the with recursive uh command um I've literally done that in interviews I think three times two two three times something like that and I've done it um on the job zero times never not not even once it has not come up not even one time so uh that's a great example of one that is literally like asked a ton as if it's like something that you're going to need and really the only so does anyone else like is anyone else like super triggered by this like little mouse on his nose I don't know if that's intentional but like um The Little Mouse and his nose just been there it's funny I'm just going to change the play speed as well time that you would ever need that is in the case of where you have like a parent child uh query around like managers and reports that's the only time I've ever really seen it was when you have this crazy like tree hierarchy that's the only time you ever really need to use recursive CTE um people like essentially think like oh if you know recursive CTE that's that's when you know right that's when you know that you are the that you are the honcho right you are the sequel wizard you have officially you are now level 100 at Sequel and you can get you know you can get your cape and celebrate and it'll be awesome um I don't know I uh I'm skeptical of it because of that um correlated subers are the same way like I I I feel that there's uh like I it's one of those that like I've never needed to use actually in the job where you can leverage joins and aggregations and all that stuff just like keeping things simple keep things simple right and like that's one of the other things about like the recursive CTE where it's like even if you wanted to do recursive CTE for a management a manager report structure the thing is is most companies they only have a couple layers right so instead you could just do two or three joins where you just join each layer of the company and uh you just do it that way instead of using a recursive CTE and that would solve the problem probably more efficiently and uh but my whole point here is they're going to ask you a lot of stupid stuff like a lot of things in the sequel interview that are going to just not be relevant uh anywhere else and these things fall under the guise of advanced SQL and because they're like well if you've never used recursive CTE before you're not a senior data engineer and it's like bro I've never used that in my entire career and I'm a Staff data engineer I've been doing this for 10 years so like I don't know man like I think that some of these some of the questions that people ask need to be better like they need to like improve and they need to uh not necessarily uh make those hard assumptions that like you have to have like one specific nugget of knowledge I have noticed though especially between uh when I interviewed at Facebook and when I interviewed at Airbnb because there was like five years between those two interviews and um I noticed that like a lot of the at least in big Tech that a lot of the the the SQL questions have gotten more like more relevant and but still tricky more relevant and but still like the same level of tricky but not obnoxiously Technical and like obnoxiously doing stuff like this because I I remember like when I was interviewing at Airbnb and I and I got I got my SQL question it was like I I solved it with a self join it was definitely a self join that I solved it with and I I probably could have used a window function but like I lean into not using window functions at all when I'm doing uh SQL interviews unless there's like specific keywords like if they say rank or they say rolling or they say like very specific words in the question then those words are like they tip me off to say use a window function because everything else is going to not be worth like trying to trying to do it with like an SQL is not going to be worth it and you're not going to get through the problem in time and they're not going to ask you to do that either because it's too complicated in like a 30 or 45 minute setting so anyways they're different and one of the things that I hope if any of y'all have failed a um a SQL interview before that that they that that is not that doesn't necessarily mean that you are not Advanced enough at SQL I've failed SQL interviews before I had some crazy ones some crazy ones with like where they asked me just really ridiculous stuff and I'm always like dude that was okay it was not meant to be that company obviously only has massively hierarchical data and that's the only data sets that they have and that's why they ask every data engineer that question even though that's definitely not true but anyways let's uh let's go to the next slide it's definitely true there are some things about the de interview the de SQL interview that they do get right A lot of times you're going to get pressed in the de interview around uh uh table scans like how many times are you scanning the table because that really does make a difference for the um uh like the optimization and the performance of the query number number of table stands is the number one uh uh like thing for the performance of a SQL query and so that's going to be a big thing to think about um one of the one of the slam dunk things that you need to do in your data engineering SQL interviews and elsewhere like you're you're going to use these a lot in a lot of places in life as you're doing count case when so you put a some sort of condition in your aggregation statement and that is going to be uh just such a good thing I've used that so many times to Pivot out data to do all sorts of really awesome SQL queries that are like fast and efficient and scan the table once and uh you can do really really powerful things with those so definitely if you haven't tried out count case when I'm pretty sure you have because like yall are pretty experienced but if you haven't definitely do that and obviously cative table design comes up again where uh where in this case like you can have queries that are cumulative and then uh you you show them how you would then incrementally build it up and incrementally solve your problems that way so that's another thing that I've done a lot like with uh um in the interview process is talking about how to query the minimum amount of data to solve your problem uh then obviously there's like writing clean code clean code is just just so important like don't like especially as data Engineers like dat that's our whole job like other people can write terrible sequel monstrosities but that's I'm glad I don't have to worry about this part like of having to like do the interview because I'm not really doing it to to pass an interview so um I'm glad I don't have to to worry about this part like we should take pride in writing good Sequel and obviously commentable expressions are your friend it's like the with keyword uh use that a lot uh pretty much always especially in the interview if you use with a lot you're going to be good there are some edge cases with commentable expressions with like postgress and older versions of postgress and MySQL and stuff like that where like you can hurt performance but like in the interview always use it because it's always the cleanest way to write the code and then use aliases don't ever have a column name that's like not Alias especially if it's a derived column because like uh then your result set is going to look ugly and obviously you don't want an ugly result set so uh those are the things that like I've noticed that de interviews kind of nudge me towards they really nudge me towards actually doing these things and actually caring about efficiency so that's where it's like I like them but I don't like them because like they're kind of a wild card where you can be ask like some crazy question as well so anyways that's kind of the idea behind de interviews and how they will um uh kind of nudge you in the right direction for writing a good SQL which some of this stuff I wouldn't consider Advanced but it's more of like thinking through your queries and I think another big thing that you can use here uh not necessarily in the interview but like it will help you get better at writing queries is this's this thing called the explain keyword explain is very powerful it shows you uh the query plan so it shows you how the query is going to be ran underneath the hood and if you can uh learn more about the explain keyword and then like use it when you're writing your queries and then understand like oh this query creates this plan and this query creates this plan and then kind of building up a fundamental understanding of how queries like how SQL is turned into this a thing called an a is an abstract syntax tree which is essentially one layer lower where your SQL gets converted into an a which is going to be all of the different uh like filter aggregation join sort of like mechanisms that are within SQL and The Ordering of them because the tree has to point right where you have like because you know there's the order in which the SQL keywords operate in right we from and join happen first and then where and then Group by and then having and then uh and then select and then order by or whatever those like that kind of ordering that a SQL happens in and so that is another part about the abstract syntax tree that if you understand how the ests are generated with SQL you're going to be just a way better uh practitioner of sequel because I've noticed that like once I started like digging deeper into that kind of stuff I like the odds that I wrote a bad performing query just like went away for the most part and I was like wow okay like I I know what's going on here even complicated queries so that's an important piece of the puzzle as well so we're going to talk more about this stuff and we're going to use some of this stuff in uh the lab today as well um so there is a a suite of three um Group by mechanisms here there's one called grouping sets one's called by Cube and one's called Group by rollup and we're going to talk more in depth about these it's a way to it's these are essentially a way to do multiple aggregations in one query without having to do like nasty unions so you can like group by like you can imagine like you can Group by like gender and Country and then you can Group by just gender and just country and then overall as well if you want and you can essentially pick all of the different rollups that you want uh and we'll go into more details there um self jooin is another important concept that I think is often times s brushed aside when um thinking about SQL and like because most of the time when people think about uh data modeling they think of like Kimble which has like facts and dimensions and joins but it never really talks about how like a table can join with itself and so this uh use case here a lot of times like what we're going to work on today at least with in terms of self joins is is great one things is like Sundar doesn't even realize how how grateful I am for what he was talking about so he talked about funnel analytics today in the in his Boot Camp or in his presentation and we're going to be creating a funnel we're going to look at funnel and conversion rates on my website using self joint so it'll be that'll be a fun uh little project we're going to work on and that's where a table is joined to itself so that will be fun um obviously window functions window functions are very very critical uh in the data engineering interview I want to talk a little bit more about like the signals uh that you want to look for when when you should know to use um a window function so you have different things like if they say monthly average they almost always mean rolling monthly average and if they mean rolling if it's rolling monthly average then uh window function is going to be your best bet and like that's going to and you're gonna have to like B bust out like the whole window function you're going to need to do like um average over and then probably partition on some sort of Dimension and then order by date and then it'll be rows uh between 30 proceeding and current row and that will give you the um that the the lagging uh 30-day monthly average and then obviously uh you also have I thing called a cross joint unnest and lateral view explode uh unest if y'all remember from weeks one from week one and week two yeah we did that a little bit in week two as well unnest is essentially how you can turn an array column back into rows and it essentially explodes out the array into rows that's why it's called explode sometimes it's called unest or explode cross join unest and lateral view explode are actually the same they literally do exactly the same thing uh it just depends on the query engine um I personally like frost drin unest as the canonical word so it's like essentially how it works is like frost drin on Nest is like if you're working in like a SE focused engine like Presto or postgress or MySQL or Oracle or like it's very SQL based you're going you're going to see cross joint unest and if it's jvm based like uh spark or Hive or any of like those kind of more big data oriented map reduce any of those um kind of oriented Technologies then you're going to see lateral view explode that's what um because lateral view explode was birth out of like Hadoop and cross joint unest was more birth out of like post out of like the postgress kind of syntactical stuff so um we're not going to cover those today but because we already did and y'all have been doing Advanced SQL techniques throughout this entire boot camp and uh I hope that the homeworks in weeks one and two are also helping you get even more up to dat and more fresh on some of these Concepts so let's go let's let's dig a little bit deeper into the this first one this aggregation thing I was talking about well let's talk about um how this works so this is uh the most complicated of the three we have grouping sets so you can think of this as so in this um query we are going to this is actually going to give us this is essentially doing four queries in one so we're going to group on OS type um so I think he's planning on taking it down the first week of February device type browser type like all of them together and then OS type and browser type and then just OS type and just browser type so this is going to actually do four aggregations the way that you would do this without grouping sets is you would need to copy and paste the query four times and then you would say Group by OS type device type browser type and then Group by OS type browser type and then Group by OS type and browser type and you're going to need to put dummy stuff in there for the the values that are not in the group bu like so for example when you're grouping by just OS type then uh if you want the union all to fit with all the rest of the query you're going need to put like overall for device type and overall for browser type and because uh because those are just ignored in that aggregation so they are essentially just completely not even looked at and so one of the things that's really nice about this way of doing things is you can um one you get performance benefits because Union all is one of the slowest and most terrible keywords in all of SQL so you get a nice uh performance bump from uh doing that also you get a nice performance or you you you get a readability bump because and you won't you don't have to copy and paste the query five times so you you only have to edit the query in one spot which is a nice uh maintainability readability bump as well and then you also get access to whatever uh combination of um Dimensions that you want to look at so one of the things that's important when working with this pattern is when uh when a dimension is ignored in one of the grouping sets like for example in that last one when we're just grouping on browser type that means that device type and Os type are both ignored and what grouping sets does to those columns in the query is it nullifies them so one of the things that's important to do before uh using grouping sets is you want to make sure that these columns are never null because they get turn they get nullified uh when they get excluded from one of the group buys and then the problem is is then you don't know if it's nullified because it's excluded from the group buy or if it's nullified because it's it came in null so uh uh kind of one of the best practices here is before you use group sets or any of these grouping patterns for that matter you want to coales all of the um the grouping uh the grouping Dimensions you want to coales them to things like unknown or like some other value like unknown na or whatever you want to call it and uh because then you'll know you'll know in that those cases unknown essentially means that it was null and then uh if it's null that means that it was excluded from the group by on one of the the the fewer Dimension groupings if that doesn't make sense don't worry we're covering this in depth in the lab today so this is um so keeping in mind this is the most complicated way um that you can use grouping sets it gives you the most control because you you pick all of the all of the grains that you want to aggregate along um let's let's take a little bit of a shift here and talk a little bit more about Cube because cube is another interesting one so so you'll see cube is a little bit um simpler um Cube essentially what Cube does is it gives you all possible permutations here so in that case uh one of the ways that you will know what the the number of permutations is it's actually a lot so um uh so how it works is if you have three what it is is you get um you're GNA have uh like there's going to be the combination of three and then you have the combination of two combination of one and then none right and so that's going to give you all the possible uh values that you could have right so uh in that case you're gonna have all of them and then you're gonna have you got to pick two and when you pick two right there's going to be three possible uh values right when you pick two and then when you pick one there's going to be three possible values again because it's just any one of them and so that's uh 1+ 3 plus 3 that's seven and then you also need to pick zero so that's when you just do the overall Aggregate and uh that will be where you essentially Group by nothing and so in that case this one is actually this it looks like this is like one this one line is actually doing eight different grains of data and so it's very powerful because like you see compared to the grouping sets line right where you have to specify each of the grains uh it with Cube it just gives you all of them automatically but this can also lead to problems especially if uh you one you pick more because uh you saw how like if you have three dimensions you get eight right but if you have four dimensions it it blows up it blows up and like you get so like because the number it's a combinatorial explosion right so you can only really use Cube on like I generally my my perspective is three like uh you should only use three dimensions uh maybe four if you want to like get real fancy but that's going to you're going to have a lot of like different combinations and permutations of things when you use four so I um don't know if I would recommend using four uh Dimensions here because the combinations just blows up definitely not five because you think about like five factorial right five factorial is 120 right and that's that's a lot like I wouldn't want 120 grains of beta so anyways that's Cube so there was one more here that I think is important to talk about which is rollup so rollup is a little bit different in the fact that how it works is that you use rollup for hierarchical data um where I hope that it does the lab showing each of these because it'd be good to see what the transformation of the original data to using this does imagine if you have like country and then you have state and you have City so in that case we only like we always want to group on country and then sometimes we'll group on uh country just country or then we'll group on country and state and then we'll group on country and state and city so in this case like for this rollup it would be we group on OS type by itself we also group on OS type and device type and then we also group on OS type device type and browser type so Roll Up is a little bit different you don't get all like so cube is like cube is a lot like generally speaking uh from my perspective when using these three different ways of doing group VI rollup is going to be uh like probably a better choice because with rollup right you just get the number of Dimensions is also equal to the number of aggregations that you get so it grows linearly it doesn't grow um like factorially or com I guess combinatorially so in that way cube is just not very scalable but it is a very expressive way to uh like talk about a bunch of possible permutations of Dimensions um so like but generally my experience I don't like I have never used cube in uh Big Data like in uh like at Facebook Netflix airb I've never used Cube uh I've used grouping sets at all the companies and I've also used rollup at all the companies I've used both of those uh but I've never used Cube and the main reason for that is because of this like crazy like it's like because one of the things about Cube that I don't like is that it's going to do all the combinations even when there's some that you don't even care about and that's the part about Cube that I don't like is it gives up it's it just does everything and some combinations you probably don't care about and if you don't care about some of the combinations then like you're wasting compute whereas rollup is nice because it's very clear that it's only going to do three and then grouping sets gives you that very fine grain control over like I want this grain this grain and this grain and for me as an engineer I am more of the I I don't like uh what's called automagical stuff like where like you write one line of code and then it magically produces all sorts of crazy stuff because I think that um automagical stuff is one hard to debug and two like um it's going to you lose control you like in this case like cube is not going to be as efficient as here if I only care about these four Cuts then why would I use Cube and get eight right why would I use Cube and get eight if I only care about these four so that's one of the things that you want to be careful about is uh how um how much control that you are giving up by using this expressiveness um so in this case my perspective is you want to use grouping sets or roll up and kind of just ignore Cube unless it's like small data and you want to like I I think cube is like the one place that I would I would say cube is like okay is when you are doing like exploratory data analysis and you're just trying to like look at like trying to really understand all of the dimensions really quickly because that will give you a lot clearer picture into like what's going on and then you don't have to like use grouping sets because like that's annoying because imagine having to put all eight in here for grouping sets just so you can understand what's going on um so those are the three those are the three kind of like specialized Group by keywords that like I have found to be very useful uh there is one caveat I want to talk about here is if you you're using big query you only get rollup that's it you don't get like big query does not give you grouping sets big query does not give you Cube it only gives you rollup so uh just letting y'all know if you're like big query fans okay okay so I want to talk a little bit more about window functions window functions are all are just just an very important part of analytics uh we're going to go super deep into window functions today in the lab uh one of the things about window functions is there's just a couple pieces to them uh you have first you have the function you just going to be like rank sum average dense rank row number um lag lead I think first value last value I think that's it it's pretty much it there's not that many there's like 10 and um so like one of the things I want to talk about real quick is like there's three that are important to talk about where you have rank dense Rank and row number uh which essentially do the same thing they kind of like they give an ordal they give an ordal output based on another column and like my perspective is never use rank ever because like rank just like skips values and stuff like that and like it's super obnoxious like I rank skips values so like uh and there's hi hm how you doing no benefit for that I mean unless like you're okay the one exception is like if you're in the Olympics and then there's like a tie for second place and then uh then the next place is not third it's Fourth uh because of that uh I think that that's an okay spot like but that's literally if you're a data engineer on the Olympics like there is like I guess an extreme case where rank is better but generally speaking I use dance rank or row number every single time I would say I lean into row number more um because it gives a unique value for each row even if there is a tie so if there's a tie for second uh so you have those two rows that are both ranked as second in row number one of them will be get will be ranked third and uh it's based on the natural ordering of the data whichever row uh in the in the database is for is comes before will be the one that's ranked higher um and so that is don't worry I didn't mention I did mention the deadline in when talking it's just I didn't type it out is uh and then DSE rink is great because uh you do have ties but then you don't skip any ordinals because then it's like if you have a tie for second then third place will be third instead of fourth because in rank it's Fourth but in Den rank if there's a tie first second then third place is third and I like that because then you have like a continuous numbering you don't have like weird gaps where like sometimes like third place doesn't show up and stuff like that it's terrible terrible so anyways function big part of it uh then you have uh the window which because then you have like the over keyword so you have like function over and then you have uh the window itself which has three things right you have the Partition by order by and rows Clauses so partition buy is essentially how you cut up your window so like maybe you only want to look at Windows per user or per country or per date or per month or whatever there's like all different ways that you can cut up the window and that will help you define things uh then you have orderby which is how you sort the window uh this is great for ranking functions uh and also for uh you need to have the order by in there if you're doing like rolling sums if you're doing like a like a a monthly roll Su or something like that then you're going to need to have a good order by because otherwise it will just pick the last 30 rows and those that might not be the right 30 rows so you definitely need to think about order by when you're doing rolling uh functions and sums and averages as well and then you have the rows clause which essentially just determines how big the window can be uh you can you can have the window be the entire window and that's like what you can say is and we're going to look at this in the lab but there's like so you can have rows between and then it could be unbounded proceeding and unbounded um uh following and that's just in that case it's give me all all the rows before and after the current row and so then you can get the entire window that way and you can understand like how to do like a sum of the entire window and so that can be another really powerful way so the default value for rows right is between unbounded proceeding so unbounded proceeding is the default value for um uh the proceeding like the the left side of the window and then the right side of the window is current row so it's always like backward looking it always just looks from the current row backwards and it will go from the current row backwards in time so if you do like a sum with a a partition by and order by without the rows clause you're going to essentially get a cumulative sum up to that point in in the rows so like say you have 30 rows of data and you're looking at the row that is the 30th row and you're summing then you don't use the rows Clause you're going to have just all the data in that window from that row backwards and so like if you don't Define the rows uh um Clause then your default is you get accumulation where you just get um all the rows before the current row based on order by this is the idea for window functions um I'm sure some of y'all already know this uh we're going to cover window functions a little bit in the lab as well today so um uh we're going to look at modeling versus Advanced SQL so um obviously like if you are a data engineer and you're just given raw data and then it says like find Insight in this data like you could probably do it right you might have to write a lot of SQL you might have to create a lot of temp tables you might have to run some pipelines so um this uh image here uh that I uh I posted this like funky image so what it was was uh I went to do e and I uh used like I punched in SQL gymnastics and uh that's what I got like that Dolly said that that is what SQL gymnastics is and I'm like okay thank you AI thanks for making this presentation better so what can happen here is uh if you are working with a data analyst and they are working on your tables one of the things is is like you as a data engineer a lot of the time know SQL very well or you should and you will have this boot camp if you don't and um one of the things about that is you sometimes might have the expectation from your uh analytics partners that they have the same Proficiency in SQL so that they can just tackle whatever data model you give them and that's a lot of times not the case a lot of times like uh these people are going to be uh uh they don't know all the crazy new udfs or yeah yeah you should be able to um resubmit so I resubmitted a few times and it's regraded it so it it uses the current grade really so yeah that you should be fine to do that functions or stuff that make it easier to Crunch this data and you need to give them uh the ability to just like understand it easily and if you if they need to like have to like for example if you give them a table that's not dded and then it's like okay just use row number to DD it right you could save that as a data engineer and just like advocate from all responsibility right and just be like no like all quality are on you when you're querying and so and then you can just use your row row your row number function and uh window functions and I'm just going to give you garbage and then but I I it's on you to fix and uh that can happen I mean obviously like if you do some amount of data modeling it doesn't even have to be that much and yeah I um I've had a c I think in the previous homework which was the Flink homework and then I resubmitted it and got a B and then I resubmitted it again and got another B and then I got an A eventually um yeah but it was really the link homework was very like um particular it wanted you to be it wanted you to document stuff and it was like the actual homework piece was quite vague and data modeling and data quality like what we covered last week then uh a lot of these problems will disappear and you'll be able to have data that like makes it so analysts don't need to do Advanced sequel but obviously sometimes analysts like this is not necessarily always the case like sometimes analysts want to do Advanced sequel because they're trying to like they're trying to answer a very very specific question that like you aren't going to write a very specific pipeline for because that's like a waste of time and so like I'm not saying that like just because a data analyst is doing crazy gymnastic SQL that means that like you did bad data modeling but it could be it could be it could be a signal so that's one of the things where like if analysts are struggling to like get insight out of your data that's something that you want to take in and be like oh maybe I need to uh revamp this table or I need to change how it is or I need to do something to it that will make it so that the analyst can understand things better this becomes even a bigger problem as as the data scales because analysts will want to do more complex queries on longer time frames and when you have uh and if you have more complex queries on longer time frames uh big of bigger data then uh you that's just a lot of complexity each one of those is complex by itself so when you stack complexity on top of complexity on top of complexity you're going to get slow queries queries that don't run queries that fall apart queries that take forever right and uh you don't want that you want your analy Partners to be happy right and that's where working with them on like if they want to try to solve those complex questions and they're not like one-offs that's where it's important to understand the analytical patterns that your your analysts and your analytical partners are trying to do so that you can provide data sets that are delightful to use in that way similar to you know in week two when we were talking about like the long-term analysis stuff I did with like the the fact Dimensions like or the fact arrays where you have like that long array metric that was that's a great example of that where I noticed that I was like hey my analysts are freaking they're quering very long time frames of data and obviously I can give them just the regular fact data or the regular daily data but like that's just going to still be really slow it's going to be really terrible and so that's where uh you can you can think of this as like both the terms of volume but also on complexity because a lot of times they might do the same query over and over and over again and I think this is quite important because um because if you you need to have an idea of the amount of data that people are going to be working with so that you can make sure that the way that you write and aggregate your your data is is is is most optimal and I think the longer time skills the more you need to aggregate like data together and really what you should do is materialize that and then they should just query off of that directly and then that would massively speed up their analytics so a lot of times like one of the things I always do as a data engineer is uh and this is something I do especially when I first started on a job is understand where the analytics partners are at and like what they are querying and what they're building and what they're presenting just so I can understand like where some bottlenecks are and like how I can potentially uh speed up their processes because if you can make analysts faster as a data engineer you're doing your job you're doing a great job at doing your job and that's uh like you want to do that okay so uh this is I think we're almost done here and we can take a transition to the lab here in just a second so you have a symptoms of bad data modeling right you have slow dashboards uh this is where grouping sets can be really useful because if you are using like row or daily level data um like in your dashboards and it's not it's not pre-aggregated then your your dashboard will if your company scales to a big enough scale then uh your dashboard's not going to work it's going to eventually it's it's destined to not run anymore and so uh like if you use pre-aggregation though and you and you only have your dashboard be um pre-aggregated data then your dashboard will be infinitely scalable I noticed that like when I was working at Facebook because it's like I was working on uh this dashboard that like consolidated all the metrics between WhatsApp Instagram Facebook Messenger all the different like apps and uh each one of those apps had like a billion users so it was like am I going to really load in uh you know five billion rows and Tableau am I going to do that like is Tableau GNA like is tblo g to do that or even even postgress is postgress going to do that is post G to freaking uh handle that and it's like obviously not and so uh what I did was I figured out like kind of a framework where you always pre-aggregate and then but you preaggregate along the dimensions that people care about like whether whether that be age or country or device or um app like we also wanted to see like yeah we have this many WhatsApp users this many Facebook users so you want to aggregate on those Dimensions because then that data goes from being billions of Records like and then with app it's like to like five right so that's like going from billions of rows to five rows is like so efficient you know so that's the thing to think about when uh like when you're dealing with uh some of this like analytical data modeling is slow dashboards is like a very common symptom that you need to pre-aggregate your data obviously uh another one here is going to be around like if you if your analyst is like you look at their query and there's like 10 CTE in the query it's like okay like maybe there's a staging step maybe there's another step in the middle here especially if you see these again and again and again and you like they keep sharing these cre with you that are like freaking 10 CTE that usually means that like the first five should be materialized as a temp table or as a staging table and then they they do their analysis on top of that and maybe that table has short retention so that you don't like burn down the warehouse but like that's like definitely do that don't like have your analyst just like suffer and always because remember this this is something that I always think is very important to talk about with data modeling is storage is cheaper than compute Right storage is cheaper than compute across the board cheaper so in those cases where it's like if you have an analyst who's running 10 CT if you materialize halfway through then and and that's the table that they run off of then you pay a little bit of storage to save a lot of compute and then um even if it was one to one right you pay the same amount of storage to get the same amount of compute that's still better because then the amount of time that the analyst gets back is also better right because that's other thing you got to remember is that like there's Cloud cost there's employee cost right of like waiting for queries to run and then there's also storage cost right compute and storage cost and employee cost so even if like if you're making the storage a compute trade-off and the storage cost is the same as the compute savings it's usually still worth it because you get a different benefit and that benefit is going to be on the uh employee Time Savings and time reduction and you get more business velocity so that can be another massive thing another big uh thing right is case when statements like that just means that like if you have an analyst that's doing all sorts of garly case when statements that usually means that your data model is not robust enough or it doesn't it's not conformed enough you aren't conforming the values to what they need to be so those are the the big ones there's going to be a lot of other ones that you could probably think of uh but those are going to be the three big ones that I was thinking about when I was thinking about like how I work with analytical Partners so yeah like uh remember that that like bad data modeling and like I would I wouldn't I don't know if i' call this Advanced SQL I'd probably call it more complex SQL which is probably a better word to use on I'll probably change the slide here before I upload it but uh that those things kind of are connected and that like it's your job as a data engineer to make it so that your analyst has simpler queries especially if they're running those queries a lot congrats on getting to the end of the day two lecture you taking this class for credit make sure to switch over to the other tab so you can get credit for the day two lab so we're going to be almost exclusively working with I'm just going to um get my PG admin set up just I had a nightmare morning today by the way I um have posted about it but um my phone decided that it wanted to do a factory reset today so that was fun um so that was a bit stressful with this events table again um the is the table we're going to be working with and uh initially what we want to do is I want to figure out for every person who goes to my sign up page how many of them actually sign up so one of the ways that I can show you uh so there's essentially two uh URLs here that will determine this so if I say we URL in then I say sign up and I say comma um AP back SL API V1 users okay so you see here are okay and you see how there's some some of these are API V1 users and some of them are signup so essentially what these are are doing here what we're going to be doing here is we're going to create we're going to create a funnel say I go to Zack wilson. Tech e s second great and then I go to sign up all right so here's the signup page and if I actually sign up what will happen when I hit this submit button is behind the scenes you'll see in my uh in my code here there is this where is it um API B1 user yeah so you see right here this create user route it used to be called users in the old data that I made a change and like that's why we're it's users because like this is data from six months ago and then I changed it to user because user is actually correct when you're designing endpoints uh like all your entities should be based on uh like the actual user um object or it should be singular objects right so anyways the idea here is is you'll you'll see some of these are actually going to be like kind of connected where people are going to go to that page so you see here this user actually did it right this person was they they visited the signup page and then they created an account that's literally what what happened for this user so what we're going to do is we're trying to figure out what percent of users who got to the signup page actually signed up that is our plan here today and uh this is going to be um more tricky than you would think and we're going to do it with no window functions we're do this with a self join so if you guys want to follow along the queries are already in uh py charm in this applying analytical patterns uh you'll see we'll have this um funnel analysis query that we're going to do so one of the things I know about this is like my logger and uh this data set is actually a little bit buggy in terms of uh like duplicates so what we want to do first is we want to get rid of those duplicates so let's say duped events as and then in this case I'm going to just take this and pop them in here then remove this and then we also have null user ID so we're going to say we're user ID um um is not null and then what we want is we got user ID we want a URL and then there's event time and then we can also get date of event time and this is as event date uh I'll explain why we need this in a second because we essentially want it to be like we we care about the time stamp but we also care about the date so then the last thing we want to do here is we want to say Group by right Group by user ID URL event time and date event time so this is going to give us our first set of data right so we say like select star from deduced events so let's just look at this real quick oh forgot to buy so we do this bada bada boom we have all of our data it's now duped because of that group bu great so now what we want is we can also filter because we only care about two events in this case we care about sign up and uh like visiting the sign up page and actually signing up so we're going to say URL in in this case we're going to say back SL signup and back slash um API V1 users so those are our two events that we care about and we can filter everything else out for now so that like it also will make the query a lot faster it kind of shrinks the data set down so now what we want to do is we want to say okay did this user ever make uh a sign who visited a sign up page did they ever sign up after they visited signup page because we don't want it to be before because technically you could like log out like you could sign up and then log out and then go visit the sign up page again just like what I did I did that literally in this lab today and we don't want and just because I create I I did sign up earlier we don't want that new signup to like have that attribution so in this case what we want to do is we essentially want to join the table on itself so that we can have the sign up event and the uh and the visiting the signup page and the actual signup event happen on the same row on the same record right so in this case we're going to say we're going to say DD events D1 and then we're going to say join dded events D2 and then in this case we have on all right and then in this case we're going to say Obviously the first one is D1 user ID equals D2 user ID that's like the obvious one and we also want to put in and D1 event dat equals D2 event date that's going to be uh probably another way way that you would want to look at this uh this is an interesting one where it's like okay do we measure conversion over a specific time uh like we only count the visit if it happens within the same day uh I think that makes this an this analytics a little bit easier because otherwise like if you just join on user ID and then you have this last condition right there's one more condition here which is and D2 do event time is greater than D1 event time so what this will do like let's not do select star but let's do um we'll say D1 user ID and we'll say uh D1 URL D1 or D2 URL D1 event time D2 event time so let's let's run this query this quer is going to be a little bit slower but oh not not too bad so um oh interesting uh okay so we have one more here though right we need we need the join to also include uh like see the the URLs are matching here and we don't want them to match like and uh so that means this this person signed up like twice or something they must have like double clicked the button or they updated their account or something like that so what we want to do is we want to add one more condition in here we're going to say D 1. URL does not equal D2 URL because that's um that's a mess and uh so now this will give us probably better okay but the problem here is oh there's actually one more here because we have the uh we we only want the D1 we want this to D1 we only want to care as the signup page visits and you see like we have both now e so uh I think that would be in the wear Clause though so we can say where D1 URL equals back signup and okay so we want to do uh on D1 user ID equals user and D2 D1 event dat equals D2 event and D2 event time is greater than D2 D1 event time and D1 URL does not equal D2 where do you want yourl equals \[Music\] and v2. URL equals back API V1 users and we can actually get rid of this does not equal condition because like this essentially will make it so like the we Claus is going to do that in Big Data we might want to keep that because it will filter out more of the data but um in this case uh we probably don't need it okay there we go so now we have a we have a visit that turns into a signup that is essenti us and uses let me just see just figuring out why this isn't showing you okay I'm just gonna have a look at the the code that Zach ended up posting hian how are you for okay so I don't have the same data that um Zach has apparently um do I need to just going to run his um I'm just going to on his thing that he created yeah it's not in there I'm just going to keep playing the um I'm just going to keep playing the uh lab for now and then see what's happening here and you'll see some of these events are pretty close and then see if there's anything on the uh um Discord about it right this visit like these are not that close but they're like you know like this is what 16 minutes apart like so obviously there's probably a um like you would want to have like an attribution window of that sign up visit like because you could like imagine you could visit the signup page leave and then come back later like uh um that you come back at like a um like a day later and we would still get the attribution here right and but that's fine so we have all of these signup events that are are here but you'll know that like some of these events actually do not have like there actually isn't uh um uh like a created event because they could visit the sign up page in a banded so that's where this one this this wear Claus is actually buggy right because this right now is filtering down to only the people who have converted so we don't want just the people who converted but like and that was the whole point here was to show you okay these are the people who converted and this is what's going on so what we want to do is we essentially want to create another uh we're going to call this like self jooin as and then this will be uh this query here this will give us the uh kind of self dra query but we're going to remove this second wear condition because we don't care about that we will do that in the um uh in the next in the next query here that will show us like how this is going to work so we say select start from self joined you'll see in this case now there's going to be all these other things that can happen right um CU oh wait yeah yeah that that should be fine so you'll see sometimes there'll be a user who has like just a sign up or they will have like another record here on the other side that will um potentially be um this is interesting why are these all like the same because okay so this person visited sign up like 18 hours apart that's essentially what's going on this person like refreshed sign up a lot before they actually like created it but that's fine because uh like we would expect that to be the case but what we want to do here is we want to aggregate along users essentially so we can say uh so I have user ID and then uh it doesn't matter how many like uh joined events here we have because obviously there's going to be a lot so in this case we have user ID and then uh obviously the URL is just going to be sign up so we don't need to have that but we can have a a oh we need to change this one uh we need to change this as destination URL because otherwise we have see have URL and URL so we need to change the second one so what we want to do is we want to say count we can say case when uh destination URL um equals API V1 users then one end and this is going to be as um as converted and oh wait a minute because this is but this this is probably going to double count I want to make sure like because we only want to count each user once like whether or not they converted okay so I think oh yeah see this is going to double count so what we need to do here is we need to throw we need to slap a distinct on there so that uh it's just like a a Boolean right so we can see uh that like is actually there or uh you can actually no that's not the way to do it just that's like super ugly you want to actually do Max right Max is going to that'll give it what we want then we want to put lse zero here so we can have the so now this means that that user converted and these other users didn't convert even but these other users did visit the signup page key thing to remember here so we now have our uh users who convert it and the ones so but we can also have uh another column here right which is going to be so we have the converted column so this is essentially our next like this is like user level as so this is going to give us our users who have converted so now we have user level and then we want to do one more aggreg right so if we say uh select and then we can say count one commed from user level this is going to give us one big old number here right and so you'll see in this case we have uh our number here of we have 145 counts of uh users and then the sum here is 49 so we can also do a division here right and we can see like cast as real so this is going to be our like overall conversion rate and but keeping in mind this conversion rate is different than the one in uh in here because in here the difference is is I um okay because in this one the difference is is like we do it where it's at the um it's at the URL level instead of at the um the user level so essentially what this means is 33.7% of all the users whoever visited the signup page will convert that's essentially what this means right because we have all the users here and then that like and that was the conversion rate so this is going to be as converted and so this is going to be one way to do it um I want to compare this though because I think that this this is going to be very interesting to y'all so if we throw in uh URL here and then instead we group on uh both username and URL but in this case we also need to put a count so count one is this is as number um of hits so because just because uh the conversion rate is that good where it's this high right that doesn't necessarily mean that that's the level that you would expect from each individual uh like web traffic right because some users might have visited like the sign page 10 times and then they finally uh converted on the last go right and so that that that's different that's like the per hit as opposed to the per user conversion and so in this case uh you'll see where what we can do here is we actually want to uh stomp out this line 21 so that we can just have like wherever they're coming from so in this case we're gonna have URL and then we're gonna have Su number of hits uh number of hits uh and then we want to group by URL and so now this should give us a a fairly different picture like how is that oh yeah because we need stomp out that we got have like all the hits now this should us very different picture okay there we go so yeah I'm trying to find the um I'm trying to find the events details let me just see if um if I go to the GitHub I just want to see whether or not like the DAT you know like the data dump has been updated or something it's definitely not been updated since um is there different maybe if I do that what I just do that maybe it's just not in this data so I'll just watch the lab oh yeah that makes sense that API users uh convert to themselves very well okay so what I want to show here though is um this is not quite right because this is we want to not divide by count one we want to divide by number of hits because count one actually doesn't give us very much so this is going to be uh let's put some converted outside here as num converted and then this GNA be um as num hits and so we want to put a having clause in here because there's G to be a lot of garbage you see all this like marijuana. PHP or just like people like hit my website and they try to go to random paths all the time it's like super obnoxious so what we want to do here is we want to put like a having here we can say having um some uh number of hits greater than maybe like let's put like 500 so that like you don't have all these like stupid hits in here then we can run this query okay there we go so that no that didn't run there we go see it's a lot slower now because I didn't filter out anything there we go so now we have our like we have about 110 records and you'll see that like some pages are going to convert better than other Pages like which makes a lot of sense that you would expect that so in this case we can sort and okay so now this is going to give us a lot clearer picture of who converted where right and this these numbers seem like a lot clearer and a lot better so you'll notice uh that some of these Pages uh like we have like essentially like a a 3% like conversion right here I think there's going to be a oh there is a divisor problem here though because this converted is like a one-time event whereas uh the other one is not right the other one is going to be the all the times that they visited the website so like even if I so you know I signed up but then if I visited the sign up page again I'm not going to sign up again but that's still going to count in the denominator so that's why these numbers now look a lot worse you saw how like in the previous example we had like it at the user level and the conversion rate looked like 30% and now it's like. 3% but that's because of like all of the like there's a lot of bots and a lot of other things that are in here that are going to be uh really dragging this number down quite a bit um and so that's going to be like probably the big thing that I would say that's definitely happening here um but uh this is the idea but you can see how like oh yeah if they actually signed up and like then that's going to be where you get this conversion here I'm curious like if we change this to a sum does what does that number look like okay so okay this this is probably actually more accurate yeah this I think this is going to be more accurate on like the like what I would say is the numbers because obviously if they signed up they signed up but like in this case why these numbers are different right that's a that's an interesting thing and the reason for that is based on the self-join logic right and because like uh the sign up doesn't uh isn't preceded or isn't followed by a signup or what that means is that like half these is like a sign up is followed by an edit and so that's essentially what's happening here but this this conversion rate seems like more accurate for what you would see in like normal traffic right so it's like 2 2.7% or so that's kind of the perspective that I would expect uh when I'm uh creating my funnels and doing my traffic is it's like okay they went to the sign up page and now they're over here and it's now 2.7% which I think is totally reasonable so and you'll see that like some of these Creator pages also have like a decent percent because like they can edit as well and so this is kind of the idea that and but you'll see some of these Pages down here are terrible like so it's like the about page so if someone lands on the about page the odds that they sign up is very low it's like 0 2% and that these numbers like I I I trust these these seem like pretty accurate now so anyways that's the idea behind funnel analysis right the idea here is you have two events right and you have like you give this is like fact data in a lot of cases right where you have two events that like you want to see like this one came after this one and uh how often does that happen versus other things happening and that's what we're g to that's essentially what's going on here when we're going through this lab together that like now I know that like I probably need to increase this number because this 2.7% is not good enough like I think that more people should be giving me their email and converting probably need to optimize things but obviously like this data only goes through like this is only up through like March like I think this is only like the first month that I launched my startup I don't have data after that so uh that's that can also be a little gotcha um for this so um now what I want to talk about is I have another one here around grouping sets so what I'm going to do is I'm going to just grab this real quick going to grab this just to start with and then um we can go and work for a it from there okay so if I say select star from events augmented like you can pull this query in from the repo uh this query now has uh just a couple things so what I'm trying to do now is just look at my website events and try to see okay what event is um or or what type of device is the most common device that hits my website and I think that we'll be able to do a lot of really cool stuff with this as well so we have all the all this device information here so you could imagine that what we could do uh if we were doing this like kind of in a na approach we could just say OS type device type browser type and then we want to say count one and then we can say Group by OS type device type browser type right and let me just like format that real quick so this query is obviously going to run and then we can see oh we have all of our data here and you'll see that like the most common one for me is Android on generic smartphone on Chrome mobile so that's uh the most common one but like how what about all like I want to know about just Android traffic not like Android generic smart home Chrome mobile traffic I just care about Android I care about Android but I also care about other Cuts here so what we want to do is we're going to introduce A New Concept here called grouping sets so if we go in here we can say Group by grouping sets and the first grouping set like so you'll see if I don't put everything in the grouping set this query will fail because you have to put all of the all of them in there at least once otherwise it's not going to work so you can have that first like and this is essentially the finest grain which is our the old data but then we can also put in what if we put in like browser type and we can put in OS type and we can put in device type so these are three other uh grains that I want to put in and you'll see now if we do this okay there we go so now you'll see um we have uh like if we sort on count here okay you see these nulles see how there's nulles here now so those nulles are because of grouping sets so that's because those are ignored so what that means is this is the browser type uh column that's what that means and so a lot of times what people do is they're going to put coals on these columns so that the the data doesn't look so weird and then this is going to be our and they coales it with overall in parentheses to show like hey this is our overall column and it's kind it's similar to like you know like a grand total or like a subtotal if y'all are like familiar with like Excel that's like uh where I would say this uh kind of has a similar vibe to it right and so this is going to give us our you'll see now if we run this query that's just going to get rid of those NES and then we'll have uh can scroll up here and you'll see we have um we have a better idea here where looks like Chrome mobile is going to be our biggest one but I don't know if that's true we say order by um count one descending I don't think that that's right okay there we go okay so it looks like uh actually other is the most common and I think that's actually from uh Bots like Google bot and stuff like that so and then after that we have like iPhone and then we have IOS at the highest level then we have iPhone then Chrome then Android right those all make sense right then generic smartphone I love that I love that that's the name of it and then there's that Chrome mobile hit and obviously it looks like these ones like Android generic smartphone and Chrome mobile these are like almost the same you see how these dimensional cuts are like almost the same so there's like a small number of values here these two spots that are different but what you can see is like if you use grouping sets you get a lot of really powerful information here so one of the one of the things I want to talk about though real quick is there's actually another thing here called grouping um I want to say this has is it it's like this I want to say you use that I will uh let's uh see real quick if that gives it okay cool so okay so what happens here is is if the so this is how we determine if the grouping set includes or IGN change this to log in for people said to use login but then the previous URL is not loging e I think the data something's wrong with the data um bye I'm just going to try that for now this might be well but I'll that out the dayes uh each of the things here right so let me let me show you more like if we add more of these groupings in here so if we say grouping of of we'll say a device type and grouping of browser type these are different right and um but like I think it'll make more sense when you see like all of them together so okay so we have this overall other and you'll see uh in this case the grouping is not device type but uh what this means is device type is is uh set on and then the other two are off that's what this means right because you see how it's like overall here so in this case that means that grouping the the device type grouping is there but the other ones are not so and you'll see in some of these cases like for this Android generic smartphone in this case that means it's all three so what we want to do is we want to essentially build a a new column for this table that uses these groupings so we can say case when uh OS type equals zero then OS type and then uh uh that's going to give us our first um that will give us like that that first aggregation right to understand like how all these work together right um so there there's essentially like the way you want to do this is you want to do this in a smart way where it is kind of it looks at like what your choices are here um so in this case we know that if uh they are all zero then uh like let's do that one first so we're going to say case when grouping type equals z and grouping uh device type equals z and grouping um browser type equals z then this is our uh so in this Cas we have OS type device type browser type so like at Facebook what we did was we kind of like split it out where like in between each uh column name we put a double underscore to signify that that's the aggregation level so then uh like then the rest of them are just sing singular right so then we can say when grouping browser type equals zero then browser type and then when grouping uh device type equals zero then device type when when grouping um OS type equals zero then OS type and then this is we can call this as aggregation level so now if we run this you'll see we have this nice aggregation level column so what I want to do is I'm just going to uh get rid of these top level groupings here and um now we have our aggregation level this we're going to say is as a number of hits so we have this table and what I want to do is I want to call this we're going to create this as a table we're going to call this create table uh this is going to be a divide um hits dashboard as so we're just going to create this table real quick and then we're going to run this okay so now we have our table so if I say in this case I say um select star from device hits dashboard where aggregation level equals device type like this you'll see okay now we have uh we can see okay here's our aggregation level device type and then we can get all the hits and then you can also say like OS type and then you see how this is so powerful because now you can like just you don't have to query the event data and it say pre-aggregated and like you can now have uh like your dashboards can just be powered based off of we conditions and not based off groupi conditions so and then obviously you have the the big Behemoth one right the this one OS type device type like this guy and so you got to teach your analyst about the double underscore kind of uh Mech mechanism for that aggregation level but then you can see like okay this is uh like like the the cut that is the most common is going to be these three together if we actually include all the dimensions so um that's a what how this works um I kind of want to go over how this is different from like how this is different from like uh like how grouping sets versus uh roll up and all this works uh before that though I want to uh move this query into I want to move this query as this quer is better it's a lot better than this one so that and you could put other things in here right and in the old query I had like some like signup conversion rate stuff that was in there but like those numbers weren't even real that really we have like this number of hits is going to be the big uh thing that we look at so if we change this right if I just change this real quick and I say root by Cube and then I put OS type device type and um browser type in here and then uh I got to get rid of this create table real quick and then comment this out so if we just change this the cube real quick you'll see that um sometimes we're going to have an aggregation level that's not there because this is like the overall aggregation level and um then you'll see all these other ones that are going to are going to show up here that are based on different uh versions right but the thing is like with Cube you have other combinations that you need to wor about you have the double combinations like OS type and browser and um browser and device type right you have those like the double combinations that you got to worry about so um some of these aggregation levels are going to be wrong right you see like this one here like this this one is not device type because it has Linux and other this one's actually uh device type and Os type together uh that's how that one works so I think there's another like I want to say that there's also one called grouping ID is there is that oh no it's I think grouping that's in Presto I think that's like a different way to do it but so you'll see with Cube one of the big things you get is you get that overall right you get this like overall one that's like essentially like grouping on nothing you get that like like the that million hits or whatever that's in this data set and you just have all the records and so that you can see how like cube is pretty awesome so I just want to compare that with rollup so that y'all can kind of see very vividly like what's going on here so in this case with uh with rollup okay so okay rollup still does okay interesting rollup does still give you the um uh the overall uh aggregation but then it also gives you uh then you'll see that for the rest of the time though that OS type is never overall it's always it's always present but then you'll see that uh device type can be gone and browser type can be gone but sometimes they're in there so this is what I talk about the hierarchy right so OS type so rollup gives you like the grand total which is nice but then OS type after that will always be in and it won't go overall and then device type can be going overall and browser type can as well uh where but like but then uh when you'll see like if a device um or if browser type is over all I if device type is overall browser type has to be overall you'll notice that that's going to always be the case because that it's a hierarchy right so um it's kind of like that whole country state city kind of hierarchy and so um in this case the this would be like the state and that would uh that's that's essentially what's going on with that data so that is essentially how you do um fancy like grouping operations in post press congrats on finishing the day two lab and the course I hope you enjoyed it if you like this Channel and like this boot camp make sure to like comment and subscribe and share this content with your friends I'm so happy that you're taking this time to invest in your knowledge and getting better at data see you at the next \[Music\] one I think I might have to rewatch that one because it was a bit um know I just realized I've watched that on the um day two lecture uh La great um I'm going to have to rewatch that one anyway because it was a bit it was a bit all over the place but um um oh well here we are uh right I'm going to end there because it's been a long day um had pretty intense morning um yeah quite like that that's quite nice um quite like some of these changes that that they've done um so yeah but I'm just going to leave that to run make sure you're taking this class for credit make sure to switch over to the other tab so you can get credit for the day two lab so we're going to be almost exclusively working with this events table again um this is the table we're going to be working with and uh but I'm going to end there and I'm just going to let it play because I have watched it um and you've seen me watch it so I'm not cheating um yeah okay go I'm going to end there thank you for following this with me even though it was a bit of a it wasn't the best um best following of it it was a bit confusing it it's always really hard when the data doesn't match the lab because then you just debug in the data rather than following the lab so um that was that was tricky but yeah I'll see you next time bye everyone


 > [!info]
> - **SQL Interview Tips (2:03):** Data engineering SQL interviews are more relevant to the job than software engineering interviews, but they still test you hard.
> - **Window Functions (3:04):** Interviewers may ask you to rewrite solutions without window functions, which can be tricky and sometimes absurd.
> - **Recursive CTE (5:07):** Recursive CTEs are often asked in interviews but rarely used on the job, except for parent-child queries like manager reports.
> - **Table Scans (9:16):** Number of table scans is crucial for SQL query performance.
> - **Count Case When (9:39):** `COUNT(CASE WHEN ...)` is a powerful technique for pivoting data efficiently.
> - **Clean Code (10:24):** Data engineers should prioritize writing clean SQL code with commentable expressions and aliases.
> - **Explain Keyword (11:51):** Use the `EXPLAIN` keyword to understand the query plan and optimize SQL performance.
> - **Grouping Sets, Cube, and Rollup (13:16):** These are ways to perform multiple aggregations in one query without unions. Grouping sets offers the most control, cube gives all permutations (use with caution), and rollup is for hierarchical data.
> - **Window Functions Usage Signals (14:40):** Look for keywords like "monthly average" (rolling average) to indicate the need for window functions. Avoid `RANK`; prefer `DENSE_RANK` or `ROW_NUMBER`.
> - **Data Modeling vs. Advanced SQL (29:27):** Bad data modeling leads to complex SQL gymnastics by analysts. Data engineers should provide well-modeled data for simpler queries.
> - **Symptoms of Bad Data Modeling (34:40):** Slow dashboards, queries with many CTEs, and excessive `CASE WHEN` statements indicate data modeling issues.
> - **Storage vs Compute (36:42):** Storage is cheaper than compute. Materialize intermediate tables to save compute costs and improve analyst efficiency.