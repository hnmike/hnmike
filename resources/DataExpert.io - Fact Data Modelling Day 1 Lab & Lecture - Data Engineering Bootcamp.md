---
title: "DataExpert.io - Fact Data Modelling Day 1 Lab & Lecture - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2024-11-28
source: "https://www.youtube.com/watch?v=8LgRIZJv-is&t=1350s"
image: "https://i.ytimg.com/vi/8LgRIZJv-is/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgZShlMA8=&rs=AOn4CLD0RQ_o2sH7NjSPQbaBou_lYucrEg"
created: 2025-03-23
tags:
  - "youtube"
  - "data_engineering"
  - "fact_data_modeling"
  - "data_quality"
summary: "DataExpert.io - Fact Data Modeling Day 1 Lab & Lecture - Data Engineering Bootcamp. Thank you for joining me on my Data Engineering Bootcamp journey! "
---
# DataExpert.io - Fact Data Modelling Day 1 Lab & Lecture - Data Engineering Bootcamp

![DataExpert.io - Fact Data Modelling Day 1 Lab & Lecture - Data Engineering Bootcamp](https://www.youtube.com/embed/8LgRIZJv-is&t=1350s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> for for for what's a fact like most people think of a fact as like truth something that happened something that occurred like some sort of uh evidence something like that \[Music\] like fact data is the biggest data that you'll ever work with in data engineering when I worked at Netflix there was some fact data I worked on that was two PAB a day of data so why is fact data so big compared to dimensional data fact data is every event that a user can use so you know a company like Facebook has two billion users every user can do a thousand or 2,000 events every day by all the things that they're doing so you do two billion times a thousand you get to trillions of records every day that are generated so when you're modeling your fact data you got to be very careful because the volume can cause your Cloud bills to go up a lot so in this 5H hour course that have put my heart and soul into we are going to be covering all of the details and the nitty-gritty about how to manage both small volume and large volume fact data so in day one we are going to be covering fact data modeling fundamentals like what is a fact like how to model a fact like how to make it so you have facts that join with Dimensions it's a lot of like Kimble data modeling in the lecture and then in the lab we will be covering how to build a fact data model on top of the MBA game details table in the postes database like if you don't have postgress setup follow the link in the description below for the setup instructions uh in day two we will be covering the blurry line between fact and dimension so when you aggregate a fact it can kind of behave like a dimension and it kind of gets blurry of like what's a fact and what's a dimension and we're going to be going over that in detail like how to kind of draw the line between these things and we'll also be covering the date list data structure which is a very powerful data structure that is used at Facebook to model user activity so you can look at like the last 30 days of user's activity as one integer so it compresses the data a lot and it allows Facebook to compute monthly active users very very very efficiently and there's a link in the description below for more details about the dateless data structure and then in the lab we will be building the dateless data structure and using fancy bit operations that are in postgress it's going to be really really exciting and then day three will be a deep dive on Shuffle and how you can use Shuffle in spark and trino and how you kind of want to minimize it and the way that you minimize it is through this thing called A reduced fact so A reduced fact is a way to minimize the volume of all of your fact data by preserving the most important bits and by building reduced facts you can supercharge your analytical patterns so at Facebook I built a reduced fact framework called the long-term analysis framework and what it did was it enabled decade long analyses that used to take weeks for data pipelines to run to now be done in hours so the way that that works is by minimizing completely eliminating Shuffle so that allows your pipelines to run as fast as possible and then in the lab we will be covering how to build a reduced fact framework so that you can minimize and supercharge the analytics at your company so I'm really excited for you to check out all this content I love building this content and if you want to learn more about how to build this stuff in the cloud with high success Frameworks like trino and Iceberg I highly recommend checking out the data expert.i Academy in the description below we have all sorts of this content oh nice what what was it that made you eventually decide to sign up for the boot camp it's really interesting or it's really drwing I guess cuz it's very different to what I'm currently working on at the moment at work and so I'm in a deep te I was in a deep technical PR today at work and then I'm switching to this and it's just a it's it's a lot of context switching yeah I'm a bit yeah it's bit it's bit draining but we have we have to do what we got to do that is completely done in the cloud so I hope you enjoy the course I spend a lot of time on this and uh you know get cracking there will be time stamps for every single section in case you want to skip to one or you know go to whatever pieces you want since this is a long course so what's a fact like most people think of a fact is like truth something that happened something that occurred like some sort of uh evidence something like that like a big thing like you think of like an action maybe like a user logs into an app you you venmo someone $30 you run a mile on your Fitbit even though technically running a mile is uh a little bit more dicey in terms of like what is a fact because of the fact that a mile is is is more of an aggregation because of the fact that you have all of the steps you can think of each individual step in that mile as like the lowest granularity because you can't really break you can't really break down a step into any smaller component so that's one of the things to always think about when you're thinking about okay what is a fact like it should be something that really can't be uh it's like Atomic like you can't break it down into a smaller piece and running Mile with your fibit you technically can right you can you can break it down into the individual uh steps or individual pieces of that all the way down and so that is something to think about uh facts can appear in all sorts of layers and granularities like that where you can aggregate up you can aggregate down all sorts of or or you can you can push it back down and like be at the atomic level or aggregate up to like something like a mile so uh one of the things that's really really really nice about facts though yeah so just like that's an interesting one cuz obviously like steps for for me versus someone else is going to be very different in in length so I'm not sure that would be a good measurement maybe like you know I guess more would it be more like you know um chunks of like 0 like one kilm and stuff like that hi mon Hi ymin how are you both doing and yeah um that's that's one of the reasons why I joined as well because he seems like a good teacher so um that's that's I I like I like teachers who are quite um clear and concise so he seems that is uh they don't change they like it's like I took this step at this time and uh it's not like I can go back and change that it's one of the beautiful things you know like I I I I know that's one of the things I I talk about in therapy sometimes is uh you can't you can't change the past and and facts are just the past like you can't change the fact that you logged into Facebook 30 times today and maybe you need to have better boundaries with your phone or something like that but what I'm trying to say here is you don't have to worry about that part that was a part of Dimensions Dimensions have this like slowly changing transitive sort of property to them that like yep I've got like a notepad here I I like to write my notes on my notepad I think um for me it just sticks better um so I write things down kind of make them hard to deal with but you don't have to worry about that with facts which is a beautiful thing very beautiful thing but facts have their own set of challenges and their own set of problems that like honestly like when I when I compare facts and dimensions and I had if I had to pick which one is easier like I would say dimensions are slightly easier and uh mostly because of what we're about to talk about Let's uh let's go to the next slide okay so facts are uh more challenging because there's a lot more of them if you think about it like let's go back to the the number of steps that you take every day so if you have 10,000 steps a day that's a lot more data than you as a person right then so it's like instead of having one take one row of data for you as a person you have 10,000 rows of data for every step you took so uh there's a lot more data so and it will scale like way more right and like that's an extreme example uh where like I would say the more of the general rule is there's going to be between 10 and 100x uh the data that you would see uh compared to your dimensions and the the big the big way that you can kind of figure out how big the uh fact data is going to balloon is based on uh how many actions are taken per Dimension like for example when I worked at Facebook uh we sent about 25 to 30 notifications a day and so what that meant was from the two billion users that balloons to 50 billion notifications and that's I that's just a lot of notifications and I know that a lot of people like turned off their notifications and all sorts of stuff like that because it is like just an excessive amount of notifications and we're going to be talking quite a bit about these notifications and how they worked and like kind of some of the challenges that I ran into with this very bulky fact data uh another big thing that can be important with fact data that is less important with dimensional data is you need context for Effective analysis for example um say we sent a notification well like that fact in isolation is pretty terrible it doesn't really provide any insight but say we had say we sent a notification and then 20 minutes later you clicked on it and then five minutes later you bought something and we have that funnel of like sent to click to bot and we have that like funnel analysis and then we have all three of those facts in a in a line and then that goes like the first one being like Oh we send a notification in isolation that is worthless but if you have those three facts together then you can have a $50 billion doll business because that's all that's literally all Google and Facebook are are conversion funnels where it's like hey you saw a you saw a result you clicked on that result and you took an action and they they are they have optimized that really really strongly I mean I don't know if y'all have seen the where Google ran this experiment where they tried out 40 different shades of blue for the links on their website and then one shade of blue uh caused people to convert and purchase a little bit more than the other ones and that was that made them like like I think like 10 or like something like 10 or100 million some some weird number of millions of dollarss for just like freaking changing the colors of blue and so like one of the things to think about like when you're working with fact data is what other data do we need or Dimensions do we need to make this fact data more valuable like for example another way to think about it is say we don't have that conversion step that purchase step but we have the we have the sent and clicked so we have like the Click through rate um but what if we had another dimension we brought in the user Dimension and we now we can see like oh you users in Canada have a better click-through rate than users in the US and it's like why and that can kind of like kind of slowly build up kind of intuitions and have you ask more deeper questions for your fact data so that can be a big one thing when you think about like K like like this though is just how much um is needed to like or how much you don't realize is going on behind the scenes um at like companies like this to get like the performance and the uh ability to like just do the things that you do on on the applications um though like I do think like when you're talking about like how they've optimized this funnel where it's gone a little bit mad in some of them so like on Instagram it's ridiculous I am getting married at the moment and well I'm not getting married right this second though but I'm I'm like engaged and I'm plan in a wedding and I saw five adets in a row on Instagram five when I was going through my like um friends stories like ridiculous sorry that was just around yeah big thing another really common and annoying thing about fact data is that duplicates are going to be way more common way way way more common and uh duplicates can be caused by a couple different like angles of things like maybe uh the software engineering team they push out a bug in the logger so every time someone clicks it actually logs to records and that's more of a data quality error but you can also have duplicates that are genuine like for example in notifications you can have it where you send a notification you click on it like in in in hour one and then you click on it again in hour 7even and those are both genuine actions that we want to log but we don't want to count that as two because then like we could have it where it's like oh our click-through rate is 200% which like doesn't even make sense like you need to be able to DD a lot of these records uh when you're working with fact data because if you don't DD like your metrics are going to look really weird and they're not going to be like what you expect them to be and that is actually probably one of the most challenging parts of working with fact data is this kind of D duplication step that can really um cause a lot of pain and frustration so yeah let's let's go next slide how does fact data modeling work well you can think about it in two ways on one side you have uh uh there's normalized facts versus denormalized facts and they both are very important and I've seen uh both of them actually be very powerful and I've seen both of them cause a lot of problems uh in both cases like in the lab today we're going to be showing how denormalized facts are causing issues but uh in a slide here in a second we're going to be talking about how normalized facts can actually cause issues so what is the difference between normalized and denormalized well so imagine instead of when you when you have your record of like Zach logged in at this time but it wouldn't be Zach it would be like user ID 17 logged in at this date and so you would have that as a record but maybe uh for your analyses you want to not do a join so instead you would say Zach 29-year-old male uh who lives in California made this uh logged in at this time so then you bring it you can bring in some of the other dimensions from like the other actors or objects in the fact data and that can make it so you can just do group by and you don't have to do a join and that can make things faster but it can also obviously make things duplicated because then it's like okay what if I have like 50 facts a day then we just copy my we copy that 29y old male in California 50 times that doesn't sound very very efficient sometimes that is the better option and we're going to go a little bit deeper into like when that is a better option and when uh you want to go more of the normalized route the key thing to remember here is normalization like the smaller the scale the better normalization is going to be as your option because when you have normalized facts you remove all the duplicates and you increase the data integrity and you don't have to worry about restating stuff so that's where normalization especially at the smaller scale normalization is going to be a big win um and so there is a trade-off between these two and they both work pretty well and so yeah let's let's go a little bit deeper into this so one of the things that I think people get wrong here is uh raw logs and fact data are not the same logging and fact data are like inextricably linked they're almost married where like if you don't have logging right it's very hard to get fact data right um so how are they different i' would say a big difference is raw logs are usually owned by people who don't necessarily have really strong data skills I know at uh Faceook Facebook when I work there the raw logs are owned by software Engineers who mostly are responsible for working with online systems and keeping the server running and they don't really care quite as much about the actual log data so that's where you can have a lot of impact as a data engineer is you can work with the software Engineers to get the data logged in the correct format because that will make your life in the fact data modeling layer a lot nicer so that's one of the the biggest problems right with fact data is you usually have ugly schemas uh that are from the online system and you have other you know it could contain duplicates and other quality errors the raw logs are don't have any quality guarantees um the fact data is going to have longer retention nice column names quality guarantees it's going to be just a lot better like the trust in the fact data should be an order of magnitude higher than the Trust In The Raw logs and that is one of the biggest things that you can do as a data engineer is being able to convert those raw logs into highly trusted fact data you you think of like facts from this angle of uh think of this as a there's a few here you have who where how what that's that that should be who where when what and how sorry that that there's a double how in there I need to fix that okay so um who fields are usually pushed uh as IDs you can think of this as user IDs this could be uh surface IDs this could be like imagine like a Tesla has a car and they're driving down the road there's the ID for you as the user there's the ID for the car like the car ID and you have uh also you have maybe an identifier for the operating system all sorts of things that like help you identify who is part of the event and then you have where where in this case can be a couple different things this could be a location like a country or a city or a state but this could also be where in the app like are you on the homepage are you on the profile page are you on like where in the app uh so this can be uh very similar uh where it it can be modeled with IDs as well uh but a lot of times they it's not modeled with IDs it's model is just like like the the pages are like back SL where or it's like the country or things like that um how and where a lot of the times in uh fact data modeling are very similar uh because we're in this virtual world and so it's like he used an iPhone to make this click so is is is the iPhone the where or the how because it was on the iPhone but I would say the iPhone in this case would be the how and uh the where is going to be the where on the web page or in the app or what button they clicked that is more of the where and the and the how is going to be more of like the methodology and that you could think of uh this is one where it gets close between who and how are also um interesting like in the Tesla example you could think of like uh the the Tesla car as more modeled as a how as opposed to a who But It's tricky because it is like some there's ownership there because this person owns that car so it's like it's kind of in the middle so definitely uh those two are kind of really closely linked um then you have uh we have the other one on that we need to talk about this is going to be the event like the actual thing that happened so like in notifications there was a funnel of events right you have like sent or you have generated which is like a notification that could be sent you have the notification that is sent you have the notification that is delivered to the device and you have the notification that's clicked on and then you have the notification that's converted into an action Downstream whether that be like a like a comment a share a purchase something like that and uh you can think of all there's all sorts of What fields like someone made a transaction someone took a step someone uh did some sort of event someone through a party there's all sorts of different uh events that can happen that are like what and think of those what events usually as atomic and uh because you could think of an event that's like a little bit higher level like for example you have like I threw Burning Man as a party and that's like that is true that that's like an event and there was probably like but like that like is more of like a group of events because that's like a week-long thing but you could think of it as like a shorter level of like I registered with the Bureau of Land Management to be okay to get that just to throw this party that's more Atomic right and you can think of like all the kind of atomic level things and then those higher level components are like okay those are more aggregations of because like you think of burning man is like the all of those events that are happening at one time they can kind of all aggregate up into the whole party or like it's like a it's like a dimension almost at that point because it's like okay all of these events are tied to this one thing and it's like a kind of an aggregated view of one uh kind of set of facts and so but we're trying to talk about modeling IND individual facts so remember that the atomicity of facts is also really important and then this last piece is also critical for facts is the when uh the when is going to be almost always modeled as a time stamp or a date usually a time stamp and it says like okay I clicked on this notification at this moment in time and so that is critical for facts like without the when like you don't know where to put it and and like some things to be careful here about is that you really do want to make sure that all of your devices are logging uh the same uh time zone and this should be all be logging UTC time zone because a lot of times like when companies are moving to being more mobile first where they do a lot of the logging on the client side they do client side logging then the client side logging happens most of the time in the time zone that the client is in which is bad for uh when it gets pushed to the server so you want to make sure uh in the Cent true story I went somewhere where we had to like do this change of moving things from cuz like in in some parts of the system it had been saved as local when it should have been saved as UTC um and it was it was a nightmare it was a nightmare because because we had very specific so so the place that I worked you had um you had event time zones and customer time zones as well so the clients sometimes wanted to see things in where they currently were but also in the time zone where their event was hosted and yeah like the the way that the data had been saved um in some of the parts of the system was made it really tricky it it was really really nasty to kind of resolve but but yeah this is a very important tip save things as time zone like UT see because it's a nightmare to to if you don't an app that they are logging things in UTC not in whatever the client time zone is so that when it gets pushed to the server all of your time stamps are in the same time zone so that you can easily know like that like when they actually happened and you're not it's not all shifted because of weird time zone problems so that's a key thing to remember when a company a lot of companies are moving that way because it is better to do things uh client side logging because client side logging is higher Fidelity than server side logging is because you get all the interactions that a user takes not just the ones that uh cause a web request to happen so that's a big thing to remember when you're thinking about the when of of client side logging and and and fact data modeling okay so more things about facts uh fact data sets should have quality guarantees um some some of the key ones that I really like for fact data is uh no duplicates or like that's one of the key things is like can you crunch all the data and duplicate it down so that like when an analyst is looking at stuff it's all kind of flattened out uh you also have uh fields that need to be there like for example that event field like the what field and the when field the what and when field should always be not null it should always be there like if they're not there like what like you can't even you can't even analyze that data if it's null it doesn't make sense it's bogus so like you should definitely and then like really who the who field should also have some sort of not null um fact data should generally be smaller than raw logs uh raw logs a lot of times will log a lot of extra stuff that you don't actually need that they might need for like diagnosis or for like other like the software engineer needs it to like understand what's going on in the server and like that's not something that you need to know to understand what's going on in the business and so a lot of times you'll end up modeling that stuff away um yeah and fact data should parse out hard to understand columns like a lot of the times the software Engineers will pass you a a column that is like a string but it's actually a Json string that's just a blob of like nastiness and it's like really hard to even query it and so fact data should not have very many of those fact data should for the most part just be like strings and integers and decimal numbers and enumerations and it shouldn't really be um uh as much of those complex data types there could be some complex data types like for example uh when I worked at Facebook one of the complex data types we had was just an array of string which was this notification was part of these experiment groups and that was a really powerful way to um like easily kind of slice and dice the notifications conversion rates based on the experiments that they were in so it's not always the case that you shouldn't have complex data types but like you definitely shouldn't have like these massive Blobs of Json that's just like painful and that's like your analysts are going to be so sad if you just like give them the raw logs and say parse the Json yourself like that's like literally your job as a data engineer is you should be making data sets that are really fun and delightful to use and if you're not doing that then yeah you're not being a good data engineer so definitely uh make sure that those things are things that you are considering when you're doing fact data modeling now I want to talk a little bit about uh in in one of those previous slides I talked about normalization versus denormalization and so one of the questions with fact data modeling is when should we like when should we bring dimensions in like instead of just like the ID but we want to actually bring in some of the values so I want to talk about this uh Network logs pipeline I'll give you a link to this as well that you guys can read more about it so uh one of the pipelines I worked on at Netflix was this network logs Pipeline and this is the biggest pipeline I ever worked on in my entire career uh this pipeline uh was you know it's like it did about two pedabytes of data every single day two pedabytes of brand new data every single day and one of the things that we had uh or and what it was was the The Source data set was just every Network request that Netflix receives just every single one and so that's why it's so much data and what we wanted to do with this data set was we wanted to see how the microservices were talking to each other because I don't know if you know but Netflix is actually uh like very famous for like uh pioneering the microservice architecture and the microservice architecture is really great uh for uh making your development faster and to making your uh uh responsibilities got of split out and like you can really do really great things with microservices uh but for security they're actually kind of a mess because it's like okay if you have instead of having one app that you can secure if you have 3,00 apps think about like all the ways that things can get hacked if you have 3,000 apps well there's just a lot more things just to clarify with that as well like that's true for big companies like Netflix that it will genuinely make your teams go faster and and and quicker but if you are a small startup up typically it won't give you that that like speed benefit because you'll have a small team having to do more um having to separate out their code and then factoring things like Network networking and latency and things like that because they're using microservices to connect their systems so it's not always a case that it'll make it faster but in big companies that's definitely true things to worry about and so what we were trying to do was we were trying to figure out like okay what are all of the apps and how do they talk to each other and like so then we can see like okay if this app talks to this app we know if this app gets hacked we know all the other apps that it talks to and uh the only way that we could do that is by looking at all the network traffic and that was like insane but like anyways how it worked was we take this network traffic and we join it with this uh small database of IP address data that we were able to um come up with which was um like just all of the IP addresses for Netflix's microservice architecture and it worked it actually worked and the only reason that worked is because of the fact that that data set was small enough to do a broadcast join and a broadcast join in spark for the people who don't know is a way that you can do a join that's very efficient but the only way it works is if one side of the join is small and by small I mean like less than like five or six gigabytes if it's more than five or six gigabytes then spark is going to have a really hard time doing a broadcast join um but one of the things that happened was we were like oh this pipeline needs to be upgraded and um we needed it to go to IPv6 instead of ipv4 and we needed to bring in a lot more IP addresses and then we realized like this isn't going to work anymore this join is no longer going to work we will not be able to figure out like based on IP address what the app is uh so that then we can because after that join what we did was like we figured out what the app was and then we aggregated down to see like oh this app talks to this app this app talks to this app right because it's just ip1 to ip2 you just have like two IP addresses and you just do that join but like if you can't broadcast join it then we I mean we tried we we literally tried to have it run without broadcast join and do a shuffle joint and like uh the cost ballooned the cost ballooned like over 10 times and so we were like wow okay so how do we solve this problem like how do we solve this fact data modeling problem because like we can't just like give up our security questions um if because we want to upgrade the IPv6 and so what was the solution here to solving this like very difficult problem even though like we initially did solve it in like kind of a more denormalized way of solving it so what we needed to do was we didn't need to do that join that like what we needed was instead of doing that we needed all of the apps to just log their app every time that a network request happened we just needed to log which app was what which app it was receiving so then like instead of having to do this join we would that the app would just be in the network request data itself so that was like really crazy that we realized that that's what we needed to do was so what we needed people to do was adopt this thing called a sidecar proxy which what it did was every app could just adopt this sidecar proxy so every time it received a request it would log the app that it was and was coming from so that would give us the app name and we wouldn't need to do a join anymore and so we would have uh but now you'll see the fact data is now denormalized though because the IP address is the identifier and so um bringing in that app information that string information uh actually does denormalize the data because that is something that you can answer with a join but since the data is so large and that join just becomes unmanageable you need to denormalize it and log it ahead of time and so every time that the data is logged so then you don't have to do the join at all and that made this pipeline more efficient I mean it was also a massive massive massive massive pain to do because then we had to go and talk to 3,000 app owners and be like yo can you install this sidecar proxy and we had to like get everyone on board and then uh and then get all new apps on board as well and that was like so many conversations right and that's just another great example of where a lot of the times the impact that you have as a data engineer is not writing a pipeline and it's not optimizing a pipeline it's actually going upstream and solving the problem at the source in this case the problem was we aren't logging the app on our Network requests and we're trying to walk around it by doing these joins and like trying to essentially solve putting a Band-Aid on the solution because we're like wow we really need this information but like we're going to solve it through this uh kind of a crazy join way and when we realized oh wow we can change how uh the the whole company manages this data and looks at security and that was you know what we ended up doing even though like as Engineers we're like when we think about that we're like which would you rather do own a pipeline that processes 100 terabytes an hour and does a join or have 3,000 conversations with app owners uh I mean I I I think that a lot of data Engineers are going to pick the the 100 terabyte hour pipeline because it's like that's a lot of conversations and like some what what what people say no and you have to like uh talk with them and understand like why they don't want to adopt what you want them to do you have to like debate and persuasion and all this kind of stuff like that so but that's that's that's a beautiful part of being a data engineer is that is being able to help people understand the data practices that they need to be adopting in order to make the company better and more secure so this is like for me this is like one of the more like bigger impact of things that I worked on in my career that I noticed was like wow this is a a great way to solve this problem so yeah denormalization wins in this case right so that just shows you that like sometimes denormalization is actually the solution to large scale problems and not the cause of large scale problems okay so we talked a lot about logging so um how does logging fit into fact data uh logging should give you all the like pretty much all the columns that you need except for maybe some of the dimensional columns and those honestly you probably shouldn't even be in the data table to begin with you should just have the IDS and then people can join in on those IDs to get at my previous place when we created our um fact and dimension tables we tend to only make like we tend to only create Dimensions that we knew weren't going to get that big um so so that and and and log the things that we knew that probably would get big um so I think it has I think that's pretty much the the essence here of like if you know that it's going to get if you know that you're going to be working with a lot of data and and you can't predict what the DAT is going to be then you put it in the fact table whereas if you know that like for example at my previous company we had events and we were a St and we knew that the events you were only going to get like like couple hundred events like in the year that that that would be hosting so you knew that the actual table that you were going to be working with like the dimension table for that was going to be really really small and so it made sense to create to have that in a separate table um so that's just an example hi hi sester the dimensions that they need that very B important thing uh remember this is all in collaboration with online system Engineers they are going to be the ones who are going to be knowing a lot more about the event generation like when those events are actually being created in the app so they're going to have a lot more context on that like as a data engineer you probably won't have as much context on that um another big thing is is like don't log everything uh log only what you really need because a lot of times these raw logs can be very expensive and they can cost a lot of money in the cloud and so a lot of times uh companies can fall victim to this idea that like we need to log stuff like just in case um and so that's something that you want to not do that's something that is very uh an anti- pattern and against kind of the efficiency ethos of data engineering when story um I have a friend who I'm quite I'm quite good friends with and they um they work at like another company um as a like infrastructure like engineer and they were telling me how like like they had a ridiculous Cloud Bild because like the developers were just logging every everything and he was just like why is been like I'm getting all these erors in the logs what like why why are we logging all of these errors and it was like oh those errors don't really matter you know and he was like oh so I can just delete all these logs in and they were like no because um because some of the logs might be useful and he was like well I we get rid of these errors then because these areas are um are making our B go up drastically and that was the only thing that actually got them to remove like the pointless errors so all of the stories don't um don't um log things that aren't important uh SAS sasat is that the correct pronunciation of it where data engineering is kind of like we're going to give you all the answers to all of your questions the most efficient way possible so that's a big thing uh the last thing I want talk about with logging here is conformance uh so when you are logging your data there should be some sort of contract or schema or shared sort of vision for things and this can be tricky because of the fact that like for example at Airbnb they like for a long time they had a lot of their app in uh Ruby and we did all of our Dev in Scala and it's like Ruby and Scala don't uh you can't really just import their Ruby libraries into your your scholar libraries it doesn't work so what you need is a middle layer between the two so between Ruby and schola they shared some of the schema so for example like I worked in pricing and availability and I worked on uh like some of the shared schemas of like okay what describes a price at Airbnb and what describes uh you know what's available and these things were defined in what's called a thrift schema so Thrift is actually uh a specification that is language agnostic and it's a way to describe schema and to describe data and functions as well and data Engineers mostly just use the schema part I don't really use the function part as much but like it's mostly like schema and data and you can describe that stuff and it's in a in in a way that is shared so the Ruby code and the Scala code will both reference the same schema for price and so I'll know if like the Ruby code is if they add another column to their price data like I will my my schema will also have that new column and hopefully like my there'll be a test there that will break my code so that the Ruby team can't push their code until they talk to me and they say like Hey we're adding this new column that's going to change the calculations so you need to go and update your code as well so that like I can then go update my test and I can make sure my pipeline isn't going to produce junk data just because the online teams decide to change how they calculate price so that can be a beautiful way to have an integration between what the online service teams are doing like however they're calculating the things in the app and how you are calculating your things things in the pipeline because if you don't have this shared schema and this shared way of talking about things those things are destined to drift and they will slowly and slowly kind of fall apart from each other and that's where having this kind of shared knowledge and this shared talking can be a very very beautiful way to keep everyone on the same page and keep everyone kind of working towards the same kind of shared Vision let's talk about uh some of the other things that is super important uh to think about when you're dealing with high volume fact data um sometimes oh that's nice thank you and hello like the solution is to not to not work with it like to actually just filter it down and not work with all of the data uh that can be a very powerful way to solve these problems so for example one of the things I worked on at Netflix was this thing called infrastructure impact which was a metric that like showed if an AB test caused a high higher S3 bill or a higher AWS Bill than like another uh experience like test and control like the test group oh like we're going to see 2% higher AWS costs or whatever so but the way we looked at that was we did a sample of network requests and we only need needed to do like a 1% sample of the network requests and that solved our problems and we were able to build pipelines a lot faster different than the security problem because in security you can't sample because you have to know all of the possible like you have that needle in the Hast stack problem right where it's like you can't sample so sometimes sampling doesn't work right in like especially in cases like security or like where you where you have like those very low probability events that you need to capture then uh sampling is going to essentially be a show it's not work at all but for a lot of metrics and a lot of directionality things you really are going to it's probably going to give you almost the exact same uh number or the exact same metric but it just uses a 100 times less compute and 100 times Less storage so in a lot of ways it's more ideal uh sampling works because of this concept called The Law of large numbers which is as more as you have more and more examples and more and more rows of data it approaches more of a gaussian distribution and so the as you get more and more data you get a diminishing return on what that that distribution looks like and I mean the lob large number says 30 right if you have about 30 data points then like you start to approach that uh normality distribution and so that is a very powerful technique that you can use especially when you're working with metrics that are like mostly used to kind of gauge directionality of things but not like when you need to know the specific row data because if you know need to know the specific row data that's when sampling is not going to work um another great example of things that you can do with fact data especially if it's high volume is going to be bucketing where in these cases you can bucket things on like user ID or you can bucket things on like whatever the actor usually you bucket on The Who The Who IDs and um the cool thing about that is like if you need to join on that column then you can join on that column and then it like you don't have to shuffle everything you don't have to shuffle across the entire data set you just have to you just join within the buckets and then uh it can be a lot faster so so you can do bucket joins that is something that we are going to cover in week five uh for the infrastructure track um how to do bucket joins in spark uh and it helps minimize Shuffle uh and if you go to a sorted merge bucket join which is an SMB join then you can actually do joins without Shuffle at all because in those cases you have like I mean I I need to like understand like these different um terminologies because I've not had of book at Jon and Jones both both buckets are they line up and they're sorted so it's like a zipper so they like come together and then it just Zips down and you don't have to uh there's no there's no Shuffle that needs to happen at all and that was a big uh SMB joins was a very powerful technique that I use at Facebook to really make things more efficient it's not quite as powerful or as used nowadays because spark kind of uh shuffling in spark is like a lot nicer than shuffling in Hive was so but it still is a technique that I see uh in in practice and I even saw at BNB so it's not it's not gone yet though so bucketing that's the key thing to remember bucketing can be a really great way if you have a very high volume uh fact data set bucketing is going to be a really powerful way that you can handle uh that data another great question is like retention uh so so you have high volume high volume fact data uh you can't just hold on to it forever dimensional data you can kind of hold on to as long as you legally Legally are allowed to um one of the things that I noticed uh for fact data in big Tech was uh the whales which are any fact data tables that were greater than 100 terabytes they had very short retention usually it was like a week maybe two weeks and those like the whale tables which were like you know those are only like maybe 2% or 3% of all the data tables and then uh and then it was like if it was less than 10 terabytes like in big Tech it was like okay like doesn't matter it's uh the only thing that mattered was like if it got anony anonymized or not and that was because they only wanted they cared about the legal risk but from the efficiency perspective they didn't really care um obviously uh depending on the company and the budget of things these rules might change maybe it's uh at your company instead of 10 and 100 it's 1 and 10 or it's 100 gigs in one terabyte or whatever there's like usually there's an order of magnitude between like where you're like we need to make this more efficient and we need to reduce the retention on this because the cost outweighs the benefit and that's a big thing that like as a data engineer that you should be thinking about is what is the ROI for holding on to more data and because you'll get push back from data scientists all the time because they're going to be like oh uh we we need this data we need longer retention on this data just in case and that just in case rarely comes like every time I hear that just in case like I don't know it's it's almost always that's almost always a cop out and that's like not like very rarely actually something that they need to worry about um so yeah retention they definitely thinking about that when you're dealing with high volume data this is the last bit of this presentation and then we'll take a break so um I talked about how a d duplicating fact data is uh challenging uh because you can have General actual duplicates of data and um but one of the other things about it is like you have to think about the time frame like I know in notifications right where it's like I imagine it this way like Facebook sends you a notification you click on it today but then you also click on it in a year because the notification is still in your tray and then you click on it in a year do we care about that duplicate like like do we honestly genuinely care about that duplicate probably not so there's going to be kind of a frame that you have to like care about duplicates of some some time frame where those matter uh but there's also a long tail where they don't matter and so that's a big thing to think about is how like what is the distribution there of duplicates and you can even look at that like you can do some analysis on the data set itself to see like okay where is the the big chunk and like what is the way to do this like I found um one of the things that can happen with these things like so I worked on D duplicating the notification event data set at Facebook and when I got there uh that was just like one query and it was like it took like nine hours to run in Hive and it was very painful because it was Master data a lot of other data sets relied on and so we wanted to reduce the latency of that data because it was like it was available like at like way way late and then all these other pipelines could only start after nine hours because they were waiting on this data so what are some options for uh making or reducing the latency of these dding so the two big ones are going to be uh streaming and microbatch is so microbatch might be like on an hourly basis and streaming obviously is going to be on like a even lower basis than that so we're going to go a little bit more into details on like how I actually D duplicated uh this very large volume data um streaming allows you to capture the most duplicates in a very efficient manner because you can essentially capture the duplicates on whatever window you want uh and you just like say okay we saw this notification ID and we're going to hold on to it and then if we see any duplicates over the next like 15 20 30 minutes or whatever then we can capture most of them that way because uh you know a large majority of the duplicates happen in a short window after the first event and so you can capture a large majority of the duplicates with streaming in a very uh small window one of the things that was interesting about this for the requirements that I had in notifications though was that we needed to hold on to every notification for the whole day and we needed to D duplicate throughout the entire day not just in like a 30 minute or an hour long kind of window and one of the things that caused was there was just so much memory it was used so much memory and like actually streaming didn't work I tried I tried I I tried doing streaming stuff on this for like six weeks and I could not get it to work and I was like wow this is this like an unsolvable problem like how do we get past this problem in notifications and um so but streaming especially if the volume's less because you know keeping in mind that my problem I was working on was like 50 billion records a day so like if if your problem's less than that you're probably going to be in a better spot where streaming is going to probably be great for your use case especially like if your duplication window is smaller then uh streaming it can be like almost a slam dunk and it can be the one of the simplest Solutions so I'm not saying just because streaming didn't work for me just wondering like in terms of of in the scenario that I mentioned about notifications if someone clicks on something just say the click on it again in like an hour why does it matter that it's duplicated is it just a data size thing is there like a reason that they need it to be not duplicated um that's what I'm not quite understanding at the moment that streaming wouldn't work for you it could definitely work for you in doing these kind of D duplication of facts why do I hate Apple products what's wrong with them um I don't I've just I'm just not used to the ecosystem so um I've tried to work with Max before and it's just very opposite to what I am used to working in and I'm of the opinion that like it's the things that you work with are just tools and I've not seen a reason why need to use a Mac um to be more productive I don't think I need that I I predominantly working um I I predominantly work with Windows but I predominantly work in WSL um which is like um you buntu so I'm pretty comfortable with working in that environment so I don't see the point in getting up to speed than Apple unless I'm given one as part of my like like I I nearly had to use one for a customer because they required you to use their machines but then we didn't end up going with that customer so then I didn't need to to use one this was how I actually pulled this off uh uh D duplicating um the Facebook notification data so use this thing called hourly microbatch ddop and so what this did was instead of taking nine hours it was actually available 1 hour after midnight so and that was um great we were able to really solve this problem but like what does this pattern even look like like it's probably uh kind of messy so we're going to go over each piece of this real quick so the first step in hourly microbatch ddop is you get all the data for a specific hour and then you aggregate down right so in this case uh you would have your um product ID event type and this would be for a given hour like for one hour you collect all of the data for one hour first right and you do that for every hour all the way from hour zero to hour 23 you just get all of them together and you group them and that's how that's the first step and what this group bu does is it eliminates duplicates within an hour so within that one within the hour after this group buy all the duplicates are eliminated so that's step one then step two what you do is you do a full outer join between hours zero and one or hours uh two and three or hours uh four and five etc etc and then what this full outer join does is again it makes it so that it eliminates duplicates that are across ours well if find interesting is just generally the terminology because I've had to do this before but I have not heard of the microb action terminology um but I have had to do this so it's just interesting like sometimes like you might think okay I've not done that before and actually you have you've just not necessarily known the right terminology or you the way that you or the way that you know is different to how someone else knows it Jay do I have any plans making some content on YouTube and if so what about yes I have been thinking about different things so I've got my like msal off series at the moment that I've been doing but I was also thinking about doing more with aour because like recently for example I um I have been working with aure open Ai and apim like aure API management and I've had a lot of customers that have struggled with getting that working and I've had to help them get it working and so I reckon like people would probably find that really beneficial showing okay how can you get this working how can you how does managed identity work how does conditional access policies work loads of different things in aure basically that I help our customers do the the challenge is especially with like the open AI aure Services is that I just need to verify whether or not I can get it on my msdn like subscription because I I obviously can't film my work subscription stuff so um so I need to use my own my P private subscription so if there was a duplicate that happened in like an hour zero and an hour one this full outer join would get rid of that duplicate so that you could be able to have uh like a d duped two hours duplicate or a d duped like two hours event stream I mean so then what you do is these all come together and it branches like a tree so let let me kind of show you what I mean by that so this is kind of a diagram so what this does is like you have you wait for your hour and then you do that group by that's what is DD hours one through eight dot dot dot and then you do the merge which is that full outer join query and then after hours one and two merge and hours three and four merge then these two merge together to do hours one and four and then you keep merging keep merging keep merging until finally you get down to where you have hours one and 16 and hours 17 and 24 and then they merge together and then you have your final daily DD data set and yeah this like was a lot of work but it's a it's this way is a very resilient way to DD fact data because like it's still batch but like it also uh handles data in a very it will dup data in a very low latency way way so definitely I highly recommend checking this out because this works at pretty much an arbitrary scale uh yeah so so today's lab what we're going to do is we're going to be uh looking at some of our different kind of uh values here so we we are working with the NBA data set again where we we have games and game details we're going to be looking at things like teams and games and players a couple different things like that and we're going to be building out our fact data sets that will be using a lot of these data congratulations on getting to the end of the first lecture in the fact data modeling course if you are watching on the platform make sure to switch over to the next one in the links so that you can get credit for every lecture in lab here you don't want to just keep watching completely thank you so much for getting here and make sure to like comment and subscribe so today we're gonna be working with okay I shall view the next lesson then So today we're going to be working with uh mostly with this table select star from game details let's just look at this table real quick so this is our table and this is what we're going to be working with so one of the things that what we're trying to do is this table is actually terrible like there's so many things that are wrong with this table and uh we're going to go over a lot of like what's wrong with this table but um there's uh like let's let's just let's just think about this for a second so game details like the grain so when you when when you're working with fact data the grain of the table matters a lot and the grain is going to be the what is considered the the the the lowest common denominator like the unique identifier of this table and for game details in this case we're going to be working with um uh uh we're going to be working with um this right and so this is mostly one row here is a player and their points so in this case I'm pretty sure we have game ID uh Team ID player ID and then uh there's obviously I think that that's pretty much it so what we want to this is like when we identify the grain of the table which is like for every game every team every player we have uh that's kind of the unique identifier here right and what we want to do is let's just go ahead and see if there's any duplicates of this table first so what we want to do is we want to say okay um this is a very common query when you're working with logs that you're going to want to run is you identify the grain of the table then you have some sort of count and then like because what we're saying is this should be unique and then this is what we want to kind of uh clarify so this is query see it takes a little bit because it's like aggregating a lot of data so it looks like there's uh almost two of every record in here so that's one of the things that can happen what do I think can AI replace programmers or is it just a scam I don't think it will replace programmers as long as they have a um pragmatic approach to solving problems rather than being like I tend to find like um the programmers are are going to do well with AI are going to be the ones who um Can quickly adapt so you know being able to use AI to help them like get better being able to like solve new problems as they come up like AI at the moment is really really bad for solving things that haven't been done before that is just like how how do you solve something when some is basically just taught on data that is is there how you sol something that hasn't been done before which is I think why they're putting so much effort into trying to create um like like um is it um like truly artificial AI like intelligence artificial general intelligence is what they're trying to create isn't it and yeah I think I think that's not as far as close as they they're making out to be happen right and that was actually kind of intentional because I wanted to show people like sometimes like when you're logging uh you end up getting double data so that's going to be one of the very first things that we want to do is we want to create a a filter here to get rid of the duplicates so that is not too crazy right so let's go ahead and create a thing called a duped as and then uh I'm just kind of move him in here and then say in this case we can just put a star here and then uh so we have a row number here and then over and then uh Partition by and in this case our partitioning is going to be game ID team ID player ID call this as ronom and then what I want to do here is I'm going to say select star from dded so this will give us all the same columns for do I use chat gbt or Co palette more I probably would say I use chat gbt more only because I use it to help me write documentation because I hate writing documentation I find it so boring the most dull thing in the world so um that's why right as the last tap but now we're going to have this nice little ronom kind of feature as well so at the very end here you'll see ronom right and so ronom in this case is there's we have a bunch of ones here but like you'll see if we say like order by row num descending you'll see that there are duplicates in this table because we saw that with the count one kind of thing right wow wow I like how this query takes a million years like because it's like has to process all the data and like game details so you see here are all these R nums to so these are all the people that are duplicated all right so these are all like duplicate records that we want to get rid of so in that case all we want to do is we want to say where row num equals one and that is going to get rid of our duplicates so that's going to be our start query that we're going to work with right a lot of the times like what you you'll end up doing here is you're going to have like an order by here there'll be some sort of like other uh thing that you order by so that you always pick the F you always pick the first row uh but you know what's interesting about this data set is there is nothing to order by and that's actually one of the other problems with this data set that we're going to solve so um okay so we have our um duped uh kind of game data now and we have this rolling this is looking great um one of the things that we want to talk about here is like there's probably a lot of uh things in here that don't matter like for example okay so we have all this data and we want like one of the things about this fact data is that it's very denormalized right because you see how like we have this like team ID and then we have like team abbreviation team was it hard for you to learn programming when you started and did you want to give up uh yes it was really hard um um especially when I started uni because there was a lot of people in my course who had already done it like programming before and I was coming in from like a math background but I hadn't really done much programming and so like once they're like figuring out how to get the app running I'm figuring out how to use the command line um and so that was quite quite difficult um and I hadn't used Linux before and we were you know using Linux in our in our like in University so that was another thing so there's a lot of that was quite hard and then um and then I kind of felt that way for like the first few few years of my like being a software engineer it's only been like the last few years where I've felt quite like actually yeah I can do this and I I I can learn quite quickly and um I feel quite confident in my skills now City and we have like player ID player name and then like all sorts of like other kind of columns in here that like really uh um are not really necessary and it's it's interesting because we actually have uh both columns in this table that we don't need and columns that are missing CU one of the things that uh if you remember from our fact uh uh presentation is that we need that when right and there's no when column in here at all and so the when column actually is we get that from game so let's let's go ahead and get that in our we can just probably uh join that here where we can say uh join games g and so this join here will give us our this is the game details ti. game ID so this is going to give us our um an let's called this GD so because this is obnoxious okay so we we we want to keep everything from game details but then from game we have you'll see there's this game date EST so uh back to your question we can actually use this to determine like okay are these duplicates are they not duplicates so in that case we actually probably want to throw that in the order by here say order by this and then we'll just pick the first one uh like based on the game the game date and if those are also the same then like like it I think it comes back to it it goes back to like it doesn't matter which record we pick so now now if we quer this let me put the game date first and uh format things a little bit so if we run this you'll see now this code is going to be looking a little bit nicer so you see how we have uh we have the the game date we have the game ID we have like the team ID um and then we have like player ID but you see like we don't need team abbreviation and team City because remember when we're doing fact data modeling like uh if if you could can join something cheaply then we don't need to put any of that data in with the fact right and because team how many teams are there in the NBA 30 and like even in a hundred years how many teams are going to be in the NBA 100 like it's not ever going to be a it's never going to be like big data right it's it's ever remember when I was saying earlier about the events that's what that's what we kind of said like if if um we know that this event is going to be what like Ma we're going to be doing Max 100 per year um we didn't need to worry about the fact that like we about performance of of that so putting that into its own table made much more sense then why choose back end not other it film Fields I don't I didn't choose it I just kind of fell into it really um I just joined as a web developer at a company and I just started gravitating more towards it I just find that the problems are interesting it's an interesting problem space it is about solving the problem rather than making things look nice um but many of those fields are interesting I really find security quite interesting as well and I do a lot of secur is part of my world hi uh where should you start if you are completely new to this I think getting suar to the basics of SQL is probably the most important thing for you um because you really do need to understand what the commands are doing to be able to them follow along because if you don't understand what the commands are doing then um then this is going to not mean very much to you like all the you can put all the teams in the NBA for the next 250 years and they will all fit in Excel easily so the fact that we have the team abbreviation and team City in this table is an Abomination and we should not have those but games is different right and that's one of the reasons why we are bringing in that game time right because games is going to grow if we in 250 years how many NBA games have have played you know thousands hundreds of thousands there's going to be a lot of games so like not having this game time is going to be a very uh it will impact our analysis of our facts a lot more than because if we have to join that in for all the 100, I just prefer the back end I'm not a I'm not um I'm not very artistic I'm in terms of visuals I'm much more like Analytical in terms of there's a pro there's a there's a a functional problem that we have there's there's it needs solving how do we solve it I don't care about how it looks I just care about actually solving the problem and and so being a front end engineer would be like like don't get me wrong I can work on the front end and I can like if someone else is doing the visuals I can you know help you know I can write tests I can you know add and like I can make it look look the way that they've told me that it needs to look but I am not a visual like I'm not an artist so like that's why the front end isn't for me I'm much more of a okay it's functional it works it does the logic that I need it to do I'm happy actually I really really dislike most modern UI at the moment because I actually think they they're awful I think they're awful like in terms of they um they just look they look they look nice but from a functional perspective they're really really bad to use like the really really bad like bad to navigate anyway give me an Excel spreadsheet or give me like some bloody like just give me the data in this format I'm way way happier records this query is going to get really slow even just give me the DAT to in adjacent format it's way better than some of these bloody uis that you get sometimes because you saw this query right now takes seven seconds right and that's only on 10 years a data so imagine if we were doing 250 years of data like this is going to take a lot longer right it's going to grow kind of uh like even like it's not even going to grow linearly right it's going to it's going to be even slower than that let's start to think about the columns that we care about here right so obviously we care about game dat EST and game ID because game ID is something that like well let's look at that table to see if there's other things from game that we want to pull in because if there is maybe that's what we want to add uh if not like Okay so so okay I think there is one more column from game and then if we bring in one more column we probably don't need to bring in game ID because all the rest of it like doesn't matter right so in this case we have all of these kind of columns in here most of these columns are aggregate columns right um like for example assist home assist Reb like all these are kind of aggregate columns we do have um some things here that I think are important though which are going to be the the home team ID and the game team ID or the home team ID and the visitor team ID because we want to be able to see if a player plays better when he plays at home or when he plays away so we need these two but we probably aren't going to store them as columns we're just going to use them to determine other things right so in this case um we the other column that we car care about here is season uh you see how there's season here so let's let's go ahead and put season in here g. season and uh G doome team ID g. visitor team ID we're going to use these mostly uh to to compare the team ID in game details to the team IDs in to these team IDs to say like is it a home are they playing at home or are they playing away uh so there'll be kind of like a Boolean that we'll end up using here but we don't really need game ID after that because the main reason for that is every other column in here is an aggregate right that we can essentially just aggregate the game that happens on that day and we can get all this data ourselves so a lot of this is like derived so that's pretty much the only columns that we really need from the game table and so that means that we don't have to put game ID in there so in that case let's just put the other columns in here real quick so we have season home team ID visitor team ID okay and then let's go back to game details let's kind of look at all the columns in here so you saw how we had uh we do need team ID here right um so in this case we can say uh Team ID equals home ID right was say uh and we can say and we can say this as so we need team ID as well so let's let's not mix these actually real quick cuz we what we have all this already we know these are coming from game so let's make a new column here we can say team ID equals home team ID as and this is U playing at home we can say like dim is playing at home right that's the because if they if they're equal then and then it also has the the false this will also have the false so we actually don't really need visitor team ID we only need home because like there's not three teams in the NBA right so in that case to make this query more efficient we're going to get rid of visitor team ID and get rid of visitor team ID here and uh we don't need to have home team ID as a column right we just need we need it to have it be this dim is playing at home because this is a very valuable uh column that we can also have so we can have Team ID here now okay so other columns here team abbreviation team City we don't need those are we can just join on team ID and that will be a quick fast easy join okay player ID we uh we probably need player ID and uh player name it's an interesting one right because uh the number of players in the NBA grows a little bit faster than the number of teams but not that much faster right where it's like it's probably still um one that we can just use player ID or maybe uh like adding player name I think adding adding player name is nice because then we don't have like the this is a great example of where we can add a column to make the queries nicer so that people can just like know who the player is and they don't just get some integer so I think adding both of those is probably Fair player ID and player name um because just because player also will grow a little bit faster than uh team but it will grow less fast than game so we'll we'll bring in both of those um um so nickname we don't need uh okay start position I think we do need because that is the attribute of a game because of the fact that uh like for example LeBron James Sometimes he plays small forward and sometimes he plays power forward and so it depends on the game which one he's starting in so we do need start position because that's an attribute of the player in the game this is like a part of the fact and then um if we go down further here here we have uh other columns here we'll we we'll put we'll keep comment in for now and we might end up doing some other things with comment later um okay let's go through a lot of these uh ones here that are probably interesting so probably Min minutes right minutes fi goals made fi goals attempted uh we don't need fi goal percentage right that's uh a waste right because that's just um FGM divided by fga so we don't need that I think we can have the the three-pointers right uh why is this like not going down okay there we go so we have field goal we have the three points made we have the three points attempted we can get rid of that we can do the again we have free throws made free throws attempted our goal here right is we like anything that's easy to derive like all these percentages like we don't care about right so we can have all the rebounds orb DB and Reb those are all the the rebound columns and then uh obviously we'll just keep them all here we'll say assist steel block um what is this why is this like not letting me okay so um I don't hate I don't hate doing it I just I wouldn't choose it as an option um I think Hate's quite a strong word but um I quite like I I like things that I haven't done before I like doing things I haven't done before so like today for example um it was installing code ql into our Dev container and making sure that it ran the static code analysis uh for for our C like code which seems really simple and it and and you know once you figure out the commands there's not a lot of code there but um the the challenges that we had was that one um the first of all like um the it's not just enough to install code ql you have to install the binary the the um bundle the second thing was that the C queries doesn't actually run the security extended pack so you had to figure out a way to actually how do you run the extended Park and then um another thing that was happening is because we was running in the dev container and because we were running through a make file um it wasn't getting the correct permission so you had to figure out what the permissions were and that's the sort of stuff that I really like doing figuring out some that I haven't done before to then um to then show showcase to our customers how you can do it going forward so so that's really like then we have a turnover here so uh this column is dumb uh so I would probably rename this to column because of the fact that you see it's like blue you see how like we're getting this like squiggly here because to is actually um a keyword right in in SQL so what we can do is to and then I would actually just rename this as turnovers because that's what it actually is so that like we can we don't use keywords using keywords in your columns is bad like imagine calling your columns select that's like a terrible name for a column right so we have personal fouls points and we'll keep plus minus in there as well so these are all the columns that I think we should keep because they're all like uh fundamental as a you as someone from the UK these columns mean nothing to me because I have no idea what the game like what the points are nature of the fact so this is really close now but let's go ahead and just run this query and because I think there is uh there's one more thing that I want to show with what this is doing okay it's running it just take takes like eight seconds right wow that's a slow one like I did not expect this to take 15 seconds like this this is like not that much data right just saying Zach M man took far so who's winning here U we might want to put like a wear Claus in here somewhere to like have this be filtered down so here we go this is our new data set that we have right so now we have our game date we have our season and we have the team and then we have dim is playing at home right and you'll see it it does have the check right so that is like there is home team and a away team then you have our player ID start position so you'll notice that some people have a null start position which probably means they didn't play um okay so one of the things that you'll see here is um there is uh this comment is uh an interesting one where there is um see there's like dnp like there's a couple different uh things for this comment that I think are um interesting and like because this comment is a really hard Dimension to work with because it's like kind of like very high cardinality but you see the um the first little bit there is wow dude my my uh okay there we go um you'll see there's like dnp DN so there's three there's nwt dnp and DN D those are the three that uh are there and what they stand for is did not play which means they are sitting on the bench they just didn't play and then there's a DND which is did not just wanted to say thank you Sam thanks for stopping by appreciate the like button I did the same for you earlier I haven't done many UD me courses to be fair \[Music\] um I think there's a lot of good stuff though on on on on Zach doesn't Zach a lot of the basics on his channel I'm not sure but um but I I think even like Microsoft have like SQL Foundation courses that are free um I have to check but um there's a lot of resources out there for sure yes I am doing this for I'm trying to do it for every lecture for sure so I'm just um I don't know like if if people feel this too but having like a body double is really really helpful for me so like it it keeps me um it keeps me motivated it keeps me engaged I know like I know that on the night I'm just going to sit down and learn and then I can talk to people as well to kind of break it up a little bit and um to make it a bit more fun dress which means they showed up to the arena and they were there but they weren't ever gonna play because they didn't ever even like wear their uniform right they didn't wear their Jersey and then nwt means that like they weren't even in the arena they were like not even there so you see here's DND right all these different uh uh did not travel right so I honestly think that these columns here like we want to essentially look at these together to see like maybe these are other facts that that we can learn more about these players with so I would think that like this is this column is a great example of like a raw data column that we would want to parse so in this case what we can say is um um I think there's a so so there's a way to do this like so let me show you how this works this is like a very strange postris um thing what I'm going to put one more thing in here I'm just going to put uh I'm just going to put one uh I'm just going to filter this down for now so that like this query doesn't take so freaking long oh a 10 okay we'll do 10 104 so that we can uh this query should really be fast there we go there it's so now it's like instant um so we're filtering down to just one day of data so that like we don't F we don't process everything at once so one of the things you'll see here is we have this thing called position so you see uh um this is uh like equals like so what this this doing is like this is doing like a string position so we're saying like is dnp in the comment right and you'll see it is in this comment and it's at position one right uh so in this case we want to say this is greater than zero and then what we want to do here is we can say coales this with zero because we want this to uh be a Boolean right but like when it's null if there's no comment we want this to be false because we know that it's not dnp because uh so now if we run this let's call this as um as dim did not play so if we run this there we go so now you see how we have uh it has the check marks for those days that were did Dim did not play right and so this column is going to be way better to work with than that comment column right and like this is a very common thing to happen when you're working with fact data so one of the things we want to do is we want to add in a couple more here that's like dim did not play dim did not dress and then we want one more here which is dim um this is not with team and then this is nwt and so this will give us all three of those column chicken yeah that's a nice one actually um I've not ever made it myself but I do like it from um from a restaurant so that is probably a good one to try do you have any good recipes that you'd recommend and that should uh you'll see we'll have all three of them now and then they'll be which one ones are kind of checked off right here did not dress all sorts of things like that right and um uh like one of the things about this though is that they kind of kind of cascade on each other right because of the fact that you have like if they did not dress they did not play and like so these kind of all of these things kind of like Cascade on each other because like um but like uh so that's a one thing to think about right so but that is something that we can do later on that's like I think that like having these be like this and like just logging like the raw like is this data here or not and not really baking in the business rules at this point is probably the better play and then letting analysts kind of work with these columns themselves uh later on is probably the the better play so that they like they they can see exactly what was in the data so that's pretty much what is in that comment column so because we now know what's in that comment column we can probably just not even have it right that's an it's an interesting tradeoff here of like if you feel like you parsed everything or if you haven't but I'm feeling feeling pretty good about it so I think we can get rid of it so this is now looking pretty close to what we want um let's let's go through all the rest of the columns here and just see if there's something else that might be uh might be missing so the game stuff looking great team ID great playing at home then you have um player ID player name then you have their uh start position and then uh okay so I hate this column I hate this column so much like like this Min column like what what are you going to do with this column like like like this is like this is not this is not a column that I would want to use right so I think that we probably want to change minutes here to be um maybe fractional uh instead of this because this should be uh this this is a string right now right so that's a terrible problem right so let's go look at let's change this uh you can do a thing called split oh split part okay so I think if we do this is it that okay we say like as minutes right and maybe uh is it split part two as seconds I think that's what we want but then we can maybe uh turn that into a decimal there we go that's exactly right so now that is going to give us what we want um so that is essentially what we want here um I think what we can do here is kind of like I just like to use fractions like I don't think putting minutes and seconds like this is this is a great example of like okay when like what is the query pattern that our analysts are going to be looking for so in this case what I would say is we can say uh we can cast this as real and then what we can do is we can do uh plus this we can say cast as real and then this is going to be minutes so I'll I'll past you guys this query in just a second and I'll show you like what this is going to do so so that is essentially what we want here um I think what we can do here is kind of like I just like to use a fra like I don't think putting minutes and seconds like this is this is a great example of like okay when like what is the query pattern that our analysts are going to be looking for so in this case what I would say is we can say uh we can cast this as real and then what we can do is we can do uh plus this we can say cast as real and then this is going to be minutes so I don't understand how that would make it work because wouldn't this be showing in seconds I'll I'll paste you guys this career in just a second and I'll show you like what this is going to do so now well that looks like doesn't this need to be divided by what 60 oh you're right you're right there's I missed the division I was like what you're totally right thanks for the the catch there of dividing by 60 there we go that was funny then because um because he wasn't saying that to me it sound like you're saying that to me there we go now now this is looking this is looking better right so now we have like it's now we like this is now a usable column right where when people are looking in doing analytics now they can do things like field goals per minute and they can do free throws per minute or rebounds per minute and you can easily turn this into a rate that is going to be um a very powerful thing that you can do with this column a big thing to remember when you're doing fact data modeling is like are the columns that you're even giving useful right and so now I think we're pretty close here to having what we are looking for um for our table so let's go ahead and make our ddl because I think that's probably um because I think hold up okay that is a date okay good so um let's go ahead and create this ddl so we're going to say create table um we're going to call this fact game details so this is going to be our table here um okay and so do we care so like this is another great example of like where we want to think about each column name right and uh so this First Column do we care that it's eastern time probably not it's the it's a date so I think the First Column here is going to be game date she's going to be a date and then so one of the things with fact data is a lot of the times you want to uh label the columns either as measures or as Dimensions so in this case season here like so our our game date is probably not game date it should be dim game date and then you have dim season and this is an integer and then you have dim team ID this is an integer right I think or is this a long oh I think I think we want to be careful there think I think it should be good I think because this is only like one billion so I think we can say integer but it's like that one's pushing it right and then we have that dim is playing at home and this is going to be a Boolean and then let's go over again so uh I like to put the more identification columns first so we have like game uh dim team ID then we should have dim player ID which is going to be an integer and then uh dim player name is going to be a text uh so then after that is where um columns can be kind of like fli I think the start position is probably going to be the next good one though like Dam start position the text right then then we have the all the same columns that we had like dim uh did not play Boolean did not dress Boolean dim not with Team Boolean okay so great um and we we have a couple other ones here then we have um minutes so in this case is actually um a measure so in this case a lot of times people like to put m in front of it like M minutes because it's the number of minutes that was uh measured and this is going to be a real or you can say real or a decimal they both work I like real here because decimal makes you like like provide like the actual like Precision uh so field goal FGM should be M FGM because that's field goals made it's G to be an integer then M fga \[Music\] integer right let's do a couple more MFG 3M integer MFG fg3 a integer right because you want to put all of these over right into what they like so that people are aware right because if you if you have these naming conventions of like okay if you put dim that means it's like these are columns that you should filter on and group by on and M are these are columns that you should Aggregate and you should like do all sorts of math and stuff on right so we can say mftm say M FTA m o um DB and mreb right and then uh I think we're almost there then we have uh M assist M steel M Block M turnovers and I think oh we have three more so then there's um God why is it this is never ending personal files M points and M plus minus okay so great now uh we have to do one more thing here which is what is the the primary key of this table so that we can make sure that we have more guarantees on this right so we can say primary key so I think in this case we're g to have a dim game date and then uher ID di game date probably dim player ID and I I mean technically you can put team ID in there as well like but like is that really necessary as the primary key I think it is like I'd put it in there too because you might be filtering on that and that the primary key helps create indexes and that would be my one thing I would say why we'd put team ID in there so let's put all three in there we're g to say dim team ID dim player ID the reason why you don't need to put team ID is because it's like can a player be on two teams on the same day and I think the answer to that is no like unless someone can like switch teams halfway through a game or something like that and I don't think that that's actually possible and so now what we need to do is essentially do all of these over into the right kind of uh columns here right so okay so we created the ddl and then what we want to do is essentially move our query with a bunch of as right so we're gonna this as dim game date this as dim season as dim team ID and remember we want to fix the ordering here so D then we want to put player ID player name as dim player ID um okay so then we had um start position as dim start \[Music\] position okay so now we're we're close then what I have start position and then we have this is I am not vegetarian had the playing at home then we have all these and then I think everything else is in order and I don't have to freaking worry about it yeah okay so then this is as M minutes then can you do column selection mode here oh yeah oh I I I should have done this like before see oh that's so much better so satisfying to do that oh wait no we got to do it as like um gonna put it on the other side here right oh because these are as right oh man it's you actually uh you can't really it's probably a way you can do it that way but like uh whatever uh let's do it uh mfga m fg3 m is not going to take that long M fg3 A as M FTM as M FTA as m o red as MB as mreb emist so one of the one of the big things I'm trying to illustrate here is like yeah like you should be changing the name of things like if you are uh doing fact data modeling because like a lot of times the names of columns that you're given are terrible and this is a great way to fix them okay I think we got got it here so now what we can do is we can um like we can do an insert into here so let's go ahead and do that so we can say up here I'm going to get rid of this uh thing here let me turn off column section I'm going to get rid of this real quick and then I want to say insert into fact game details D play ID why is that not working that's it there silly me so now this query is going to run this quer is going to take like I don't know like 20 30 seconds oh there we go it's done there we go I don't have to we don't have to debate about why it's slow even though it took two and a half minutes it definitely should not have taken two and a half minutes but it it because it's only like a couple thousand rows of data but um okay so now we have all of our data here and we okay I missed the comment on so I need to put did we say as them player ID um okay so then we had um start position position as dim start \[Music\] position okay so now we're we're close then what I have start position and then we had the playing at home then we have all these and then I think everything else is in order and I don't have to freaking worry about okay so we got rid of columns okay so then this is as M minutes then can you do column selection mode here oh yeah oh I I I should have done this like before see oh that's so much better so sad satisfied I'm going to do is I'm just going take this player that there and just compare G dick dim season di team ID player ID player name start position is playing at home did not play did not dress did not team a minute um FGM FG 3M FTM FTB DB RB asked okay that's missing B the one that's missing okay it's completing goinging to do that oh wait no we got to do it as like um GNA put it on the other side here right oh because these are as right oh man it's you actually uh you can't really it's probably a way you can do it that way but now this query is going to run this query is going to take like I don't know like 20 30 seconds oh there we go it's done there we go I don't have to we don't have to debate about why it's slow even though it took two and a half minutes it definitely should not have taken two and a half minutes but it it because it's only like a couple thousand rows of data but um okay so now we have all of our data here and we can see all of these different columns and one of the things that is really nice about this is now we have all of like people like we follow all the right naming conventions we aren't we don't have any duplicates of like excessive things that we need right because uh one of the things that like is like sad is that we lost the um uh the teams right but if we just join teams T right on t. te ID equals gdtm ID right and then we say t. star GD dostar we do that right this is just going to be g. team ID what oh matter this is dim team ID that's why so obviously you want to model everything that way but then okay so now you see we can just bring in those columns right and we can already like we can have colums we can just bring them in and it's really cheap because that team column is very uh just not expensive so that can be a very powerful way to uh use your team uh or to bring teams into this even though like we removed them from the data set so like one of the things I wanted to show here though is like like okay so we have we have like all this data here but like let's just I I think uh one cool column to do here like I like let's find the player who like who didn't uh like so it's case when um dim uh not with team so like let's find the player uh then one and the player in the NBA who like wasn't who like bailed out on the most games as um most bailed let's call this call this a bailed numb right so we say Group by one order by two descending right now this query is like way faster here right boom and now you can see exactly who this guy missed 21 right um it depends how long you you you want to do it for you should definitely be able to catch up by when it's going to be taken off the uh off off the channel which is at the end of January um it just depends how much time you have to it so like for me I'm dedicating I guess about 10 hours per week to do this and the homework so the labs that they they take about six to 7 hours because you're you're having to stop and pause and watching and stuff like that so I'm doing 2 hours for every um one to one and a half hour really and then um and then you have to do the homework which is another couple of hours on top of that see like this is the number of people who like you can see exactly the number of times right that they did that but like maybe it's different though right because you also have like count um we can say count one as numb games because one of the other things to think about is like okay but what about that like kind of bail percentage that's probably the last thing that we want to think about here and then then I think that will be the end of this presentation but if we say like this right so if we if we cast this as a real this is we're going to call this as bail percent so we want to order by three descending instead so now this query is really um better right so so you see here's our bail percentage like which didn't that didn't sort for for what that definitely didn't sort right okay a cast like that's these numbers are oh it's because it's this is four that's why okay there we go so this is probably the better way to look at it okay so this guy BJ Taylor he he has one like he's 100% he's shooting 100% so there's some people here are like half the time 20% of the time so but you see the people who had like the the number the high numbers like like that they had like 20 Bales but they actually just had a lot more games so it was like for them it was more of a volume thing so but you can see how like this query that we just ran here is very powerful and we were able to answer really cool questions from this data set that like would have been a massive pain to answer with the old with the old uh data model so this is the whole idea behind fact data modeling is can you build data sets where you can answer questions like this really quickly right and like obviously you can do amazing things with this like you can say like the number of points right you can say as total points right you can see all sorts of like whatever kind of aggregations and stuff you want right you could also put in things like dim uh is playing at home and then you Group by two right and then if we Group by two in order by six then we can see like okay who's the person who has the highest bail percentage uh when they play at home or whatever right so and you can see it's this Elliot Williams guy because he's 100% right and so um that's the whole idea here right is can you make queries that are or tables that are easy to query fun to query and that is if you can do that that is going to be a very powerful thing for you as a data engineer congrats on getting to the end of the day one lab I'm really impressed with your Hands-On abilities here if you're taking this class for credit on the platform make sure to switch out to the next link so that you can get the credit that you deserve and good job on being handson so so that's I think where I'm at to today um just think I am that's that's all I wanted to get through to be fair but um yeah I'm happy with that I think I I think I followed that one quite well but it's only because I've done a lot with facts in my previous role because we were moving over to it um where I didn't get to the aggregation that as much um in my previous rer so this was much easier to follow than last week for some reason but it was really really interesting and I thought it was really really good um and it's really really nice to um have the things that I did before be validated that yes that's the right thing to do because of XY and z and so that was good I do stream every day I am well I'm trying I've been streaming every day but it's new thing and I don't get to do it all the time so my partner's away at the moment he's in Sweden um and so I have a bit more three times on the night whereas in the rest of the weeks I'll probably be streaming like two to three times a week just because of availability but I am done for the day I'm going to save my query so it doesn't get lost cuz I'm pretty happy with that um and yeah I am done I have done my current day one lab I'm just going to check that it has been successfully started as credit and it has been cool so yeah tomorrow I'll probably try and get through these twoo fun learning with you too and make sure when you are learning with me that you put the Zach's video on in the background so that you get the credit as well because we're doing it together but you know he still deserves the watch hours and he you know um and and you still deserve to get the the um credit as well for for doing the watching so just make sure you have it on the background as well good thank you I am going to go now that is me done for today well it's gone just gone past the midnight now thank you for those who have watched throughout the full stream it's been great speaking with you just want to give a good shout out to um LX um let me just say your name sash just want to make sure SAS um thank you for all your helpful tips on what I should try to make foodwise that's been great and yeah thank you to everyone else who has been watching I'll see you next time probably tomorrow same time again um maybe a little bit earlier if I can I'll see you then goodbye


 > [!info]
> - **Fact Data Modeling Fundamentals (2:34):** Covering what a fact is, how to model it, and how to join facts with dimensions.
> - **Normalization vs. Denormalization (15:31):** Normalized facts remove duplicates and increase data integrity, while denormalized facts can make queries faster by avoiding joins. Choose based on scale.
> - **Raw Logs vs. Fact Data (17:35):** Raw logs are often owned by those with weaker data skills, so data engineers can improve data quality by collaborating with software engineers to log data in the correct format.
> - **Importance of Logging in UTC (22:38):** Ensure all devices log timestamps in UTC to avoid time zone issues.
> - **Fact Data Set Quality Guarantees (25:09):** Fact data sets should have no duplicates, non-null 'what' and 'when' fields, and should generally be smaller than raw logs.
> - **Denormalization Wins (34:12):** Denormalization can be the solution to large-scale problems when joins become unmanageable; solve problems at the source.
> - **Logging Best Practices (34:35):** Logging should provide all necessary columns, and not log everything. Conformance with shared schemas is important.
> - **Sampling for Efficiency (41:06):** Sampling can drastically reduce compute and storage costs for metrics focused on directionality, not specific row data.
> - **Bucketing for High Volume (43:00):** Bucketing can improve join performance by minimizing shuffle.
> - **Data Retention Strategies (44:41):** Balance the ROI of holding data with the cost of storage, as data scientists may push for longer retention periods than necessary.
> - **Deduplication Strategies (46:15):** streaming or microbatch deduplication can reduce latency, but memory usage needs to be considered.
> - **Hourly Microbatch Deduplication (51:16):** A process using hourly group by operations followed by full outer joins can efficiently deduplicate data.
> - **Importance of Data Modeling (55:54, 2:00:02):** Proper fact data modeling allows for answering complex questions quickly and efficiently.
> - **Transforming Raw Data (56:35, 1:20:20):** Take raw data and convert it into an actionable, useful format. Cleaning and standardizing the data is essential for analysis.
> - **Importance of Useful Columns (1:36:51):** Ensure the columns provided are useful and in a format that analysts can readily use, such as converting string-based time to numerical formats.
> 