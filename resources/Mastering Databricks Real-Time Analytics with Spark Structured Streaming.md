---
title: "Mastering Databricks Real-Time Analytics with Spark Structured Streaming"
author:
  - "Fikrat Azizov"
published: 2025-01-31
source: "https://www.youtube.com/watch?v=hpjsWfPjJyI&list=LL&index=105"
image: "https://i.ytimg.com/vi/hpjsWfPjJyI/maxresdefault.jpg"
created: 2025-03-20
tags:
  - "youtube"
  - "Spark_Structured_Streaming"
  - "Real-Time_Analytics"
  - "Databricks"
summary: "Learn Databricks Spark Structured Streaming for real-time analytics. This tutorial covers building, managing, and monitoring streaming pipelines in Databricks."
---
# Mastering Databricks Real-Time Analytics with Spark Structured Streaming

![Mastering Databricks Real-Time Analytics with Spark Structured Streaming](https://www.youtube.com/embed/hpjsWfPjJyI&list=LL&index=105)

> [!summary]- Description
> In this end-to-end Databricks tutorial, I explain the fundamentals of Spark Structured Streaming and its usage for building low-latency, real-time analytics solutions.
> Chapters:
> 00:00:00- Learning objectives
> 00:06:12- Overview of real-time analytics technologies in Databricks
> 00:09:04- Overview of Spark Structured Streaming -use cases, key concepts, limitations
> 00:16:08- Course setup
> 00:20:27- Demo: Building your first streaming pipeline from file sources
> 00:26:50- Streaming between Delta tables
> 00:28:53- Managing stream flows- start, stop, block execution flow
> 00:30:21- Streaming triggers
> 00:33:00- Common streaming source/sink options
> 00:38:02- Demo: Memory sink
> 00:38:40- Custom sink types: foeachBatch and foreach
> 00:40:37- Streaming output modes: append, update, complete
> 00:42:26- Stateless and stateful transformations
> 00:43:30- Demo: Selection, projection, filter transformations, deduplication
> 00:44:26- Aggregations
> 00:49:17- Handling Late-Arriving Events
> 00:51:31- Arbitrary State Management
> 00:57:46- Arbitrary State Management with timeouts
> 01:00:05- Streaming Joins
> 01:01:24- Joining streaming and static  Dataframes
> 01:01:58-Stream-to-stream joins
> 01:02:48-Interval matching with streaming joins
> 01:02:48-Using watermarks to handle late arriving events
> 01:05:04-Monitoring streaming pipelines-overview
> 01:05:57-Interactive monitoring
> 01:07:42-Monitoring streams with Spark listeners
> 01:10:36-Using dashboards  and alerts for centralized monitoring
> 01:12:15- Stream monitoring with Spark UI
> 
> Please subscribe: https://www.youtube.com/channel/UC8d958MxE2t1dr27QNqoOhA
> Download demo/exercise notebooks from here: 
> Data engineering with Databricks/SparkStructuredStreaming/Mastering Spark Structured Streaming.dbc
> Download stream monitoring query:
> https://github.com/fazizov/youtube/blob/main/Data%20engineering%20with%20Databricks/SparkStructuredStreaming/StreamingMonitoringQuery.sql
> To sign up for the Databricks community edition, see this: 
> https://docs.databricks.com/en/getting-started/community-edition.html

> [!note]- Transcript (Youtube)
> Hi, everyone. Welcome to my channel. My name is Fikrat. In this end-to-end course, we're going to dive into an amazing world of real-time analytics in Databricks. I'm excited to take you there. So here are the topics that I'm going to cover today. We'll start with a brief overview of real-time analytics technologies or options in Databricks. There are a few of them. And then we'll take a close look at Spark Structured Streaming. We will learn the purpose of Spark Structured Streaming, the main use cases, advantages, key concepts and limitations. Next, we are going to have a brief practical session where I will explain how to set up the course materials, in case you would like to repeat the demo exercises in your environment. In a demo section, we'll start by building the first streaming pipeline. It's going to be simple pipelines that will read from storage and write the results in the Delta Table. I'll show how to browse the streaming content. And then we'll build a streaming pipeline between two Delta Tables. Next, we're going to learn managing streaming pipeline, so key methods like Start, Stop. We'll also take a look at some other execution flow control methods like blocking execution flow, which could be required in some production scenarios. Next, we're going to talk about trigger options. So we'll consider default processing option, as well as few other trigger options like fixed-length batches, available now, continues and so on and so forth. Next, we're going to explore some source options. So Spark Structured Streaming can read from multiple source type. And depending on source type, there could be some configuration options that can help you to change the streaming behavior. We'll also take a look at different sync or destination options, and we'll learn how to create custom destinations or syncs that can help you to overcome some of the streaming limitations. Next, we're going to learn different write modes like append, update and complete. And I'll explain the best use cases for each of these. Next, we're going to move to transformations sections. There are two high-level transformation types with Spark Streaming, stateless and stateful transformations. We'll start first with stateless transformations because they are the simplest. So transformations like selection, filters, deduplication. Then we're going to move on to more complex transformations like stateful transformations, transformations like aggregates. We'll learn different aggregates types and the best use cases to apply them. Next, we're going to learn how you can build your own arbitrary state management solution. And we'll take a look at different flavors of arbitrary state management. We'll start with simple custom state management and we'll gradually move to more complex cases of having multi-value state management. And we'll also talk about different types of timeouts. So some of them are based on processing and some of them based on event time. Watermarks are an important part of streaming pipelines. We'll talk about benefits of watermarks and I'll demonstrate how to set up watermarks in your streaming. Next, we're going to move to joins section. Joins are another important transformation types with Spark Structure Streaming. There are different types of joins and their configuration will differ depending on their types. So we'll start with a relatively simple join type, which is a stream to static joins. And then we'll move on to more complex join types, which is a stream to stream joins, which requires quite few considerations. So depending of join types, whether it's inner or outer joins, and whether you want to set up a precise key matching or approximate key matching, you need to apply different types of configurations. So we'll discuss all these options and I'll demonstrate the usage in practice. In the monitoring section, we'll discuss different types of monitoring available in Databricks. So we'll first learn using interactive monitoring, using streaming handles from your notebooks, which is a good option for your development scenarios. And then we'll move on to more advanced monitoring tools like Spark listeners, which allows you to centralize streaming monitoring. I'll demonstrate how to collect streaming progress data using Spark listeners and then we'll build dashboards and alerts based on this data. Finally, we'll explore built-in ways of monitoring streams using Spark UI. In the last section of this tutorial, we'll have quick discussion around some performance considerations. So we'll discuss few important considerations that you need to be aware of when building streaming solutions. Before we continue, a quick request to everybody who is new to the channel. Please take a moment and subscribe. It will help more people to see my video. Thank you. Let's get started. So before we go deep into Spark structured streaming topic, I think it's worth to have a very quick discussion around different real-time analytics options available in Databricks so that you have a good understanding of Spark structured streaming in this landscape. So to start with, we have Spark streaming, not to be confused by Spark structured streaming. So this is relatively old technology. It's based on data frame RDDs and the Python objects. It comes with some limitations, specifically it can only use processing timestamp in its transformations and it requires manual optimization. Because of that, it's recommended only for narrow set of tasks that cannot be achieved by other real-time analytics options, which I'm going to discuss next. And the next option is Spark structured streaming, and it comes with a lot of advantages. First of all, it's easy from programming perspective. So as you will see shortly, its APIs and programming constructs are very much like static or batch transformations. So it's based on data frames and the data sets, which allows easy automatic optimizations that are available on SQL query plan basis. So Spark takes care of these optimizations, and you don't need to do much about that. You can use both processing and event timestamps in your transformations, which is a big advantage. And then it allows you to build end-to-end applications with batch streaming and SQL APIs. The next technology is a data live tables technology, which is much younger. So it's based on declarative programming. You can use SQL or Python flavors of that, and it requires pipeline. What's great about data live tables is that it allows you easy lineage tracking, so you can easily see the dependence of your tables and transformations within the pipeline. And you can easily build query rules on top of your tables, certain constraints and the conditions that needs to be on your table. Finally, it makes CI CD of your solution much easier. So to give you one example, you don't need to create databases and the tables beforehand because data live tables will take care of them automatically. So what is Spark structure streaming? Spark structure streaming is a scalable, incremental and fault-tolerant stream processing engine that's built on Spark SQL engine. So its primary purpose is to handle continuous unbounded streaming data. How does it work? Let me explain on the example of diagram from Spark official documentation. So at the top, you can see the input source, which gets constantly changed. So let's say at one point in time, we have small data set and as time goes by, more data is being added to the source. So Spark structure streaming reads all this data on certain frequency, applies transformations, keeps certain part of this in memory and then writes into the destination. So we'll talk about available source and the destinations. And we'll also have a detailed discussion around how Spark streaming handles its internal state. But for now, this simple explanation hopefully does the job. So in Databricks, you can use Python and Scala to build your streaming pipelines. Some of the use cases. So as I mentioned, Spark streaming is primarily built to handle a low latency real-time streaming pipelines. Depending on your data speed and the size, you can get up to one millisecond latency. What's interesting is that Spark structure streaming can be a great tool, not only for real-time analytics solutions, but you can also build your batch pipelines or ETL pipelines using incremental sources. Of course, there are some limitations, but using Spark structure streaming would allow you to handle some of the data engineering challenges, like building watermarks or building incremental ingestion logic relatively easily because of built-in mechanisms for handling incremental data. I already mentioned some of the advantages. So you can use similar batch and streaming APIs, which makes learning structure streaming a very easy task. Let's talk about some of the key concepts. So standard pipelines built by Spark structure streaming guarantees your data delivered exactly once. So that protects you from some device-related duplicate events. But as we will see, there are some use cases where you have to go a little beyond the standard streaming transformations by using some custom sync types. In that case, you have to build your own logic for delivery. And in that case, you can end up with at least once delivery guarantee. A checkpoint-based fault tolerance is another important part of streaming. When building streaming pipelines, you have to specify locations where log files will be written. And Spark structure streaming uses these log files to handle certain failures. For example, if your stream has been canceled manually or has been terminated because of some compute issues, you can easily resume your stream without worrying about integrity of your data. Now, let's talk about timestamps. So when building your streaming pipelines, you have to differentiate between different types of timestamps. There are event times, which represents time when events have been generated, certain devices. And the process in time represents the time when it arrives to your system and is processed by Spark. So as we'll see shortly, it's best to use event times on many types of transformations, especially when you are using joins or aggregations. But even if you don't have event time as part of your data, you can still use processing time. And the state management is another important part. As I mentioned earlier, Spark keeps internal reflection of your data as it processes and applies certain transformations. And depending on your transformation types, you have to know differences between stateful and stateless transformations. So let's talk about available sources and the destinations for Spark streaming. Spark structure streaming can ingest the data from Kafka or Event Hubs. You can also ingest from storage files. So we'll be building our demo around storage files in today's tutorial. And there are many types of files that you can ingest from. You can also read the streaming from Delta tables. However, keep in mind that these tables need to be appended only. So if your Delta tables require some updates or deletes, you won't be able to build outgoing streaming pipelines from these tables. And then there are some other sources which are more useful from testing perspective. So I have listed some of them here, like RAID sockets. In terms of things or destinations, you can write your streaming into Kafka Event Hub. You can also write into storage files. Delta tables. There are some custom things like for each batch and for each. So these constructs allow you to overcome some of the Spark streaming's limitations. For example, you cannot write into database directly by using standard streaming methods. However, these two methods allow you to overcome these challenges. And then you can also use some test things like console memory to test some of the quick solutions in your development environment. Some of the streaming challenges, I already mentioned some of them. Many destinations that are available from batch data frame perspective are not available for streaming. You need to be aware of state management for large datasets and choose correct output modes because it can affect memory resources required for streaming processing. All right, let's talk about course setup. As you know, the best way of learning new programming language is to run it in your environment. So here are the steps that you will need to do in your environment to be able to run demo materials. First of all, you need to create Unity catalog. So Unity catalog is convenient because it allows you to store both structured and structured and log files. However, if you don't have Unity catalog, then you can also store your data in Hive metastore. So within Unity catalog, you will need to have schemas. So for my demo, we'll need to have bronze and silver schemas. I'll provide the code to create these schemas as well. And within the bronze, you need to create volumes. So you need to have two volumes, one volume for data. So in my case, I named it "landing". And if we take a look at the structure of this volume, we'll need to have a few folders here. And the codes that I'll provide will create these folders as well. And then you will need to have another volume for streaming logs. It's called "checkpoints". The Databricks recommends to use external volumes for that purpose. But don't worry, if you don't have external volumes, you can also use standard volume for that. So once you have these two volumes, copy the path. So we'll need to have this path further down where we localize our notebooks. Now, in terms of code, I'm going to share two notebooks. So "generate streaming data", notebook will generate streaming data. And place it in the folders that I have showed earlier. Once you connect it to your cluster, make sure that you run the top part of this notebook, which contains useful functions. And here you'll need to provide your data volumes path, which I mentioned earlier. Once you provide that, run "prepare data" function, which will create necessary folders for you. And then you can run "generate persist streaming data" function to generate actual data. There are only three different types of data that this function will generate. First of all, it will generate IoT measurement data. So it's in other words, sensor data, which generates temperature and humidity related data. Then it will generate weather forecast data. Finally, it will generate office data. So to run this function, you need to provide three parameters. Date for which you want to run, how many days worth of data you want to generate, and this root folder that we have configured earlier. I recommend to generate just one day worth of data at the beginning, so that your streaming would start faster. Let me show you examples of the data. So this is sensor data, where we have event time, office, and data coming from sensors, like measurement type, measurement value. And here is the weather forecast data for multiple cities, whereas its office is located. And then I have office data, which is one CSV file that links office and city information. Now let's talk about demo notebook. The demo notebook will require little localization as well. So you will need to provide your catalog's name here. And then you will need to provide data volumes folder, as well as checkpoint. Once you provided these values, the rest will be taken care about by the code. I'll be sharing both notebooks in the description of this video, so feel free to download and import into your environment, and let me know if you encounter any technical issues on course setup. All right, now that our environment is ready and we have generated the data, let's build our first streaming pipeline. So I'm going to run the first command to prepare as a library, followed by next command to prepare the database context. My default database is going to be bronze. However, I have created a silver database as well, and I will be writing certain process data into a silver database. Next, I'm going to run variable configuration cell. Now we can start building streaming pipeline. So in command 10, I am creating the schema for our data. Again, this is a JSON data, and I have created one schema for IoT data and another schema for Waze. My next command creates streaming data frame. As you can see, the streaming read API is very much like batch read APIs. So I am using read stream here. However, all other methods here are very much like batch read API. So here I am specifying a format of my source data, and then I specify the schema, followed by location of the files. Finally, I applied a couple of light transformations here to bring some metadata. So we'll talk more about transformation later in detail. So this command does not create stream yet, but it creates data frame, and we can explore its schema by using this button. My next command writes stream into table destination. And to do that, I'm using write stream API. I specify destination format, table name. A few things to notice here. I am using query name here. So this is not required method. However, it's best pressing to specify that because it allows you differentiate different streams, especially if you are running multiple streams. Another thing to notice here is that I am using option method to specify checkpointing location. Finally, the results of this write stream command is assigned to variable, which serves as a handle, which we'll be using for stream management and the monitoring purposes. So we'll run this command. As I start the stream, we can see stream progress icon at the bottom. So right now my streaming is being initialized. So let's give it a few minutes. While my write stream is being initialized, let me go back to my read command and show you how you can browse the content of your streaming data. So here I have display command. Similar to static data frames, this command allows you to view the stream content. This initiates another stream. So this is much easier way of creating quick stream if your goal is only to inspect your data. In your cellization for display command, usually completes much faster because it's not writing to anywhere, it's just displaying the data. So now we can see that streaming display has changed and we can actually expand this button and inspect certain streaming statistics. We have three helpful charts here. So input versus processing rate is very useful. So input rate indicates how quickly your source gets new roles. And the processing rate, as the name suggests, is how quickly streaming pipeline is able to process your data. The next chart here indicates batch duration. So as I mentioned, streaming pipeline is in a typical configuration, is a series of micro batches. And by default, streaming pipeline will process the next micro batch as soon as it's done with the current batch. Here we can see that duration of the batches also have dropped over the time. And finally, we have aggregation state. Right now we don't have any aggregation transformation, so this chart is not much useful. So let's take a look at the data. So this is IoT measurements data. So we have event time here. We have measurement, which is a complex type. So if I expand this, we can see few nested elements, like measurement type, measurement value. We have office to which this measurement belongs to. And we have ingestion timestamp, which is metadata. We attach to the data frame. So this column represents processing time. Finally, I have attached another metadata column, which indicates the source file name. One thing to notice here is that in difference from batch data frame, which represents the data as a snapshot of the time, streaming data gets constantly changed. So we can't see it here, but as we'll see further when we explore aggregations, this streaming data will change as we watch it. Okay, now let's move on to our right stream and take a look at its progress as well. I'm going to stop display stream command, but we'll keep my right stream command running. So this will ensure that any new data is being picked up. Now we can take a look at the IoT measurement data. All right, now that we have data coming into our bronze table, we can create another stream pipeline, which will read from this table, apply some light transformations and write it into another Delta table. So in command 15, I have similar read stream command, which reads from IoT measurements table, uses select expert command to do following transformations. So first of all, I rename column name because I don't like this dot notation. Then I extract parts of my complex field, measurement fields. So I extract measurement type and the measurement value as independent fields. Next, I also apply data type transformation, and then I start write stream API to write into silver IoT measurements table. Notice that I am using different checkpoint location here, because if I use a folder for the previous stream, I can mess up ongoing stream. So let me run this as well. Now we have two streams running in parallel. So this stream initialization completed very quickly. Obviously, reading from a large data table is much more efficient than reading from many different small files. Okay, let's take a look at our process data in silver zone. Now we can see measurement type, measurement value, and we can see correct data type for these columns. Managing the stream. So earlier, we have created streaming handle by assigning results of write stream API to variable. So like this. Now we can use this handle to manage the stream. So in my next command, in command 19, I am using stop command to stop the stream going to silver table. If I run this command and come back to my silver write stream, we can see that the stream has terminated. If you have multiple running streams, you can also use command 20 to either inspect or stop them. So let me first create commands that will just display the name. So this is my only active stream. And if I need to, I can stop all my running streams by using this command. Once you stop the stream, you can use start method of the stream handle to resume it. Now let me wrap up the stream execution control topic with another useful command called await termination. So this command allows you to block the notebook execution flow until streaming completed. That means you won't be able to run any other cells in this notebook until streaming completed in some way. Either it has completed the processing available data or it has been terminated. Let's now talk about trigger options. So far we have created basic streaming pipeline and we didn't specify any triggering or streaming frequency options. By default, if you don't specify anything, spark will process new micro batch as soon as it's done with the previous one. So let's talk about other available options. So fixed interval allows you to specify frequency of your micro batches. Using this option, you can specify micro batch frequency in hours, minutes or seconds. So here's example. Another useful option is available now. It allows you to process all the available input data and shut down the streaming. So this option is useful if your data is not continuous and you want to run your streaming on demand basis. So here's a syntax for using this option. So this also could be a great option if you want to implement your batching pipeline in a streaming fashion, as I mentioned earlier. Another available option is continuous option. This is fastest processing method. By using this, you can achieve latency as little as one millisecond. And here's usage syntax where you can specify how frequently streaming will take snapshots. You need to be aware of some of the limitations of continuous trigger. First of all, it does not allow all the transformations. So it allows select and the project transformations, just like the ones I have demonstrated earlier. To give you an example, you cannot use aggregations or joint transformations. In terms of sources and the sinks, this method is also limited. So you can ingest from Kafka and the rate sources and the right to Kafka and the memory destinations. Let me demonstrate a few trigger options. So in command 23, I am creating data frame for weather data. And when I create right stream part of this, I use fixed lens micro batch option by telling to run micro batches every 10 seconds. So let me run this to commands. Finally, in command 25, I demonstrate how to use available now option. I'm not going to run it here, but feel free to run it yourself. Changing frequency of micro batches in stream is not only way of changing its behavior. So you can adjust certain source and the sink parameters to change the way streaming behaves. Depending of your source type, some of the common source options that you can use are following. So you can use latest first option, which is useful for file based inputs. By default, if you have many different files in your streaming folder, it will start by the files which has earliest timestamp. However, you can change this that behavior by specifying latest first option, which I'm going to demonstrate shortly. Another useful parameter when it comes to ingesting from raw files is max files per trigger. Spark structure streaming is very scalable and powerful technology. By default, it can process many files in parallel. However, in some cases, you may want to throttle the streaming ingestion process by specifying maximum number of files that are allowed to ingest within each micro batch. Another useful option is a rows per batch option, which allows you to adjust number of rows, especially if you are reading from Delta table. Finally, there is a clean source option, which allows you to clean the input files from your source folder. There are three different flavors of this option. By default, Spark uses off option, meaning that it does not change anything in the source folder. However, you can use delete option, which will remove the files as soon as they are processed, or you can use archive folder. In that case, processed files will be moved to another folder. Let's take a look how it works. Okay, so in command 27, I have streaming from source files. However, I have added max files per trigger option to limit number of files ingested to five files at a time. I'm also using the latest first option. If I run this command, you will see that the first set of rows in my data frames representing more recent data. So I have two days worth of data right now for January 1st and January 2nd. And you can see that it started processing from January 2nd. In command 28, I demonstrate how to use clean source option. And here I am choosing archive option. One thing to notice that if you are specifying archive option, you also have to specify source archive dir option, indicate where your archive folder will be created. I should mention that when I run this command for data source located within Unity catalog, it did not produce desired effect, meaning that it did not create the archiving folder, although the streaming was running fine. So I think it's likely because of location of these files. And if your source files are located outside Unity catalog volumes, this archiving option may work. If any of you guys were able to run this command successfully and create archive folder, I would appreciate if you put it in your comments. Now, in terms of sync options, we have already seen how to write into Delta tables. Some other popular options are memory option. Then there are a couple of custom sync options. So for each batch method allows you to specify a function, custom function within which you can use static data frame methods to perform some operations which you normally cannot do using standard stream APIs. Examples of this could be writing into sync types that are not supported by streaming. For example, you can write into database for each batch function. Another good use case will be writing to multiple syncs simultaneously, meaning the same macro batch. For each method works similarly except it's on a raw level. Here you can also write into some destinations not supported by standard streaming methods. However, this is going to be on a raw level granularity. Let me demonstrate this. All right, let's take a look at the usage of some of these sync options that we discussed. So in command 32, I have example of streaming pipelines that writes into memory. And to do that, I have specified format method, memory option, and then I have named my query just like I did that for other streamings. Let me run this. Once my streaming has completed initialization and in a running stage, I can use this query name to browse the content of the stream. So if I run this select statement, I will get the content of the stream. Now let's talk about two custom sync types. So for each batch sync requires two components, user defined function and the stream itself. So I have for each batch method within the stream, which refers to that user defined function. And this user defined function requires two arguments, a data frame and the batch ID. So data frame is a piece of data that's being passed within each micro batch to this function. And the batch ID is just internal ID of this macro batch. So what I do within this function is that I apply pivoting transformation to this data frame and the right results to Delta table. So pivoting is one of the transformations that is not available within standard streaming transformations. And by using for each batch method, I can overcome this limitation. Another example of this customization would be writing into multiple destinations. So I can have another data frame here and apply different transformation and the right results to different table. And here is how my pivoted data looks like. Another custom sync type is for each and it works similar way to for each batch. So in command 36, I have examples of that where I have stream referring to user defined function. The biggest difference between this and for each batch is that this user defined function works on a row level. In this example, I am writing content of this row to the file system as a text file. However, you can also use for each transformation to write to some of the destinations that are not supported by streaming. Let's talk about streaming output modes. There are three output modes, append, update and complete. The names are self descriptive. So with append mode, the rows in the destinations tables are only appended. So you never update the rows. So it's great option for detailed streams like the ones we have been building so far, where you are applying only transformation like select and mapping. You can also use it for joins and event based aggregate systems. With updates transformations, your rows in destination can also be updated. So it's great for detailed and aggregate streams. Additional advantage for aggregate stream is that it will require less memory because it usually does not keep the data which is not being updated. So we'll talk about that in more details when we discuss stateful and stateless transformations. Finally, with complete mode, the internal memory is being updated every time. So obviously that mode is not appropriate for detailed streams where your number of rows is constantly growing. This is great for aggregations type transformations where your output data size is much smaller than your input. The append mode is default mode. So that's why I haven't specified in the previous streaming pipelines. But let me go back to the memory things that I demonstrated earlier. And here you can see that I am using update method. Before we go deep into transformation discussions, it's worth to have a brief discussion about two large transformation categories. They're called stateless and the stateful transformations. The stateless transformations are the ones which can be applied to each of your rows independently. So examples could be selections or mapping where each row does not depend on the previous row. A stateful transformation, on the other hand, were preserved in your state in memory so that you could apply transformation incrementally. So, for example, with aggregates, you would store this aggregate of your data until the end of your stream. Another example of stateful transformation would be stream to stream joins. Because with joins you require matching keys from different streams, you need to keep these keys in a buffer, in a memory for some time so that you could execute this matching. Let's take a look at a few examples of stateless transformations. In command 41, I have example of pipelines that applies select type of transformation. So I am using select method here to change the data type and parse some of the complex data types. And I also apply filter, which is also considered stateless. Another example of stateless transformation is deduplication transformation. So in command 42, I perform deduplication on my data frame based on three keys. It's recommended that you include your event timestamp as part of your keys because of the nature of this column. So it's natural to expect that event time would be part of primary key in any event stream. Let's talk about aggregations. Aggregation can be divided into multiple types. So global aggregations are the simplest one. They work on the entire table level and you don't need to specify any keys for that. Grouped aggregations, on the other hand, require certain keys from your data sets. So you basically group your data based on certain keys and then you apply aggregations. The time based aggregations are a variety of grouped aggregations. And here you need to include a time based column as well. So it usually makes sense to include event based columns as part of your group key sets. And to do that, you have to specify window function where your first argument will be that event timestamp. Second argument will represent the length of the window. And third argument will tell how frequently your window should be triggered. Based on comparative values of length and the frequency, you will get different types of window, which you can see on the right hand side of this slide, which I have borrowed from Spark official documentation. So the simple window type is called tumbling, where you have window length equaling its frequency. So, for example, here you have five minutes long windows triggered every five minutes. So with this kind of windows, there is no overlap. With sliding window on the other hand, you get window which is longer than its frequency. So in this example, I have 10 minutes long windows triggered every five minutes. Obviously, with this kind of window, you get overlap, meaning that some events will contribute to multiple windows. The session window is a little different than these two window types. With session window, you specify gap duration, and the window will not start until it receives the first events. So as soon as the first event starts, so in this example, let's say at 12 or 4, a window starts and it will continue on until we receive events. Once there are no events for the gap period, then windows close until the next set of events arrives. And as I mentioned earlier, with aggregates, you can use append, update, complete. So it will depend on your sync type as well as the nature of your aggregations. Let's take a look on how aggregations work. So in command 45 and 46, I am creating simple aggregates working on entire table level. So although I am using group by method, I did not specify any keys here. So if I run this command, I will get raw count for entire stream. So right now, this number is static because my stream does not receive any data. However, if you run this command while generating the data, you will see this number is growing. In command 47, I have grouped aggregations with some keys. So in this case, I am grouping IoT measurements based on office and the sensor columns and calculate the sum of measurement values. Here is how my output data will look like. In command 48, I have example of time-based aggregate. So this aggregate is similar to the previous one, except I have included window function here. And as a first argument to window function, I have specified my event time. And then I say my window's length is going to be one hour. Because I did not specify window's frequency, Spark will assume that this is a tumbling window-based aggregates. Once I calculate aggregates, my command writes the results into this table, silver table, which I can query. So this is how my data will look like. So using window function, create complex type of column like this, where you have start and end components, which of course you can extract and parse if needed. When building stateful transformations, it's very important to account for late arriving events. So late arriving events are the ones which arrive to your processing system well after they were generated. So that could happen due to different reasons. For example, if you have device which emits events, this device may go offline. So sometimes later when it comes online, it will connect to your system and send your data. Obviously the timestamps will be little before your actual system time. So this is a great diagram that describes that concept. I borrowed this from Spark official documentation. So in this example, I have five minutes long window sources. That's been triggered at 12, 12 or 5, 12 or 10. And let's say some of the events that have been generated before 12 or 5 did not arrive to my system until after 12, 10. So by default, if you don't do anything, this event needs to be included in the later windows, which will make your aggregates incorrect. So to handle this properly, the best practice is to delay the closing of each window. So in other words, you would specify some grace period or delay period. Each of your window will not be closed until that period. This is called watermarks. So in this case, if I want to handle my late arriving event properly, I would account for watermark of 10 minutes, which will ensure that my 12 or 5 window is not being closed until 12, 15. So let me demonstrate that in an aggregate example. In command 50, I have aggregation transformations, which is event-based group by transformation. What's different here is that I have specified with watermark method, which requires two arguments, the timestamp column name and the duration of that grace period. So in this case, I'm telling wait 10 seconds before you close each window. Let's talk about arbitrary state management or in other words, custom state management. So state management can be automatic or standard state management, which Spark does for any aggregations or joins by default. So basically it preserves the internal states and it takes care of internal logic of your aggregations. However, with Spark, you also have flexibility of implementing your own custom state management functions. There are many use cases that require custom approach. So for example, you could create windows based on counts of a given key rather than specifying some fixed window lengths. So in terms of actual implementation of arbitrary state management, up until the recent times, you could only do that within Scala. So there are two functions for that, map groups with state and flat map groups with state. And recently Databricks added support in Python for arbitrary state management. You can do it using apply in pandas with state function, which we'll be exploring shortly. I have provided few key state management methods here. So get method is useful to receive the previous state of your state. And then you can use update to refresh your state. And then you can use remove method to remove the state. You may want to remove your state whenever it expires. So what expiring means, sometimes when you apply some stateful transformations, for example, some aggregations, your keys are not evenly distributed. So for example, you may have some keys coming rarely to account for that. You have to implement timeout. So the way timeout works is that whenever you are processing your next macro batch, you set certain timeout based on either processing time or event time. And if your stream does not receive the data for some particular key during this timeout, your state expires. And based on your state's expiration status, you can perform certain tasks. So don't worry if it's little hard to understand. So I'll explain this on a few examples shortly. I have few examples of custom state management. Let me start with the simplest example. So here I have stream from silver IOT measurements table. In command 55, I have a simple state management user defined function called custom act. And then here at the bottom of this command, I have stream where I group by on two columns and call apply pandas with state method. And I'll refer to that function. So let's take a look at the function definitions. So function requires three standard arguments. The first argument is a top of your keys. Second argument is an iterator of data frames. And third argument represents your states, the previous state. The result of this function should be data frame iterator type. Within this function, my first command extracts the components of the key. So in this case, I have two keys, office and the sensor. So accordingly, I assign it to different variables. And then I use state arguments exist property. And if the state exists, I receive the previous state using get method. And then I have a loop logic here. And for each data frame, I check if the value of temperature measurement is above 23 and count number of these cases and assign to another variable. Finally, I summarize these two variables together. So this will give me a number of times the temperature in some particular office has vent above 23 degrees. And at the bottom, I use state update method to refresh my state. And then I generate the data frame with the new values. Another thing to notice that we have to specify output schema of the data as well as state schema. So because in this case, my state is single variable. This is how my state schema would look like. And this is my output schema. So let me run this. This is how my output would look like. So as you can see, this is very simple example where my state is single value variable. And I'm not using timeout here. Now that we know how arbitrary state management works, let's take a look at little advanced case. So in command 56, I have similar user defined function. However, instead of storing one value in state, I store multiple values. So I'm calculating a number of times my temperature measurement goes above certain value. But I also calculate a delta. Delta meaning how many times I have got abnormal temperature within this micro batch. And if we take a look at line 16, I update multiple variables here. Accordingly, when I receive old state values, I extract multiple variables from the state. And because I am storing more values in state, my state schema has additional variables. One thing to notice here is that one of the parameters I supply to apply in pandas with state function is timeout settings. And in both cases, I use no timeout value. Now let's take a look at more complex cases where I have added timeout handling logic as well. So in command 59, I have specified processing timeout as an argument for applying pandas function, which means that now I can add timeout logic to my user defined function. First of all, I added conditional to check if my state has expired. If it's not, then I set timeout with the duration of certain periods. So in this case, it's once, once out of milliseconds. So this part of the code will work whenever I receive the state for certain keys. So basically, each time I receive the key, I set the timeout duration for that particular key. Once I stop receiving data for that particular key and my timeout duration expires, this part of the code activates where I just receive the old state and the generalize the data. So timeouts based on processing time may in some cases not be ideal, especially considering that processing time can lack. For example, if your events come with some delay because of device connectivity issues or because of distance between your device and your processing server, then this logic may not work for you. So in that case, you can use event timeout option within apply in pandas function. And to use that, you have to set timeout a little differently. So you can see that I am using set timeout timestamp method here, which works similarly as set timeout duration method. Another thing to notice here is that I also added watermark here and because we have to allocate for certain grace period. Now, my data is evenly distributed in terms of keys. So it won't be easy to demonstrate internal logic of this based on my data. However, here is sample from the table where I am writing arbitrary state management aggregation data. Let's talk about joins. The syntax for streaming joins are very much like batch joins. However, there are a few important considerations that you need to be aware of when building streaming joins. First of all, let's differentiate different types of streams. So you can join streaming data frame to static data frame. So this is probably simplest form of join. One important consideration is caching. So make sure that you are caching static data frame because that can significantly improve the performance of your join. As to joining stream to stream, you have to differentiate between inner joins and outer joins. The watermarking that we have been talking about earlier applies to joins as well because joins are stateful transformations. However, for inner joins, watermarking is not required. Whereas for outer joins, you need to provide watermarking for both queries. Another thing to know when you're joining streams is besides providing precise matching, you can also provide interval matching condition. We're going to explore this shortly. Let's start stream join exploration with stream to static joins. In command 65, I create two data frames. The first one is reading from office data and creates static data frame, whereas the second one creates streaming data frame. Notice that I have cached the static data frame in this case. In command 66, I join streaming data to static data. Let's take a look at the results. Now let's take a look at stream to stream joins. In command 70, I display weather forecast streaming data. This data is generated on hourly base. However, notice that it starts at the fifth second of each hour. I have deliberately introduced some shift to demonstrate joins with approximate conditions. My IoT measurement data, on the other hand, does not have that shift. Let's see what happens if I try to use precise join matching when I try to join these two streams. In command 71, I'm joining these two streams using common columns like city and event time. As you can see, my stream does not produce any rows. Now let's take a look at modified version of this code. In command 72, I have similar join except I am using interval matching condition for event time column. You can see that in lines 6 and 7, I provide a precise matching for city column. However, for time column, I say my event time from IoT side needs to be within 15 seconds of running. 15 seconds of range of weather forecast event time. Let's run this command and take a look at the results. As you can see, now I get the results from both data frames. One thing to notice here is that I did not provide watermarking clouds from neither data frame because it's not required for inner joins. However, it's best practice that you allocate some watermarking window because data may come with delay from both sides. And if that happens, unless you provide watermarking window, you may get mismatches. Now let's take a look at watermarking example. In command 74, I have similar weather streaming data frame. However, in this case, I have allocated 20 seconds watermarking window. Similarly, I have provided watermarking window for my IoT measurement stream. Now I can join these two streams using left outer joins. Now, because in command 75, I am using precise matching. And because we have outer join in place, I've got the results. However, the results only represents my IoT measurement data. Any columns related to weather forecasts are empty. In command 76, I have modified my matching condition to interval matching, and now I get results from both sides. So let's talk about streaming monitoring options with Databricks. So one of the easiest ways of monitoring streaming progress is to use interactive queries. So earlier, we have created streaming pipeline and assigned the results to streaming handle. That streaming handle provides a number of useful properties to get useful information about streams, like streaming status, progress, and so on and so forth. A more advanced method is to use Spark listeners. Using Spark listeners, you can actually build a centralized dashboard to monitor all of your streams. Finally, you can use built-in Spark UI to monitor your streaming progress. Let's see how it works. In command 78, I have streaming pipelines that reads from one table in bronze and writes into another table in silver. And notice that I have assigned the result to a streaming handle. So if I run this command, I will be able to use streaming handle to get the status of the stream. So let's run this command now. We can see that it started processing new data. So if I run this command multiple times, I may see different status here. So sometimes you will see waiting for data to arrive status, whereas sometimes you will see the new data processing status. Another useful streaming property is last progress property. It allows you to get progress-related statistics. This JSON includes a few important fields that allow you to judge processing performance. So you can see input rows per second, process rows per second. You can also see batch duration. In addition, we can get some helpful information about the source and the destinations. Now, the recent progress property demonstrated in commands 81 is similar to last progress property, except it contains multiple microbatch informations. We can also use stream.active property, which produces a list, to get information about all of your active streams. Now, the interactive progress monitoring for streams is suitable for depth scenarios while you're building the streaming pipelines. However, this is not convenient for production scenarios where you have many different pipelines running in an automatic fashion. So to address that scenario, we can use more advanced monitoring tool, which is Spark Listener. To use Spark Listener, you have to create inherited class from streaming query listener. So in commands 83, I have example of that. With that class, you have to three methods. The own query started methods will start when any of your streaming starts. In this case, I am just printing a streaming start related message here. Then another useful method is on query progress. This method allows you to receive streaming progress and do something about that. So you can either print it somewhere, log it, or do something else. So in this case, the code I have included here extracts some of the key streaming progress related informations and the rise into Delta table called streaming monitor. Another method that you can customize for is on query idle, which fires when your query have completed processing data and is waiting for new data. And finally, on query table method allows you to build monitoring for stream termination events. Once you build this class, you can add it to your existing streams using add listener method. One thing to notice here is that Spark Listener works asynchronously, meaning that no matter what logic you build inside your monitor logic here, it will not impact your actual streaming performance. So let me run this command to add Spark Listener to my streaming pipelines. And if I go back and rerun my streaming pipeline, I can see the message that I included in my stream listener method. Let's take a look at the sample of the data collected by using Spark Listener. So here I have included query name, batch ID, number of input roles, number of input roles per second and the process roles per second. So these two parameters are the same that we have seen in the chart. And by comparing these two, you can get sense of your streaming performance. Another advantage of storing your streaming progress in Delta Table is that you can build dashboards and alerts to have proactive monitoring of your streaming events. So here I have created a dashboard where I have included input roles per second and process roles per second. Now I can come here and select the streaming query I am interested in. For example, this one, IoT measurement silver, and see how it has been processing. And I have included batch duration related chart here as well. Having streaming progress in Delta Tables gives you some other advantages as well. So for example, here I have built a query which calculates the difference between process and input roles to get backlog roles and compares this backlog between different micro batches and finds the micro batches where backlog starts increasing. I'm going to share this query as well in case you want to play around with this. And once I have that query, I can build alerts to monitor for certain conditions. For example, I can say, send me alerts whenever my backlog roles per second exceeds certain threshold value. Now let me wrap up streaming monitoring topic with another great way of monitoring, which is built in in Databricks. So you don't need to do anything about that. I'm going to go to Spark UI to get some key information about my streaming. So to do that, I will navigate to compute, open my compute properties, go to Spark UI, and hit open a new tab. Here we can navigate to structure streaming tab, and this page will contain a list of all my currently active and completed streams. So let me open one of the completed streams. Here we can get information about input rate, process rate, input roles, and so on and so forth. All right, let me wrap up today's tutorial with quick discussion around performance. What is specific about streaming performance is that you compare it with batch workloads performance. Streaming workloads usually involve much smaller amount of time because it processes data in micro batches. However, there are a few things that you need to be aware from performance perspective. So first of all, depending on your transformation types, you may choose different cluster configuration. If your workload is mainly persist of stateless transformations, transformations like select or filtering, then you may need to allocate more nodes so that you could distribute the workload. However, if your workload is heavy on stateful transformations like aggregations or joins, your streaming performance will benefit from having more powerful nodes with more memory. Another consideration is managing your ingestion rate. If you are noticing your backlog increasing over time, you may choose to change some of the source options to reduce streaming pace or to make your micro batches less frequent. All right, that's all I wanted to cover today. To recap, we have learned building basic streaming pipelines. And then we have also learned applying some advanced transformations like joins, aggregations. We have also explored different ways to change streaming behaviors, different types of triggers. Sync or source options. Finally, we have learned monitoring streaming, which is important when you push your streaming solution to the production environment. I hope this discussion was helpful. If you liked it, I would appreciate if you give my video a thumbs up and subscribe to my channel. Thank you, and I'll see you in future videos.


 > [!info]
> - Overview of real-time analytics technologies in Databricks (0:20)
> - Databricks Data Live Tables allows for easy lineage tracking and CI/CD (8:14)
> - Key Spark Structured Streaming concepts: Scalable, incremental, fault-tolerant (9:09)
> - Use similar batch and streaming APIs, which makes learning structure streaming a very easy task (11:26)
> - Differentiate between event times and process in time for timestamps (12:51)
> - Custom sink types like foeachBatch and foreach overcome streaming limitations (36:53)
> - Use interactive queries for Development scenarios (1:05:04)
> - Use dashboards and alerts to have proactive monitoring of your streaming events (1:10:36)
> - Use Spark UI for built-in ways of monitoring (1:12:15)
> - If workload heavy on stateful transformations like aggregations or joins, your streaming performance will benefit from having more powerful nodes with more memory.(1:13:27)
> - Adjust source options to reduce the streaming pace or to make your micro batches less frequent.(1:13:55)