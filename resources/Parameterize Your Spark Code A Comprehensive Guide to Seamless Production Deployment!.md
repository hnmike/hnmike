---
title: "Parameterize Your Spark Code: A Comprehensive Guide to Seamless Production Deployment!"
author:
  - "Deesa Technologies"
published: 2023-04-22
source: "https://www.youtube.com/watch?v=OT9_dQyzorE&list=LL&index=158"
image: "https://i.ytimg.com/vi/OT9_dQyzorE/maxresdefault.jpg"
created: 2025-03-20
tags:
  - "youtube"
  - "spark_code_parameterization"
  - "production_deployment"
  - "typesafe_config"
summary: "Learn how to parameterize your Spark code for seamless production deployment. Discover techniques to customize your code for different environments using TypeSafe Config."
---
# Parameterize Your Spark Code: A Comprehensive Guide to Seamless Production Deployment!

![Parameterize Your Spark Code: A Comprehensive Guide to Seamless Production Deployment!](https://www.youtube.com/embed/OT9_dQyzorE&list=LL&index=158)

> [!summary]- Description
> Parameterizing your Spark code is a crucial step in making it more flexible and reusable. It allows you to write code that can be easily customized to different environments, datasets, and requirements. In this video, we'll show you how to parameterize your Spark code and deploy it to production. We'll cover different techniques for passing parameters to your code

> [!note]- Transcript (Youtube)
> hello and welcome to another session in the spark Series so generally we have different environments we start with your Dev environment we go do all our coding here so this is our Dev environment and once we do the coding we promote it to different environments let's say uh we have a QA environment here and then we also have let's say a uat environment and then a production environment so we have four different environments here so this is your uat and this is your production so what happens is we do all the coding here first in the development we do all the coding we test the data we test our code and then once everything is set we deploy this code to QA and do the testing here and everything and once this is done what we do is we promote it to uat to the user acceptance testing here and then finally we move it into production so this is the life cycle right now when we talk about parameterizing right or let's say uh the file name so maybe in Dev we might get files in some location let's say I get here in user Cloud era uh there's a folder called as Dev and here I get the file right but in QA this might be different so I might get this file here in Cloud era and there might be another folder called as QA and then here I might get the file now and similarly for uat I might have different paths so this can be uat here and also for prod it could be similar different path right so Sim so we have different parts here right in Dev qnuity so while you parameterizing right when you give the spark submit you give this as one of the arguments that is from your last example right so one of the arguments is that we pass the file location so if you're running in Dev you have to pass this location but then when you're promoting this code in QA this will not work right your spa submit will not work right so in that scenario you need to give you need to change the spark submit to provide this argument and similarly for u8 you have to provide this argument and for plot you have to provide this argument right so your code your spark submit is not consistent between different environments I mean your program and everything is same but then the location of the file will be different so every time you promote you have to make that extra effort to make sure that you are changing the location in different environments right so this is one way how you can deploy this code and run in different environments but however there is a better way of doing things so what if I have the same spark submit I just have one spark submit so this whatever spark submit that I have so this could be my smart submit and now if it runs since Dev it should automatically pick up this folder right if it runs in QA it should automatically pick up this folder if it runs in uat it should automatically pick up this folder and if it runs in production it should pick up this folder So based on the environment where it is running it should pick up any of these folders right so this way you don't have to make any changes to your spark submit program so it will remain consistent across different environments and based on the environment that it is been triggered it will pick up the appropriate file so that is a bit that is the best way how you can achieve this right so how can we achieve this so we can achieve this using something called as a type save config so a types of config basically allows us to store configuration data into a separate files and then we use the keywords to include such configurations to those files right we will see how we can work on this now this is for using this type say we need to download some jars so okay so for doing the typeset we have to download some jazz so this is a jar that we need to download this is again I will paste this in your article session so uh just use this URL or you can just search for this com DOT type save config 1.3.4 and if you see here download session you click on the download and we want to download this chart so click on this jar right and this will download this file for you all right so uh the config file is downloaded so I'm gonna copy this file and I'm gonna place this in the external jars that I have right so I'm gonna create another folder here and this folder would be uh let's name it as config so I'm gonna place this folder or this particular jar here so we have downloaded this chart now there are two common ways of reading a configuration piece so we create a configuration file and then we put some data in it and using that we will read the appropriate data right so there are two ways how we can use a configuration file or read the data from the configuration file so one is basically uh application.config okay so under resources so you see the folders here right under this resource resources Source main resources you can create two files right you can use any one of them so we will see both of them in action one by one so you can either use application.config or you can use application.properties so these are basically files so if you right click on this and say new and you see file here give the name so you have to give application.config.com or application.properties all right so the naming should be correct so this is uh if you look at this so uh the names should match this should be application dot properties because it spark will basically look for these files to load the configuration data all right so when do we use application conf or when do you waste application to properties right okay so application.config we generally use this this one we generally use when you specify some multiple interrelated configuration right so if you open this this is the data that I have so these are basically uh relate this this is the data basically related to a particular project they are related to each other right so in this project uh for spark we have the application name uh the master that we are loading it what is the log level and then for the same project I'm using a mySQL database and these are the data for the MySQL right I have the username the password and the URL right and if you look at application.properties this is used when you have set of unrelated configurations like for example here what I have here is like these are all related to the dev environment this is related to my production environment and this is basically your input output fault right some files that I have right so you can use either of this to read the configuration data now first what we will do is we'll do a simple program to see how we can read this data and then print this data so that way we will understand how the data is read and what is it you know printing right and once we understand that we'll take a step further and then we'll write a spark program and then utilize one of these up configuration files and then we will use use it to read the files from development environment or production environment based on where it is running all right okay so the first thing is basically uh I need to the so so first thing I have this object I have this main function right now for using the configuration file I have mentioned that we need to download the jar right we have already downloaded the chart we need to add this yeah to our program so right click build path right configure build plot you have to add external Jaws right so go to the location where you have the configuration file or the configuration uh jar so this is where we have so open apply and close right now we have the configuration jar in the project okay all right so in order to use that we need to import a class we will say com Dot type save dot config dot we have to import two classes here one is config the other is config Factory right so we need to use both of them so the first thing that we will do is we will say config Factory Dot invalidate caches so this is the first thing that we do so what it means is this basically make sure that every time it will read the fresh copy of the configuration file now this is important because if let's say we make some changes to the configuration file it should basically reflect those configuration well rather than reading it from the cache memory so that is the reason why we use this config battery dot invalidate caches so every time we make some changes to the configuration file it will read those changes it will read the fresh copy rather than reading from the cache all right okay now let's first work with this one application.configuration okay so I'll do a simple printerland to specify that we are basically working with on application.com all right so now in order to read this right so uh let's take a config right so we'll say Val config variable and then I have to use config Factory Dot as if layout now I have to load the configuration file so what is my configuration file here my I want to load application dot config so this is what I want to look so Spar will basically look for this file here under resources and it will load this file right and from here if you look at this file right let me open this file okay we have the file here open right so this is basically uh has the my project details right so we need to get this my project details so we want to read the my project detail so here I would say get config and I would say my project all right so I'm reading the my project so my project is basically all this data all right now this will have all the details of this file uh application.config basically this my project tag now I want to print some data from here so I would say println now I will use this variable config Dot get string all right and I basically want to get let's say let's go back here so I want this data what is application name so application name is basically inside spark right so I have to say Spark this should be in code right I have to say spark dot inside this what do I want I want the application name so my application name is my application so I'm basically reading this data right so this is kind of a key value configuration row so I want to know what is the application name so this is under spark so I would say spark dot application name right similarly let's say I want to know the master so I can also say let me do another print Ln here here it should be Park dot Master right so this is the value that I want all right so now what this will do is this will basically go and search share and and underspark Dot application name what is the value my app so that value will be assigned similarly under master what is a value this is a value right see all right so similarly uh I can you I can get the values from here right MySQL what is the username and password so I can say MySQL dot username MySQL dot password so let me also do that so I will say this is a my sequel and uh I want to get the username and here I want to get the password all right okay so save this okay before I run this program let me explain this one more time so we have an application.config where we give the details related they are interrelated now for example this is my I am naming this as my project so under my project I have two things one is related to spar one is related to mySQL so insights par I have I have to give some configuration details like this is a key value so for what is the application in my application m equal to this my master equal to this my log level equal to this now you can give something else like you know my input file is this my output file is this or my locations all those details but here we are just limiting to this value and then we are taking a database my I have MySQL under this this is My URL this is my username and this is my password now instead of instead of basically uh now what we what is the advantage here is instead of giving the values like instead of saying hey this is the username or this is a password you can get those details here so anytime if you want to make any changes you don't have to touch your spark program rather you can just make changes to the configuration file right so this is like parameterizing better way and so you don't have to make any changes to your Spar program rather a simple change here should suffice all right okay so in order to read this data right we have to load this configuration file so this we have done here so we're loading into this variable and uh to read this data we have to say config.getstream and whatever we want to read so if you want to read this master so this is under spark so we have to space spark.masters and that will give you this result local if you want to read if you want to know what is the username you have to go MySQL dot user it will give the username value which is the root all right so that is what we are trying to achieve so I will save this first and I'll go ahead and run this program foreign so as you can see this application.config you have you know spark.application name is my applications Master is local then username is root and password is MySQL so we are able to read the data from the configuration file instead of you know passing the file names or anything within the code we are basically reading it from a configuration file all right so this is how you can read from your application.conf right there is another thing right application.property so let's also read from that so I'll take another print online statement and here this is application dot properties okay so let's open this so what do we have here so we are saying Dev dot execution mode is equal to local the input based directory for Dev the output based directly for dev then we have the production details so uh if I want to know let's say if I'm running my code in developments then I have to read this directory right this is input directory this is the output that if I'm running my code in production I have to read this directory this is the input directory and this is the output direct I don't have to explicitly mention these details in a smart submit internally I can write a code such that if it is running in Dev it should pick up these details if it is running in production it should pick up this details right so that we will see in a little while before that let's see how we can read these files right so it is important for us to know how we can read this data Okay so now similar to the application.configuration uh what we can do is we can use the same Command right same code so we can use a configuration Dot Low configurationfactory dot load and let me change this config one now instead of application.config we have to read application dot properties all right and again you can use the same print Ln and we say config one right config one dot now get string obviously same code nothing different so what is this we want to get let's say I want to know what is the input location so I would use this so I'm gonna you go ahead and use this so this is for my input based directory in depth now similarly if I want to know what is my output based directory so I can simply uh go here and I say hey this is my output based directory so I'm just gonna copy this I can do the same for my production I can read the input based directory I can read the output based dialogue right so this is my production input directory and similarly I can also do for production output directory so this is my production output director right okay so what this means is when you say get string of Dev dot input or base directory it will search for this based in Dev dot input based directory and the value which is equal to this so this is the value so this is what it will retrieve so it will retrieve here and we are printing similarly we will do for these things all right we also have something called as input and something called as output so let us also print those so now I'm gonna copy this and say print config right get string and here the key is basically input and let me also do for output so I'm gonna copy this here and I'm gonna copy here and I'm gonna change this to Output right I will save this so it's very similar to uh what how we read application.config we are reading application.properties and we are getting the string we are passing the key here and this will retrieve the value for us all right so I'm going to save this and let me first run this program so uh seems to me some there is some error so it says config exception okay so here it should not be dot get config so uh if you look at this we copied exactly what we had from the up application.com so uh here we have used the same my project but if you look at application.com properties there is nothing called as my project it is basically there and this application.com right so uh that's a copy issue so we should remove this so we don't need to load this or we know to get anything because uh it is basically not under anything right so we don't have any parent tag or anything like this this is these are basically uh something which is interrelated right so we will remove we have removed this and uh I am saving this again and let me run it now this time uh hopefully there there should not be any issue so we should have the configuration file read from application.com it should display all the data similarly it should display all the data from application.property all right so the data is now available now if you see here application dot properties so uh this is the dev bucket this is my Dev output folder this is my production input folder this is my production output folder and then we are printing the input and output files all right so this is how you can use the application.properties or application.config to basically get the data or basically get the uh you know configuration uh data from a particular file from the files right so this is good now this is We have basically uh try to understand how it is reading the data from your configuration file now the next step is to basically use one of this configuration file and develop a spark application now we can either use either of this to files application.com for application.properties So based on the requirement so let's say our requirement is quite simple so in depth we have a separate folder for Source we have a separate folder for Target and in production it's different so when I run it in tab it should pick up the dev folder and when I run this in production it should pick up the production folder we will use a very simple program we will read a file we will write to that file that's it we're not going to make any transformation changes and again uh you can write your own Transformations uh whatever you want it is not going to change the process right we are more concerned about how we can read the configuration data so we will keep it that simple and again we have already seen lot of Transformations and will still be seeing lot of transformation but to be precise to to the point uh we just wanted to understand how we can leverage this application.properties to read the configuration data in different environments all right so for this purpose what I will do is I will use C application.properties so let me close this all right now in this what I'll do is uh this we have only a few values here so we only have the input folder output folder right okay so I'm gonna make some changes here so let's say my input folder is under a cloud era so this may let's say this would be data sets this is Dev input all right and maybe under Cloud era output is under Cloud era datasets Dev Yep this is output okay let me also remove this and similarly for protection what I have is cloud radar data sets so instead of so instead of uh Dev I mark this as production right and similarly uh let me also copy this here and it should be output right so this is in my so these files are basically in my hdfs right so what we need to do is we need to make sure that we have this folder in our look in our hdfs so I'm going to open my cloud era here so I'm going to basically do a hdf I'm going to create this folders here so I'm gonna say mkdir so let's create this folder so I know this user data sets should be already there but let me quickly check this if I have this data set folder I because if you don't have this data set folders we'll have to create those folders as well all right okay so uh it looks like I actually have this data you know folders already created so I have the data set step I have data sets Broad I already have them created so what I need to do is I need to create uh this directories inside it okay so I'm gonna say or maybe let me just check uh if I have anything in this okay so yeah we have input we have output similarly uh if I do this we have we should have this folder so I had already created this folders so and the size is you know there's nothing in this folder so this is good so uh just a refresher if you want to create a folder so you have say hdfs dfsm kdir and then you have to give this so for example if you want to create this folder uh you have to do this because it is already there for me so uh there's no need to create something but I'm just showing you that you know this is the way how you can create these photos right okay so we have this folders We have uh you know the directories created so now what we need to do is we need to copy some data all right so we need because your input file input data so input directory should have your input files and then we will load this data into the output Direct okay so uh for this uh let me go to this home Cloud era and under data sets we should have some files all right so let's copy this customers and uh also let's copy orders right okay so let's copy the customers data to the dev environment and orders data to the production environment okay so when I say production environment basically uh this is your Dev and this is your production just assume that you know because we are running on the same cluster so we are using a single cluster everything we are doing in the single node but assume that you know this is a multi-node cluster and this is basically your uh development development folder and this is your production for let's just assume for the sake right so I'm going to copy this uh let's say categories uh categories let's use categories so I'm going to copy this category to the dev folder and maybe uh let's copy this orders to the production environment now why I am copying two different files is so when we run the spark program we are going to develop one spark program so when We Run The Spar program and it's we we should identify which data is getting generated right whether it's categories or orders so if it is reading categories data then obviously we can justify saying that hey it is reading the data from the dev bucket or your Dev folder if it is reading the orders data then we can identify we can say you know what this is reading the data from production correct that is the reason why we are using different folders or you know we are using different file names instead of this same file name all right okay so all right so what we will do is uh we are going to copy this uh data to our protection and also your uh development or a Dev folder so this is hdfs right so we need to copy the data from your local so I will say copy from local right and this is basically under this folder under this I am giving the full path again so this is a better way to write your code right so categories I want to copy this category and I want to copy this category to this folder right all right so I'm gonna copy this here enter the data will be copied there and uh right so okay I already have the file here but that is fine so this is the this is how you can copy this data so let me also do a copy and uh here I'm going to copy the orders right we want to copy orders and let's copy this to our production right so I want to copy this here okay so these are the commands that we use to copy this data from your local to your https again uh the file is exists so let me do a hdfs and see what what files we have here so this is the production folder so I should have orders.csv this is good and let me also check in my depth so this so this should be hyphen LS okay so this should have the categories.csv file yes we have the files all right so we have the files here we have the files in there we have the files in your production right and uh we also have the application dot uh properties right we have them so now we need to create a spark program and this par program basically need to read the data from an input folder and write the data to an output folder so it is that simple so if it is running in Dev it has to read the data from the input Dev folder which is basically your categories if it is uh running the program in your production it has to read the data from your production folder which is basically your odds.cse so we need to write this park program further all right so for this uh we need we need to import the types of config right so this is something that we need to import we have the spark session everything set up right so yeah now uh first thing that we want to do is we want to read the we want to say well config Factory we want to say config Factory Dot invalidate caches so this is basically if there are any changes to your configuration file it will read the fresh copy rather than reading your data from the cache right and I'm gonna create a config here I'm going to say config Factory and Dot load so we need to load this what what this we want we want to load application dot properties right so we will use application dot properties make sure that you know your spellings are all correct otherwise you know it will simply say that you know this is not found right so okay all right so now we have to read this uh data right so let's say if I want to read the uh data from my uh let's say development right so I'm gonna say read CSV DF and I'm gonna say spark dot read dot format and my format is a CSV and option it has a header right so let me say a header is true so my header is true here and uh yeah and then uh I need to load this data right I need to load the I need to load the path or the file right so here I have to say now instead of passing the path as it is what I will say is you can read this data from here config.getstring right config.getstream and from here if I am if I want to read this data from Dev so I have to give the dev folder so I will say Dev doc input dot base directed right so I'm gonna simply use this all right now this is fine now what I will do is I will write this to a CSV so I'm gonna say uh csvtf Dot write and of course uh you have to give the format and a format let's keep it as CSV only and uh and again the options and everything so our option is that header right and uh Let's uh say if the file exists we want to override right so we want to give the mode okay now what I want to do is I want to say uh save okay and here again uh I'm gonna use the same so I'm gonna read from this and here what should be the directory it should be Dev dot output dot base diet right so this is great okay now similarly uh this is for Dev all right this also this is for depth now similarly I have to do for production so this is my prod okay and I can copy the same thing here right no changes uh I am going to copy the same thing there are errors but we will fix it in a while so let's first basically make the changes so we want to read the edit file or from the production so this is my production input and this is my production output Okay cool so now uh I have to re so now when I'm running the program I have to read the data from my Dev so I have to either read this or I have to read my data from my protection so I have to either read this so either one of them I have to read so how do I tell my program that hey read just this one right so for that I can pass one argument in my spark submit so I can say okay my spark submit if I pass the argument dev then it should read from here it means that I'm running in depth so if I pass an argument say prod then I am basically specifying that hey this is running in production so it will read the data from here so for that I'll write a simple if condition here right so uh in my if condition I'll say this is my argument so ARG of 0. so I am only passing one argument here right so if it is this one then just execute this at whatever is there in depth so just do this okay so okay this should be here right so this should be here okay so if my argument is uh dev then you read the dev folder and you write to the dev folder similarly if my argument is prod then you read the data from my product folder and you write the data to my product folder right so simple okay so no other changes so what is the advantage here is so if you have to write if you have to use without a configuration file right what you have to do is you have to pass this uh you know this folder and this folder so you have to give two arguments and developments but if you take it to QA you have to give two different arguments right so there is inconsistency but here what we are doing is we are not giving this uh pass rather we are just passing a single argument I'm saying hey this is Dev so your when you look at the spark submit you say oh this is running in depth right and then it should take this one and if you pass production then you say hey this is running in prod so you have to take this one right so that is the reason so we will save it will not be able to run this year and so we will need to first you know uh save this build this jar and then we have to take this jar and we have to deploy right so uh let's first do that so right click on your project and then run as Maven build give your goal uh should be you can give any goal so it is this is my goal this is installed and uh it should build the jar soon yeah so the jar is now built and Target I'll go properties I'll open the folder and it will take me to the Target directory let me just delete this again this is 113 KB so our program should be there but because we are learning so a good practice is to extract this and see if we have our folder so we have see you can see here right application.config application.properties both of them are there so this is a conf file uh this is a properties file so this is great so again if you want to check our program so our program name is let me cancel this our program name is config file two so search for this you have the config file too so we have all our data available right so let me go back here and this is the data that we have so I need to copy this data I need to copy this jar to my Edge node right so open your winscp all right so uh where do we have this so we have here so go to this location and of course uh you will have this in external jars so just drag and drop right override the existing file so we have the jar we have we copied the jar and we deployed this into our Edge node all right now remember uh we also have you know we also have an additional jar right so the jar which we use for it type config right the jar that we downloaded for type config so that is an external jar so we need to send that jar as well right so where do I have that jar so uh it should be under C external jars and I think okay uh here so this is the con picture right so here uh on the external jars right I can create a new folder directory I can say config and I can copy this directory or this jar here right so this way we are basically copied both the jars we copied the Bell jar we copied the configure all right so let me go back and verify this so uh I am here see external external jars so if I do an lsltr so this is my jar and then of course I have a config directory created and if I do an LS LTR I should see this config jump right so we have the jars everything ready now it is time for us to run that Sparks up so first let's run the spark submit in depth so I will say spark submit and I'm gonna say master okay before that I just wanted to verify we have any data in the and why in the uh Target right so let me go if we have any data so we will delete it so that way uh we can verify if the data is first loaded into which directory right so uh this is the uh output directory and this is for Dev so I'm gonna do nstr and let me also copy this directory while that loads so we have this data here right and I will also check if we have the data here out in okay this is again I copied Dev all right it looks like I forgot to make changed to this so this should be prod dot output so uh let me first do an lsltr uh to see if we have the data we have the data so I made changes here so let me run the jar again so build this jar again configure build path uh uh sorry I have to build this jar right so yeah so come here uh run as uh build this jar I let's run this okay so let's wait for the jar to run uh to be built okay so uh yeah so we have the build now again I need to make sure that I go back uh let's uh so this is my uh Target directory so I'm gonna use this target directory here and uh yeah external jars uh so let me copy it one more time okay now uh we have everything so let me go back uh let me also uh delete this data here right so I'm gonna say htfsdfs hyphen RM so let me also delete uh data from the uh production so this is production and let me delete the data all right so we have the data deleted so what I will do is uh I am going to check it one more time so okay this is my output location per tab and let's see if we have any data we should not have any data yes we don't have any data we are all clear so similarly uh I will also check for my production so see if we have any data here so we should not have any data yeah we don't have any data so we are all clear so let's run our spark Summit so this is our spark submit I have to give the master uh this is my local environment so I'm gonna give local and uh yeah we have to give the configuration file right so that is the uh jar that we have downloaded for type config so I'm gonna say spark driver extra class path and I have to give the location so what is that location that we have uh let's just go back here and this should be external jars uh let's see this is config okay so this is uh where uh we have this jar so we have to copy this bar so this is under this and whatever you hack so you can give the full file name but I'm just giving star because you know there's only one file that's the one we want so there no confusion here right so the class right so class would be again uh your package name and uh of course uh your object name right so what is my object so this is a program that we want to run right so this is my object okay and of course uh you have to give the path of your jar right so my jar is basically uh let's say this is my X jar and uh what is the name of my chart so this is uh externals right so this is my jar name and you have to give you have to give another argument one arguments we are running in Dev so let's give them let's go ahead and execute right so the program is now getting executed and once this program is executed uh we should have we should see that the data has been read from Dev and then it is loaded into the dev environment that have folded so we should not have any data in your production right so that is what we want to verify okay so we have uh the par Spar program uh it is now executed successfully so I'm gonna go back and uh let's do something right so uh all right so uh let's check so let me use this data all right so this is our output directory right so stfs DFS hyphen LS and uh this should have some data it should have your category status okay so we have this data here so we can do an HTML DFS hyphen cat and you should be able to see this data so this data should belong to your categories right so this is what we loaded right okay so at the same time uh our production directory was empty right so let me check if we have any data here I should not have any data and because this actually picked up the tip so we don't have any data so this is cool all right so now what I'm going to do is I'm going to delete the data again so from here I'm going to delete the data so I'm going to say htfs tff hyphen RM so I'm going to delete everything from here all right and then I'm going to run the spark submit again so everything is deleted if you want to verify again you can do an endless and then you will see that there is no data here right all right so this is good now I'm gonna go back and I'm gonna run the spark submit here so everything is same all the only changes here instead of Dev I will say prod and then I will execute now what will happen here is it will pick up the prod buckets or the prod folders from your configuration file and it will read the data from your production folder input folder and it will write the data to your production output folder right normally what happens is you can put this as a scheduler in your controller or anything and you can basically pass these arguments from the controller but we are doing everything manually here so we are passing this argument here all right anyways that is the process I that's a gist of it right so okay so we have this executed so we have this executed now I'm gonna go and check the uh Dev folder first uh where's my okay so let me go ahead I'm gonna check my Dev folder first so because we have done production uh we should not have any data here right there is no data now I'm gonna go and check my production folder now this is where we should find the data so hdfsdfs hyphen LS uh give the path and you see we have the data we have the file so you can check the file contains uh using hdfsdfs.cat and it will give here so this is basically about the order you can see the complete pending ordered status and all right right so this is how you run your programs in different environments now the advantage here is if your production bucket has changed or your production or your development folder has changed uh the only place you have to make the changes uh you just go to this place application.properties and make this change you you don't have to touch your spark program and also you round up to specify the path you know in your spark submit it's a it looks a lot lot cleaner right so uh this is let me open this spark submit program again so if you use this spark submit program as you can see right we're not passing anything here except for that environment variable right whether it's rev warp production we're not passing the input file name we're not passing the output file name we're not passing anything else so uh this way we can basically parameterize our files we can parameterize how the how it can read from different environments so I hope this session was helpful and clear and I will see you guys in my next class


 > [!info]
> - **Parameterize Spark code for flexibility and reusability:** Customize code for different environments, datasets, and requirements. (0:04)
> - **Use TypeSafe Config to store configuration data:**  Store configuration data in separate files and use keywords to include configurations. (3:51)
> - **Download necessary JAR files for TypeSafe Config:** Download the JAR file from the provided URL or by searching for "com.typesafe config 1.3.4". (4:19)
> - **Add the downloaded JAR to the project's build path:**  Configure the build path in your IDE to include the downloaded JAR file. (9:09)
> - **Invalidate cache for fresh configuration:** Use `configFactory.invalidateCaches()` to ensure fresh copies of configuration files are read. (9:39)
> - **Use application.conf for interrelated configurations:** Store multiple, interrelated configurations in `application.conf`. (6:38)
> - **Use application.properties for unrelated configurations:** Use `application.properties` for unrelated configurations.  (7:24)
> - **Load the configuration file:** Use `configFactory.load()` to load the configuration file into a variable. (10:44)
> - **Retrieve data from the configuration:** Use `config.getString()` to retrieve values by specifying the key. (12:02)
> - **Pass environment as an argument in spark-submit:** Pass the environment (e.g., 'dev' or 'prod') as an argument to your spark-submit command to dynamically load configurations. (36:28)
> - **Store input/output paths in config file:** Define input and output paths for different environments in the configuration file. (24:13)
> - **Use conditional logic to load environment-specific configurations:** Use `if` statements to load the appropriate configurations based on the environment argument passed to `spark-submit`. (36:47)
> - **Maintain a consistent spark-submit program:** By using external configuration files, the spark-submit program remains consistent across different environments. (3:00)