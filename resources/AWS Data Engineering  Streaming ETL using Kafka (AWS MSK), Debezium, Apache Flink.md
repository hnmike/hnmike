---
title: "AWS Data Engineering | Streaming ETL using Kafka (AWS MSK), Debezium, Apache Flink"
author:
  - "Parth Soni - Data Engineering Enthusiast"
published: 2024-03-29
source: "https://www.youtube.com/watch?v=5CP_zuvwkGw&list=LL&index=103"
image: "https://i.ytimg.com/vi/5CP_zuvwkGw/maxresdefault.jpg"
created: 2025-03-20
tags:
  - "youtube"
  - "AWS_Data_Engineering"
  - "Streaming_ETL"
  - "Apache_Flink"
summary: "Learn how to build a streaming ETL pipeline using Kafka, Debezium, and Flink on AWS! This workshop covers CDC implementation and real-time data processing."
---
# AWS Data Engineering | Streaming ETL using Kafka (AWS MSK), Debezium, Apache Flink

![AWS Data Engineering | Streaming ETL using Kafka (AWS MSK), Debezium, Apache Flink](https://www.youtube.com/embed/5CP_zuvwkGw&list=LL&index=103)

> [!summary]- Description
> ðŸŽ“ Conducted tech talk and workshop on AWS Cloud Data services which boost streaming ETL methodologies on March 11th, 2023.
> 
> ðŸš€ Dive into the world of real-time data streaming with this captivating video! Witness the seamless integration of below technologies:
> 
> ðŸ”° SQL Server
> ðŸ”° Debezium
> ðŸ”° Kafka
> ðŸ”° Flink
> ðŸ”° AWS MSK
> 
> Explore the power and efficiency of this cutting-edge technology stack as we transform and analyze data on the fly. 
> 
> Subscribe for more insightful content and more videos of this series. Stay ahead in the data-driven revolution! 
> 
> ðŸ’» GitHub : https://github.com/parthsoni-planet/aws-streaming-etl-dbz-kafka-flink-stack
> 
> \#DataStreaming \#ETL \#SQLServer \#Debezium \#Kafka \#Flink \#AWS \#MSK \#apacheflink \#aws \#awscommunity \#awscommunitybuilders

> [!note]- Transcript (Youtube)
> okay you guys can see my screen right yeah okay so the agenda of this Workshop is mainly to let you guys understand like what is going to be in the dip that is data interchange plane and how things are going to get worked uh we have been went through the recent architectures in the boot camp so this will now help it will be easy for you guys to understand how this architecture will be helping us uh on achieving the end goal of our jobs so main thing is like uh so just a bit of you know uh what tax current currently we just developed as a part of Po was a CDC with using dbm uh getting the data to the Kafka clusters from Kafka and then putting some fling job jobs over the topics to process a data and once the ETL will be done those data can be put back to the another topic which then will be consumed by other teams like uh uh like explain in that like wot Camp as well so what our part is we have some SQL servers uh as we have currently right now so if there's going to be the CDC there change data capture right uh then we put some connectors that is a debm which will help us to take care of the change data capture right so dbm is good uh I'll explain more a bit in it uh after that we dbm connector what it will do it will play a role of connector from SQL to SQL Server CDC changes to the Kafka so what what will happen uh all the changes that is captured will be pushed to the Kafka topics right from the Kafka topics uh the Flink will be consuming the data from the Kafka topics and after that fling jobs uh will be doing some ETL uh things or something like what other the transformation is needed uh what we need to consume what we need to process to the CDM topics CDM topics will be our destinations so CDM stands for canonical data models uh right and then the other departments will be consuming from those CM topics so our our task will be having to uh consume like taking care of the CDC events right uh yeah so the main go of Workshop like this is like totally a dockerized uh images that we are going to give you and I'll explain you in your systems like how the things will going to go up and running the cavka the flank the SQL server and how you can write the fling jobs the ETL how you want how you can uh write your own connectors divisum connectors so for example currently we are using the Microsoft SQL server but in case if you have some different DB like post gra or something how you can uh connect the Kafka to the post Grace by creating your own connectors right so Workshop will be of two parts like first we are we're going to configure the CDC from source to Kafka so here our source is our this one Microsoft SQL server and we'll connect it to the Kafka clusters the second part is processing the Flink apis like uh from Kafka to destination I mean from Kafka to uh destination is another Kafka topic the CDM topic but in between that we are going to have some ETL jobs so in in the Legacy terms the ETL jobs but here we going to address the Flink jobs because Flink will do the trick for us okay uh yeah so this is the agenda I'll just brief you some like what is CDC and what uh tools to use for CDC strategy like how the division architecture work then how to create a connector then we'll work on Division connectors after then the Flink apis how we'll work it work on it and the Flink Workshop right so the thing is we'll try to cover as much as possible but if not as I mentioned in the starting of the workshop we'll connect tomorrow again uh but yeah uh I'm not going to go deep dive in the architectures and all just a brief summary to you guys so that you guys can understand what is the function of the text that we are using right now and if you guys feel that it's going up of your head feel free to stop me uh I'll be glad to explain it better manner okay so first thing first okay so what is CDC and why we why we are working on this text so CDC like change data capture uh as the name suggest if we have some changes in some data it will it is a strategy to pull the data from different data sources like taking the all the changes and store it in multiple destinations right so uh dpcm is going to be the connectors that we're going to use it is strategy like uh that also captures a ro level changes from the database the transaction logs the streams the various Destin such as aakaa right so that is going to be the thing uh right yeah uh even if there is going to be the schema changes on the database that is going to get captured as well so if you alter some table definition you put some Dil commands over the SQL right so those ches will be captured as well uh in here right okay so uh this says the overall you know high level architecture I mean uh flow diagram which will help you to understand it right so just imagine we have some uh tables okay and if we do some crude operations right so that is going to represent the CDC events because data is getting changed right so relational DB document DB column DB whatever DB we use uh this will detect everything the change data capture right so this is the overall stuff that is going to happen and uh when it comes like strategy yeah the deism part right so how deism is going to help us like uh it is a framework which help us to handle the CDC strategies right so it Al as I mentioned in the previous uh we have three type of DB relational DB document DB and column DB so it will support each and every DB so that's a good point of it and that's why you're having it so it connects from source to destination I mean uh the sync to D source to sync uh if we have the radish skinesis or something like that it will help us right so in oure it's a Kafka okay so yeah so how it's going to be happening in the planet okay so Kafka is going to be Central repository that will hold the data uh for for mainly the CDC data okay so CDC for uh the DB we'll be using the some bin log files on operation occurred in the database okay so let me explain you here like uh in SQL uh I'll just explain you for SQL Server guys okay uh you'll get to know this things in Better Way once you create some you know test DBS and all and how CDC changes are going to get captured so just for an example we have the table called test DB or some products okay so to track the changes that is happening the product a separate table with the separate schema will be created by the by the deum right that will be with the suffix called underscore CT right so that the these tables will get the copy of the all the schemas and what the changes are going to get occurred on the particular CDC that has been activated right so just imagine we if we have the product table then if we enable the CDC on that product table uh then there's a okay in my SQL there's a inbuilt command to enable the CDC uh yeah then go ahead no it's not pretty sure it's not thebiz that's creating the CDC tables it's actually the SQL when you enable and it doesn't capture all changes to schema of the database only data changes if you add a new column you need to disable and reenable CDC got it a note on that yeah so uh for that we have one strategy as well uh to capture that and I'll show you how we'll achieve that in here as well uh yeah that is a very good point then like yeah we need to First disable it and then enable it to but here in dbm we can achieve it and how I'll explain it in a later slides okay so yeah uh so yeah I was like if we have the product table we'll be having another table like product product under CT which will store the changes of all the records that and the schema changes and is going to store right so for example if we have the dbsm connector here and these are DB and is going to be Kafka so we are putting the cap like getting the data from the DBS and putting on the aper Kafka right and this is going to be our syns just for an examp data warehouse but here we are going to put on Flink right that is uh that I'll cover in next slides right so first uh how to create a connector okay so to create a connector like uh we need to get a custom with the dbm apis as well that we are going to call in our demo Workshop as well right so first we need to activate the CDC on Source right so it is like in SQL it exist different procedures to execute the C CDC right to actively Global the feature right and more generally at the table so let me show you that uh okay so if you see uh we have the procedure called Spore CDC enable DB right so this to procedure will enable the CDC for particular uh I mean test DB right uh so after that if we create the table called Product right and we need to execute like this uh CDC uh enable table Source schema dbo Source name uh product Ro name null and yeah the support net changes and anything so this is a like inbuilt uh stor procedure which enables the CDC per particular table okay we'll do it in the demo but this is just for uh yeah go ahead now this is a demo side but um dbas might have issues if we're creating CDC on production the impact potential impact to the database we've had CDC before and had to turn it off and remove it from a database because it was having an impact but just that was years ago yeah um but just as a point to not really raise anything I know it's a demo and how to do CDC but down the line it might not be allowed to be our responsibility to add CDC to tables exactly so it is not going to be our responsibility uh we need to might ra some uh I mean uh I mean uh G request for that and that need to be discussed like how we are going to handle it but uh yeah that is not going to be our part to handle all these things but yeah if we want to get uh CDC enabled for particular table we need to reach out to the infra team and do some discussion like of that right uh even like but if we have the post gra we'll be doing on postgress and I'm not sure in this like uh SPV models uh what database going to we are going to configure it for CDC but yeah uh yeah that's the right point but for for demo demo thing like uh currently we are hosting the our sqls on local and to show you guys how CDC works with Dum we can enable the CDC part over it and yeah you guys can play around uh if some task in future assigned to you or something like that to do some developments and all okay yeah just just pointing out that we won't be doing it on production yeah right right right won't pay but yeah uh okay yeah it might not be on production then but I think at least replica should have it because I I don't know the frequency between uh the replica and the production but for us to achieve uh this data that is getting distributed across the company right we need to put it on kavka which actually requires CDC I don't know if we will be doing it or not but it it it is one of the requirements requirement we just need to the dbas need action it because they're the ones that would need to monitor the that like the agent jobs running all that side of it to make sure it's all up to date and it's because of the impact of the production storage everything else it's their responsibility I'm just saying that we won't be necessarily be creating it on production itself yeah yeah like you said it might not be on production it might uh create a little uh hinge in the network side but uh might we might have to uh replicate uh the database that is there in the production and replica we might have to syn it pretty often that would still be um the DPA is doing replicas and stuff yeah yeah we might have to increase the we might have to increase the frequency to get the sync happening uh quickly because we are uh a to achieve a near real time uh data data capture right so if you have to do that if it is not in production then we might have to move towards that well we have clusters and everything else and that's again still managed by them um so creating the replicas and turning the CDC on which still sit under the DBA job spec role um we just need to coordinate with them to make sure that it is running as we need it and we can create the connection to the source the CDC Source where it's been def implemented but that's a discussion for a different time but I just pointing out yeah circling back to that right like I think there was an example that Pro gave us that Integra is currently present on SE 70 clusters right so those are all production are those all production servers or uh do they have equal equal number of replicas well the general setup I don't know about most like integr stuff but um the ones I know of it's they have like four servers one's a primary the other's for failover or Disaster Recovery um those are all read only so the other is the secondary one so the primary would be where you set up the CDC and everything else you can't set CDC up on a secondary because you CDC requires writing and that doesn't um that's not allowed on the secondary exactly yeah with it with it activated we might be able to read the CDC from a secondary but it needs to be activated on the primary um because we're any reading correct anyway it's a different thing we need to get continue with this continue part yeah okay uh so yeah uh now like uh once we enabled the CDC through on the SQL Server we need to like use the rest apis uh to call the like API using like Kafka connect endpoints like what are the endpoints available to configure for the so for now for the demo we configured on the Local Host c83 so we going to see it in the demo sooner okay so how to create connectors uh these are the certain like API calls right so if you if you want to create a connector just uh we need to to give the post call uh using correct like this is the like uh like X path API path to call to create the connectors and the connector will be created right we'll just going to do that in demo if we want to update a connector like if the source is changing and we don't need a particular Source right so we can do it by this manner we can update by the put API right if you want to see what all are the connectors that are already configured for particular environment we can get it using this like what are the connectors available and what are the connector plugins availables uh then there we can see by this okay so these are kind of very high level theoretical thing about the CDC with deum okay uh this is the first part uh we'll start the demo on this uh and the second part is this uh Flink okay so how now the first part is done you configured this uh I mean for SQL Server you configured the enabled the CDC configured the dbm connectors pointed it to the particular Kafka topic okay now uh you need to configure Flink okay Flink jobs so that's how it comes here okay so from Kafka the Flink jobs is going to consume the data to some ETL uh so Flink is a framework uh for Real Time data processing uh which helps to you know uh process the data in uh like distributed manner uh in a like uh like you can set a parallelism how in in a very convenient manner so that's why it's a real time right uh so yeah that is we are going to use here and after that it is like a real time ETL yeah it's real time ETL correct yeah okay so yeah after that uh Apache Flink I mean we are currently use so Flink is mainly uh written I mean uh you know uh written in like mainly used by Java itself okay but currently we are using uh py link library of it which helps us to you know uh we are going to write the Python scripts and it is going to wrap up in like jar files and get that submitted into the fling clusters okay so what we were discussing in the standup today uh is kind of like this so just I haven't covered it in the slides yet but but what is the backend architecture of Flink is like uh Flink works on like different clusters and task and workers okay so in Flink we have two things the job manager and task manager that I'm going to explain you when we are going to up uh composing our infrastructure okay so job manner will be the master node I guess getting the calls and all and based upon that distributed distribution of the task and all will be uh assigned to the particular workers and all right uh somewhere you can connect I mean correct me if I'm wrong somewhere okay so here after P yeah so we are going to use the PIP link here okay so what in PIP link there's also going to be two apis one is a data stream API and the table API so I guess the we all know SQL right so we all will be very comfortable with the table API but we have the data stream API as well that I wrote a two three jobs to show you guys how the things you going to get consumed from the topic and produce it back to the topic okay so yeah so what are the so stateful stream processing even application stateful functions this everything will be covered in it okay I mean for this demo we'll be only simply consuming and producing the data back to the different topics but in advanced Workshop that we are going to in future I'm going to conduct like as you uh last time I demoed you the watermarking stuff right so that is all like State full functions and all so that is a advanced step that we're going to discuss in the next Workshop but for now we are going to just uh consume and how everything will work I'll show you okay so this is the API like I mentioned we have the two API okay the first a fling SQL so it is SQL nothing okay but it is a Flink version of it you will have some different uh modification in SQL statements not much but you guys will be accustom with it because you know a SQL very well right so uh we have python library for pipeling we have the tle API uh so you just need to write one code and inside which you can put your SQL commands and you can execute it uh it will be easy yeah the other is like data stream apis so you will Define The Source CA topic and destination cka topic and you can uh create a whole streaming environment and you put some functions map it uh reduce it like a kind of a map reduce stuff and how you can optimize it for need okay so yeah yeah I was talking about the Flink SQL and the CDC okay so this is a kind of a small explanation of how it works so we have the Json encoded change lck the change lock is nothing but uh what is going to happen on the SQL Server if you updated the particular name of the employee you will get the CDC one for that what is the event before and what is the event after okay uh so the connector will throw that to the Kafka topic then Flink SQL we'll get it like it will create a table called clicks so and we will just put some connection strings here like we have the dbzm Json and all and if you see user ID and you'll be able to see what is the changes over it and you can do some ETL and push it to the destination topic as well right so if you want to see how it will look like okay so this is how the whole CDC will look like this is just an example so this is the metadata right so what is a connector so connector is SQL Server uh the name is a test DB on on changes that is going to occur on the database right what is the table name customers okay uh yeah like this and yeah this is the main data set like what was the before and after changes right so for example ID is par Sony so here is planet payment right but the I did some update commands on the table update email ID equal to this so I change the domain we have done it so uh event will get triggered like this format which will'll be having the before and after and we we have to consume this event uh by a Flink job and you can write some ETL stuff uh according to it right uh yeah going back to here then yeah so this is a kind of Flink code okay uh there's not nothing to worry much about it okay so I I just explained what going to happen here so I mentioned in the data stream API first we are going to create the environment so that's exactly what we are having here we are initializing the streaming environment here right for Kafka properties uh I'm assigning the local local host servers and what is going to happen and all okay uh then the source okay so it's nothing but we have the source we have the destination and in between we Hing the Flink okay so for Flink job we need to mention what is the source and and this said we need to mention what is the destination okay so that is what I'm doing here okay so Define The Source topic so if you see uh I'm passing the source topic here what is going to be a schema of it like if it's a normal string or Json schema I mentioned the string okay and Ka properties then what is a server uh that is the Kafka is running on okay the way we are going to listen on it so yeah that is it and we need so consumer is going to consume the data and producer is going to send events to another topic right so so we connected The Source after we after that we'll do some ETL here and after that we'll put the point into the destination okay so what we are doing kaf producers because we going to produce the events okay so I just put the destination topic name again it is going to be uh a string okay so for example if I'm going to convert I'm going to ingest some Json and going to get converted into the string right so I can configure accordingly right then yeah I defined the source destination and I'm just putting that addressing into environment the stream like add Source Kafka consumer and add sync as a producer okay so this flow will be completed is a simple Flink job okay so now is a demo time uh


 > [!info]
> - **CDC with Debezium, Kafka, and Flink:** The workshop focuses on using CDC to capture data changes from SQL servers, pushing them to Kafka, and then processing them with Flink for ETL. (0:40)
> - **Dockerized Images:** The entire setup, including Kafka, Flink, and SQL Server, is dockerized for easy deployment and experimentation. (2:32)
> - **Flink APIs for Data Processing:** Flink offers two APIs: Data Stream API and Table API. The Table API allows using SQL-like commands for data manipulation. (19:39)
> - **CDC Implementation:** The process involves enabling CDC on the SQL Server, which creates a separate table to store changes. (7:21)
> - **ETL with Flink:** Flink jobs consume CDC events from Kafka, perform ETL transformations, and push the processed data to destination topics. (2:00)
> - **DBA Coordination for Production CDC:** Implementing CDC in production requires coordination with DBAs to manage the potential impact on database performance. (10:44)
> - **Schema Changes:** Debezium can capture schema changes, but it might require disabling and re-enabling CDC for certain databases. (5:16)
> - **Real-Time Data Processing:** Flink is a framework for real-time data processing, enabling distributed data transformation. (17:43)