---
title: "Building a Real-Time Data Streaming Pipeline | End to End Project with Kafka Spark and Elasticsearch"
author:
  - "Cey's Data Hub"
published: 2024-10-03
source: "https://www.youtube.com/watch?v=RQ7nnobb1N0"
image: "https://i.ytimg.com/vi/RQ7nnobb1N0/maxresdefault.jpg"
created: 2025-03-20
tags:
  - "#youtube."
summary: "Build a real-time data pipeline using Kafka, Spark, Elasticsearch, and MongoDB. Analyze sentiment and visualize data in Kibana. End-to-end project tutorial."
---
# Building a Real-Time Data Streaming Pipeline | End to End Project with Kafka Spark and Elasticsearch

![Building a Real-Time Data Streaming Pipeline | End to End Project with Kafka Spark and Elasticsearch](https://www.youtube.com/embed/RQ7nnobb1N0)

> [!summary]- Description
> \#\#\# Technologies Featured:
> 
> *\#ConfluentKafka \#Elasticsearch \#MongoDB \#ApacheSpark \#HuggingFace \#DataFlow*
> 
> \#\#\# Overview:
> 
> In this video, you’ll learn how to construct a *real-time data streaming pipeline* using a dataset of *7 million records* . We’ll harness a robust stack of tools and technologies, including *Apache Spark, MongoDB Atlas, HuggingFace's DistilBERT Text-Classification Model, Confluent Kafka, Elasticsearch, and Kibana.*
> 
> \#\#\# What You'll Learn:
> 
> - How to set up and configure a Kafka topic for seamless data transmission in Kaggle Notebooks.
> - Streaming data from Kafka topics using Apache Spark.
> - Performing real-time sentiment analysis with HuggingFace models.
> - Establishing Kafka for efficient real-time data ingestion and distribution.
> - Utilizing Elasticsearch for enhanced data indexing and search capabilities.
> 
> \#\#\# Resources:
> 
> - *GitHub Repository:* https://github.com/akarce/real-time-data-pipeline-kafka-mongo-elasticsearch-pyspark
> - *Yelp Dataset:* https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset
> - *LinkedIn:* https://www.linkedin.com/in/akarce/
> - *Medium:* https://medium.com/@akarce
> - *GitHub:* https://github.com/akarce
> - *Twitter:* https://x.com/akarcey
> 
> \#\#\# Join the Community:
> 
> If you enjoyed this content, please *LIKE* and *SUBSCRIBE* for more tutorials and insights!
> 
> \#\#\# Tags:
> 
> Data Engineering, Kafka, Apache Spark, ETL Pipeline, Data Pipeline, Big Data, Streaming Data, Real-Time Analytics, Kafka Connectors, Schema Registry, Control Center, Machine Learning Integration, Data Visualization, Stream Processing.
> 
> \#\#\# Hashtags:
> 
> \#Confluent \#DataEngineering \#Kafka \#ApacheSpark \#ETLPipeline \#DataPipeline \#DataStreaming \#HuggingFace \#Elasticsearch \#RealTimeData \#BigData \#TechTutorial \#StreamingAnalytics \#MachineLearning \#DataFlow \#SparkStreaming \#DataScience \#AIIntegration \#RealTimeAnalytics \#StreamingData \#RealTimeStreaming

> [!note]- Transcript (Youtube)
> welcome to this tutorial where I'll walk you through setting up a real-time sentiment analysis system using the Yelp data set from kaggle as you can see from the architecture diagram here our system involves multiple components working together seamlessly to process and analyze large streams of data in real time this architecture starts with the Kafka producer which captures live data from the Yelp data set hosted on kaggle the data is ingested into the confluent Kafka platform where it's processed and structured from there we utilize Apache spark for data streaming and processing which feeds the data into mongod DB Atlas for temporary storage for sentiment analysis we're leveraging hugging Face's distill BT base uncased fine-tuned sst2 model to classify Yelp reviews in real time the processed data is then stored and indexed in elastic search and finally visualized in Cabana which gives us real-time insights into the sentiment of the reviews throughout this video I'll take you step by step through the setup and integration of each component starting with creating a Kafka cluster on confluent cloud setting up spark for stream processing integrating the hugging face sentiment model and finally setting up elastic search in Cabana for realtime visualizations by the end you'll have a fully functioning pipeline that ingests processes analyzes and visualizes data in real time let's dive into how each part of the system fits together in the first step search kaggle on Google and head over to kaggle.com if you don't already have an account click register you can sign up quickly using your Google account select your Google account continue and complete the registration process if the username is already taken try a different one and proceed once registered agree to the privacy and terms to finalize next head to the data set section in the search bar type Yelp inside the Yelp data set scroll down and select the business. Json file you'll see that this file contains various details such as business ID name address city state longitude latitude and more which are key to our analysis now let's move on to setting up elastic search and running the bulk upload process to insert business data into our index first search for elastic search on Google and head to cloud. elastic.co start a free trial then sign up using your Google account select your account and proceed by filling out the required information this includes details about your company your experience level and your interest in elastic once that's done it's time to create the deployment for this deployment we'll be using AWS and selecting the Frankfurt region as we be using this for all our platforms to keep data flow smooth name the deployment Yelp deployment then click create deployment this process will take some time to get ready once it's done you'll be provided with an elastic search user and password make sure to download or note these details for later next we return to kaggle go to the settings change the theme to dark and complete phone verification this will enable internet access in our kaggle notebook sessions allowing us to use T4 gpus for additional computational power which is especially useful for machine learning tasks now let's create a new new notebook name it elastic bulk upload start by removing the sample code and ensure internet access is enabled add the Yelp data set to your notebook environment first install elastic search by running the command exclamation pip install elastic search once installed we'll move to writing the bulk upload code in this snippet we're working with elastic search and the Yelp data set first we import the necessary libraries elastic search and helpers from the elastic search module to interact with elastic search and Json for handling Json data next we establish a connection to our elastic search Cloud instance we use the elastic search class providing the URL for our elastic search deployment in this case it's a placeholder where you'll insert your specific endpoint additionally we disable certificate verification by setting verified excerts equals false for basic Authentication we include the username and password which you'll also need to replace with your own credentials now we proceed to open the Yelp data set file we use the open function to read the file located in the kaggle environment the file contains business data in Json n format so we initialize an empty list called Data to hold the process documents we Loop through each line in the file attempting to load each one as ajacent object if successful the object is appended to the data list in case of any Json decoding errors we handle them gracefully using a TR accept block and print the error to the console once all the documents are read we use the helpers do buul function from elastic search to upload the data in bulk this function takes our client as the connection the dat data list containing our documents and the name of the index where the data will be stored which you'll need to specify and with that the business data from Yelp is uploaded to your elastic search index once the code is ready return to the elastic search portal and change the theme to dark by using the drop down in the top right corner of the page then go to the left drop down and open manage this deployment in a new tab in the application section copy the elastic search endpoint URL and paste it into the endpoint Ural section of your notebook next go back to the manage this deployment Tab and under the security section reset the password for the elastic user copy the password then fill in the basic authentication fields in your notebook use elastic for the username and paste the copied password into the password field once that's done return to the elastic search portal and navigate to the management section from the left panel under index management create a new index and name it businesses the index is now created but empty back in kaggle run the notebook cell to upload the business data to the business's index there will be warnings about insecure request warning due to unverified https requests this is expected and not a problem for us returning to the elastic search portal under index management you can see the documents being loaded rapidly starting from 34,000 documents and eventually reaching around 150,000 documents approximately 185 megabytes in size with that the data is successfully uploaded to the index returning to kaggle We Now focus on the reviews. Json file from the Yelp data set this file is significantly larger than the business. Json file weighing in at 5.34 GB given the size it contains a wealth of data The Columns in this file are essential to understanding how users engage with businesses and their reviews there are nine key Fields review ID a unique identifier for each review user onch ID identifies the user who wrote the review allowing us to track individual reviewers business doc ID this is a crucial field as it serves as the common key to join the data from the businesses. Json file which we've already up loaded Stars represents the star rating the user gave the business from 1 to 5 useful funny cool these Fields track how many people found the review helpful humorous or cool respectively they provide insights into how other users engage with the review text this is the actual content of the review where sentiment analysis will come into play date the date the review was posted useful for tracking Trends over time we'll later utilize this business ID field to join this data set with the businesses. Json file we uploaded to elastic search allowing us to link reviews to specific businesses the large text Fields will be valuable when performing sentiment analysis using the hugging face distill Bert model next we head over to confluent to set up our real-time data streaming pipeline first search for confluent on Google and navigate to confluent doio if you haven't done so already sign up for an account or log in you can sign in with Google for quick access once inside confluent Cloud go to environments and add a new Cloud environment name it Yelp environment now it's time to create a Kafka cluster select the basic plan for Simplicity then configure it by selecting AWS as the cloud provider and Frankfurt as the region similar to our previous elastic search setup for data flow consistency name the cluster Yelp cluster and launch it after the cluster is set up go to the API key section from the left pane here create create an API key for your account download it and save it securely as it will be required for later steps when connecting Kafka now go to the cluster settings section this is where you'll find the bootstrap server address this will be important later when we connect our Kafka producer to the cluster next we need to set up a Kafka topic head to the topic section and create a topic named raw topic finally we need to define the schema for our messages as we want to enforce a consistent structure for the data select avos schema and remove the sample schema provided by default then paste in the correct Avro schema that we prepared for our RAW reviews data this schema ensures that all messages follow the same structure particularly useful for ensuring compa ability when multiple consumers are reading from the same topic after creating the topic and schema the raw topic is now set up and ready to receive data however it's still empty at this point in the next steps we'll write a Kafka producer to stream the Yelp reviews. Json data into this topic for real-time processing start by opening a new kaggle notebook and name it Kafka producer in the first cell write the following command to install the necessary Library exclamation point pip install confluent Kafka afterward add the Yelp data set as an input to the The Notebook additionally we need to adjust file persistence settings from The Notebook session options this ensures that if the notebook restarts or is interrupted the Kafka producer can continue where it left off maintaining the streams integrity and processing flow run the first cell to install the package now let's focus on writing the core of the Kafka producer code we'll be writing a python script to read the Yelp reviews from the Json file and produce the messages to our Kafka topic first we'll start by importing the necessary libraries we need Jason to handle our Json data OS for file operations pandas for handling time stamps and the producer class from confluent Kafka to connect to our Kafka instance next we'll set up our Kafka producer configuration this is where we Define how our producer connects to the Kafka cluster we create a dictionary called comp to store our configuration settings within this dictionary we'll specify the bootstrap server this is crucial as it tells the producer where to find our Kafka cluster you should replace the placeholder with your actual Kafka boot strap server address next we configure the security protocol to use sasl SSL for secure communication we'll also specify the sasl mechanisms and provide our Kafka API key and secret which are essential for authentication these will be inserted into the sassel do username and sassel password Fields respectively lastly we set a client ID to identify our producer application after setting up the configuration we create an instance of the producer class using our kif dictionary this initializes our Kafka producer ready to send messages now we need to define the topic name where we will be sending our messages in this case we assign the variable topic to the string raw topic next we will create a delivery report callback function named delivery report this function will be called after a message is sent to Kafka inside this function we check if there was an error during the message delivery if an error occurred we print an error message indicating that the message delivery failed along with the error details If the message was delivered successfully we print a confirmation message that includes the key of the delivered message following this we'll Implement a function to read a checkpoint from a specified file this is important for resuming the message production after an interruption the function read checkpoint takes a checkpoint file as an argument it checks if the file exists if it does it opens the file reads the last successfully sent message index and returns it if the file doesn't exist we return zero indicating that we haven't sent any messages yet next we will Define a function called right checkpoint this function takes the checkpoint file and the current index as parameters it writes the current index to the checkpoint file allowing us to keep track of which messages have already been sent after updating the checkpoint we print a message confirming the updated index we also need to handle dates within our Json records so we'll Define a function called handle date this function checks if the object passed to it is a panda's timestamp if it is it formats the Tim stamp into a string format if it encounters an object of a different type it raises a type error indicating that the object is not Json serializable now we're ready to write the main function that will handle the streaming of our Json data we'll call this function dream Jason serially it takes the path of the Json file and an optional checkpoint tart file argument defaulting to a specific location in the kaggle working directory within this function we first read the last sent index from the checkpoint file this tells us where to resume processing the Json lines we then open the specified file path for reading as we read through each line of the file using a for Loop we check if the current index is less than the last sent index if it is we simply skip that line for each line we attempt to decode it from Json if successful we call the producer produce method passing the topic name the review ID as the key and the record as the value we use the Json dumps function to convert the record to ad Json string applying our handle date function to ensure any dates are formatted correctly after producing the message we call producer. flush to ensure that the message is sent to Kafka immediately then we update our checkpoint file with the new index to indicate that this line has been processed if ajacent decode error occurs at any point while reading the file we catch that exception and print a message indicating the failure to decode that specific line finally we include a conditional statement to check if the script is being run as the main program if so we call our stream Json serially function passing in the path to the Yelp reviews Json file this completes the Kafka producer setup allowing us to efficiently stream our Yelp reviews to Kafka where we can further process them as needed next go to the confluent cloud dashboard navigate to the cluster settings and copy the bootstrap server address from the endpoint section paste this into the bootstrap doers configuration next go to API Keys create a new API key and copy both the API key and secret these will be your Kafka credentials paste the API key into the SA username field and the secret into the sassle do password field for the Kafka topic go to the topic section on confluent find your topic in this case raw topic and add that name to the topic configuration in the notebook after writing the code run the Kafka producer notebook to start sending messages if the notebook is interrupted we want it to be able to recover and continue processing without data loss to check this insert a simple cell containing exclamation PIP show confluent Kafka to ensure that the package remains installed even after restarting or interrupting the notebook session this persistence is key in maintaining a long running producer without losing progress run the main producer cell if any errors like a syntax error appear such as using a single equal sign instead of double equals in condition checks fix them and rerun the cell now the kka producer will begin streaming messages from the reviews. Json file into the CFA topic raw topic to verify that the data is Flowing properly go back to confluent navigate to the topic section and open raw topic inside the messages tab you should see the incoming messages from your notebook all adhering to the defined schema this confirms that the Kafka producer is functioning correctly and streaming data in real time next cancel the current run restart and clear outputs from The Notebook and stop the session finally as a demonstration of how the notebook recovers from interruptions run the producer again after restarting the session check if the notebook continues from the last processed message confirming the checkpoint is working the messages will resume from where they left off message 163 and the checkpoint will update accordingly ensuring no data loss during interruptions first open a new tab and search for mongodb Atlas go to the mongodb atlas website and click on start free sign up with Google choose an account and continue after accepting the privacy policy mongodb will ask you some questions to get to know your project better things like your primary goal how long you've been developing and the types of data you're working with once you filled these out the system will automatically redirect you to deploy your first cluster for this choose the m0 cluster tier which is free and suitable for exploring the cloud environment you'll also want to make sure to select AWS as the cloud provider and Frankford as the region to keep it consistent with our other deployments when naming the cluster use Yelp cluster note that mongodb doesn't allow underscores in the cluster name so Yelp cluster with a dash will work just fine once you've done this click create deployment while your cluster is being initialized navigate to the network access section on the leftand menu click on ADD IP address and select allow access from anywhere click confirm to save these changes next we need to create the database users go to the database access section and click add new database user for the first user set the username to spark for the database user privileges choose the built-in role Atlas admin this user will be used for spark streaming in the upcoming steps once you've done that click add user now create a second database user with the username source and password 123123 for this user set the Privileges to read and write to any database this user will be used later with the confluent Source connector which will handle the data flow between mongodb and Kafka at this point your mongod DB cluster Yelp cluster should still be initializing so let's shift over to confluent now in the confluent cloud go to topics from the left hand menu and create a new topic the topic name must match the mongod DB database and collection we're going to create later so name it reviews db. enriched reviews collection once the topic is created with default settings click on create schema you will now add the schema structure for this topic it's essentially the same as the schema for the RAW reviews data but with an additional field for sentiment after pasting the schema structure into the editor click create the new schema is now set up but the topic is empty and waiting to receive messages back in mongodb Atlas your cluster should be ready by now go to the overview page of your cluster and click connect to see the connection string URI this is important as it contains the details for connecting external applications to mongodb copy this connection URI for later use when setting up the mongodb source connector now let's create the actual database and collection that we will use to store the enriched review data go to collections and mongodb will have a sample database and collections pre-initialized we'll create our own by clicking on create database name your database reviews DB and the collection enriched reviews collection it's crucial to ensure that this collection name matches the Kafka the topic we created earlier once named click create at this point the collection is ready and waiting to receive documents just like the Kafka topic is waiting to receive messages finally let's head back to kaggle where we will create a new notebook to handle spark streaming this wraps up the mongodb atlas and Kafka topic configuration portion of the setup now we're ready to move on to spark streaming and data processing in The Next Step let's now walk through the process of setting up the py spark streaming notebook integrating Kafka mongod DB and hugging face for sentiment analysis in detail set the session options to use a GPU T4 accelerator and enable file persistence so that if the session is interrupted or closed it can continue from where it left off name the notebook spark underscore stream uncore Kafka uncore toore next remove the sample code provided in the notebook and move on to installing P Spark in the first cell type exclamation pip install Pi Park equals equals 3.5.2 and run the cell to install P spark we'll start by setting up spark importing necessary libraries and defining our Kafka and mongod DB configuration first we import the required P spark modules spark session to create a spark session from Jason and call for manipulating the streaming data and UDF for creating user defined functions we also bring in struct type struct field string type integer type and Float type for defining the schema of our Kafka messages then from the Transformers Library we import the pipeline function we also import the logging and Os modules to manage logs and file paths next we configure logging to only display errors and we create a checkpoint directory where Spark will store its progress this directory ensures that if our notebook restarts the stream continues processing from where it left off if the directory doesn't exist we create it using os. deers now we Define the config dictionary which holds our Kafka and mongod DB credentials this includes the Kafka bootstrap server API key and API secret for the Kafka credentials copy the same ones we used in the Kafka producer notebook for mongodb we specify the the URI database name and collection name where the enriched reviews will be stored for mongod DB go to the mongod DB Atlas cluster click on connect select drivers and copy the connection string from there you will need to replace the username and password with the credentials we created earlier for the spark user username spark password spark 1 2 3 1 2 3 now to integrate sentiment analysis go to hugging face by opening a new tab on their website navigate to the model section and filter by text classification models sort by most downloads and you should find a model called Dilbert Bas uncased fine-tuned sst2 English click on the model and on the model page you'll find an option to use this model copy the string provided as we'll need it later for the sentiment analysis pipeline now return to the notebook we Define a function analyze sentiment that takes text as input and Returns the sentiment label like positive or negative e we also wrap this function in a user defined function UDF called sentiment UDF which allows spark to apply it to the streaming data now we move to the core function read from Kafka and write to this function reads the streaming data from Kafka and writes the enriched data with the sentiment analysis to mongodb and to read the streaming data specify the topic name the RAW reviews topic is named raw topic which is where our Kafka producer is currently sending messages first we Define the schema of the Kafka messages using struct type the schema includes Fields like review ID user ID business ID Stars useful funny cool text and date all of which come from the Yelp reviews data set then we configure the Kafka stream by specifying the Kafka bootstrap server we subscribe to the raw topic which is the Kafka topic containing RAW reviews security protocol and sasl authentication settings e e the Kafka stream data arrives in a binary format so we need to parse it we convert the raw kfka messages into a structured data frame using from Json applying the schema we defined earlier this gives us a structured data frame with all the fields we need need next we enrich the par data frame by adding a new column called sentiment we apply our sentiment UDF to the text field which contains the review text and the UDF classifies each review as either positive or negative finally we write the enriched data frame to mongodb we use Spark's right stream function specifying mongodb's connection details e the checkpoint location points to the directory we created earlier which ensures that the stream's progress is saved e we specify the required jars using spark. jars. packages these are the Kafka and mongodb connectors needed to work with both systems go to the maven repository website and search for spark SQL Kafka select the version that matches our P spark setup which in this case is 3.5 .2 click on Gradle cotlin and copy the library string provided next go back to the maven repository and search for spark connector again select the latest compatible version and copy the library string from there now that we have all the required dependencies e once the code is written let's check if the notebook is still alive and running however we forgot to select the GPU accelerator so we'll go ahead and add that now as the notebook restarts we'll need to reinstall P spark so rerun the first first sell pip install p park go to mongodb atlas and open the enriched reviews collection to monitor when our streaming process starts also check confluent under raw topic to ensure the messages are still being sent from the producer next run the main code cell which contains the streaming script e here we encounter an indentation error in the if block the indentation is too deep so we'll reduce it to the lowest level of indentation after fixing that we run the code again now another error appears which is a typo in the spark spark session Builder we forgot to include the equal sign between spark and Spark session Builder let's add the equal sign and rerun the cell the next error we run into is a pi4j Java error which is caused by a version conflict between the Jr files we mistakenly specified the wrong SC version when pulling the Jr files from Maven our environment uses Scala 2.12 but the Jr files we chose were for Scala 2.13 let's go back to Maven search for the 2.12 versions of the required libraries and update our code with the correct versions at this point the notebook should be restarted again to ensure the environment is reset with the correct libraries after restarting reinstall pypar pip install ppar equals equals 3.5.2 and rerun the main cell e for this time the code runs smoothly and the streaming process begins once you run this notebook it will continuously read reviews from Kafka classify their sentiment using hugging face and write the enriched reviews to mongodb this process will keep running until the notebook is manually stopped now go to mongodb atlas and check the enriched reviews collection you should see the first few documents arriving in the collection after a short while check again and you should see around 160 documents with a new sentiment field indicating that the hugging face model is successfully classifying the sentiment of each review more than 200 messages have now been streamed into the mongodb collection and there are still many more messages to process in the Raw topic around 13,000 messages total the streaming process is catching up slowly but it's working as expected finally we'll go back to the confluent platform navigate to the connectors page and create a source connector for mongodb Atlas ensuring our data flow is properly connected we start by going to the confluent connectors page and selecting the mongodb atlas Source connector for the topic prefix we leave this field blank click continue and on the next page generate an API key for the connector and download it click continue again and we now provide the mongod DB Atlas database details for the connector to do that we'll need the following information connection host to get the con connection host we go to mongodb atlas open our cluster overview click on connect select drivers and copy the host URI connection user the mongodb redon user we created earlier source connection password the password we set 123123 database name reviews DB collection name enriched reviews collection for the Kafka record value format we select Json to ensure the data is serialized correctly we also enable the publish full document only option so that each document from mongod DB is sent as a complete record to Kafka single message transforms we now scroll down to the transform section to add single message transform for transform zero we select the value 2 key transformation this allows us to move the review ID field to the key of the Kafka message we specify the field as review ID since this is the unique identifier we want for our Kafka records we select the extract field dollar key transformation type this ensures that the key field contains only the review ID making it the key for each record in Kafka remove the underscore ID field which we don't need in our Kafka topic so for transform 2 we select the replace field value transformation type and add the underscore ID field to The Blacklist this ensures that the underscore ID field is removed before the document is written to Kafka we then click continue to move to the next steps and finalize the connector configuration the connector will now provision itself and after a short delay it should be up and running with the connector now running we go back to mongod DB and check our enriched reviews collection we can see that 2,600 documents have already been streamed into the collection by our spark stream these documents will now be transferred to Kafka via the mongodb source connector let's go back to confluent and look up our topic reviews db. enriched reviews collection we can see that 151 messages have already arrived via the connector and everything looks clean and healthy we refresh the page and now 300 messages have arrived indicating that the data is Flowing correctly and at a steady Pace now that the mongodb atlas Source connector is working successfully the data is streaming from mongodb into Kafka the setup is complete and the data is being processed and enriched without any issues next we can move on to configuring elastic search for indexing and Performing additional analysis on on the streaming data step one creating the reviews index with schema first we navigate to the management section of elastic search and click on index management from there we open the console at the bottom of the page we remove any sample commands in the console and paste in the index creation command we prepared earlier which defines the appropriate data structure for holding the enriched reviews after the Kafka sync connector processes the data after running the command we receive an index created successfully message indicating that the reviews index has been successfully set up we need to create an enrichment policy to extract relevant fields from the business's index while this can be done via the UI we'll use the console for this process we paste the command to create the enrichment policy which specifies which fields we want to extract and transform after running the command we get an acknowledgement with a message acknowledged true confirming that the policy has been successfully created now we close the console and go to the enrich policy section under index management here we can see that the enrichment policy named business policy has been created however the policy needs to be executed before we can proceed with creating an ingestion pipeline we go back to the console paste the command to execute the enrichment policy and run it this command will apply the policy to the reviews data we get another acknowledged true response indicating that the policy has been executed successfully next we create an ingestion pipeline that will enrich the data as it flows from Kafka into elastic search to do this we paste the command containing the underscore inest pipeline setup which assigns the business policy to the pipeline we run the command command and once again we get an acknowledgement message confirming that the ingestion pipeline has been created step five assigning the ingestion pipeline as default the final step is to assign the ingestion pipeline we just created as the default pipeline for our reviews Index this ensures that any data coming into the reviews index through the Kafka connector will be processed by this pipeline by default we run the command that assigns the pipeline as the default and we get the message acknowledged true indicating that the operation was successful now that elastic search is set up we can return to confluent to configure the elastic search sync connector to push data from Kafka to elastic search with everything in place the data will be streamed from mongodb to Kafka enriched and finally pushed to elastic search for indexing and Analysis now let's create the sync connector first select the topic reviews db. enriched reviews collection click continue and generate an API key be sure to download this key next we need to input the connection details go to your elastic search deployment in the manage deployment section navigate to security and reset the password for the elastic user copy the newly generated password and paste it into the password field the user remains elastic now return to the manage deployment section find the Yelp deployment and copy the endpoint URL paste this URL into the connection URI field move forward and for the Kafka record value format select Json be sure to change the schema ignore option to true as we don't want to enforce strict schemas before we create the conector make sure that the index and ingestion pipeline have already been set up in elastic search if not the connector will create the index with default configurations which may not fit our needs now click continue and create the connector the provisioning process will start and after a short time our index will begin receiving documents as you can see within a very brief time span our index has received a approximately 3,000 documents next let's go to the index overview section and click on Discover index we'll create a data view to visualize the data name the data view as you like choose the index pattern matching your index name and select the appropriate timestamp field I won't use the time filter in this case finally click save data view to kibana You'll now see the data presented in the correct format aligned with the connector and the ingestion pipeline that we previously created you can search the index Fields here using the UI now let's move on to setting up a dashboard in Cabana for realtime visualizations navigate to analytics and select Dash boards from the left pane we'll create a new dashboard start by creating visualizations for the first visualization drag the sentiment field to the visualization section select donut as the visualization type save it next create a new visualization drag the review ID field and select Legacy metric as the visualization type rename it to Total reviews and save it for the third visualization drag the Stars field select average As the metric and save it next we'll create a map drag the business. geolocation field save it and add the map to the dashboard board \[Music\] for the fifth visualization drag the business do City Field select tag Cloud as the visualization type and save it it groups some cities under other by default but will disable this grouping go to Advanced disable group remain remaining values as other apply and save the visualization now drag the business name field use tagcloud again and disable the other group similarly save and return next create a table visualization drag review ID set the number of values to five and disable other in the advanced settings save it for the bar horizontal stacked visualization select stars for the vertical axis and set some of cool for the horizontal axis you can add fields for some of funny and some of useful as well e adjust the settings for granularity and save the visualization finally for the last visualization drag the date field use line graph as the visualization type adjust the horizontal axis to show weekly intervals and ensure that it's not bound to the global time picker save the graph set the update interval to a few seconds now let's save the entire dashboard and give your dashboard a name with the dashboard complete we can explore our data you'll see we have nearly 6,400 reviews inserted with an average star rating of 3.8 the tag Cloud on the right shows the most reviewed 10 businesses and the city Cloud visualizes the most reviewed cities our sentiment analysis shows that 26% of reviews are negative while 72% are positive one user has made six reviews making them the most active let's drill down to only the positive sentiment records we see that the total number of reviews is now 4,600 with an average star rating of 4.4 now remove the selection let's check out Honey's Sit and eat one of Philadelphia's most popular businesses it has 24 reviews with an average rating of 3.9 Stars we can even see its exact location on the map view as we explore the data the dashboard continues to update in real time since we started more than 400 new documents have been inserted into the index showing that the pipeline is working correctly thank you for watching if you have any questions or need further clarification feel free to reach out via LinkedIn Twitter or email I appreciate your support and if you enjoyed this content please like And subscribe to stay updated on future videos your feedback means a lot to me


 > [!info]
> - **Real-time Sentiment Analysis System Overview (0:00):** The system captures live data from Yelp, processes it through Kafka, uses Apache Spark for data streaming, MongoDB for temporary storage, Hugging Face for sentiment analysis, Elasticsearch for indexing, and Kibana for real-time visualization.
> - **Setting up Elasticsearch (2:31):** Create a free trial account on cloud.elastic.co, configure a deployment using AWS and the Frankfurt region, and download the Elasticsearch user and password.
> - **Kaggle Notebook Setup (4:05):** Enable internet access in Kaggle notebook settings by completing phone verification to use T4 GPUs for additional computational power.
> - **Bulk Upload Code for Elasticsearch (5:04):** Use the `elasticsearch` library in Python to connect to your Elasticsearch instance, read data from the Yelp dataset JSON file, and upload it in bulk using the `helpers.bulk` function.
> - **Confluent Kafka Setup (12:24):** Create a Confluent Cloud account, set up a Kafka cluster with the basic plan on AWS in the Frankfurt region, create an API key, and note the bootstrap server address.
> - **Kafka Topic and Schema Creation (13:56):** Create a Kafka topic named `raw topic` and define an Avro schema to ensure consistent data structure.
> - **Kafka Producer Code (15:13):** Install the `confluent-kafka` library, set up Kafka producer configuration with bootstrap server, API key, and secret, and implement a delivery report callback function.
> - **Checkpoint Implementation (18:45):** Implement functions to read and write checkpoints to resume message production after interruptions, ensuring stream integrity.
> - **Data Streaming Function (21:19):** Implement a `stream Json serially` function to read Yelp reviews from JSON, produce messages to Kafka, and handle date formatting.
> - **Spark Streaming Setup (34:59):** Set session options to use a GPU T4 accelerator and enable file persistence.
> - **Spark Configuration (36:13):** Configure Spark by importing necessary libraries, setting up logging, and creating a checkpoint directory.
> - **Sentiment Analysis Integration (40:07):** Integrate sentiment analysis using Hugging Face's DistilBERT model and wrap it in a user-defined function (UDF) for Spark.
> - **Data Enrichment and Writing to MongoDB (47:11):** Enrich the parsed data frame by adding a sentiment column and write the enriched data frame to MongoDB.
> - **MongoDB Source Connector (1:00:17):** Create a source connector for MongoDB Atlas, providing database details and setting Kafka record value format to JSON.
> - **Elasticsearch Index and Enrichment Policy (1:05:12):** Create a reviews index with schema, create an enrichment policy, and execute the policy.
> - **Ingestion Pipeline Setup (1:07:11):** Create an ingestion pipeline and assign it as the default pipeline for the reviews index.
> - **Elasticsearch Sync Connector (1:08:02):** Configure the Elasticsearch sync connector to push data from Kafka to Elasticsearch.
> - **Kibana Dashboard Setup (1:11:17):** Set up a dashboard in Kibana for real-time visualizations, including sentiment analysis, total reviews, average star rating, and business locations.