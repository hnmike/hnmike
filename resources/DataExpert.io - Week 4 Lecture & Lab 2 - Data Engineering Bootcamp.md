---
title: "DataExpert.io - Week 4 Lecture & Lab 2 - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2024-12-31
source: "https://www.youtube.com/watch?v=Addr237IWsI&t=6600s"
image: "https://i.ytimg.com/vi/Addr237IWsI/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCggZSg_MA8=&rs=AOn4CLDe68nuKNq68Np8S40xCsgXvQN1LA"
created: 2025-03-23
tags:
  - "youtube"
  - "Data_Engineering"
  - "Streaming_Architecture"
  - "Flink_Bootcamp"
summary: "Week 4 of the Data Engineering Bootcamp covers real-time data pipelines, Lambda & Kappa architectures, Flink windows (tumbling, sliding, session), UDFs, watermarking, and parallelism."
---
# DataExpert.io - Week 4 Lecture & Lab 2 - Data Engineering Bootcamp

![DataExpert.io - Week 4 Lecture & Lab 2 - Data Engineering Bootcamp](https://www.youtube.com/embed/Addr237IWsI&t=6600s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> hi everyone welcome back so today we're going to be streaming day two of the lectures in the labs of Zach's boot camp so let's get into it it's yeah it's it's been fun and if anyone's new not if anyone's not new here like and has been here before let me know what you think of the new layout if you like it if I should make any changes to it I think it's a lot better than before I'm slowly getting better at this YouTube and like it i' like to think but yeah let let's get into it so we're just carrying on pretty much straight away from what um what we did yesterday so yeah let's just start the lecture crazy right you're taking this one second I'm just going to make sure that my headphones are connected for some reason they've just disconnected just me to have okay class for credit make sure to so this is in now um yeah let's go switch over to the next tab so you can get a credit for day two I hope you enjoy it I wanted to talk a little bit today about what is actually going on underneath the hood of like you saw how like uh on Tuesday there was uh we have this Kafka thing that like you hit a website and then data just shows up kofka right and so I wanted to kind of explain like more in detail of like this right here is the architecture of my entire website so let's start over here with like stick figure man so this is you know this is you or a student or anyone who's visiting my website and then the first thing that happens is you'll see that like if I want to go to exactly.com what happens is it gets that request gets intercepted so there's this http the Interceptor oh my God like I've only just realized that he it's called exactly as in exactly um I always called it eactly some reason that's really really like embarrassing I can't believe I didn't know that that's what it was pronounced like and what that does is it just looks at the request and then it logs it down to Kafka and then but it then it also just passes it forward so then um exactly.com can then determine what to do with that request after uh it hits the server here hits hits Express but then like okay when that request gets logged down it goes to K it goes to a kofka producer and then that Kafka producer puts it on Kafka and then we can then uh read from Kafka on the Flink and then we can either dump it to another kofka topic which is what one of those jobs did uh in uh day day two or day one and there was the other job that dumped it to your local postgress right and then it did some of that like you know a geocoding IP address stuff as well so uh I want to talk about the other path here right so if it uh a lot of times when it hits a server then uh what the the website will be like okay is this a web page request which means it's like a normal like kind of like HTML request it's like are you asking for the about page or the login page or the Boot Camp Page or like whatever page right if it is a web page requ then what it does is it renders it server side because we use uh react right and it does serers side rendering and then it passes the rendered HTML to the client and then that client is now ready for the the user to work with to click and watch videos or whatever you want to do but if it's not then if it's not a webpage request that means that it's an API request and if it's an API request that means that it's probably an update or create or delete request so in this case uh it's going to hit postgress remember I was talking about uh in the Q&A how we use the per stack here so this is postgress right post goes over here Express right here we have react and next next and react are kind of like coupled together and then all the whole thing kind of like runs on node.js as well so then uh postgress will run and this is like maybe you sign up and then uh like if you sign up that's an API request and then uh it hits postgress and then it goes back to the server and then it will redirect you because after you sign up usually it's going to go back to another page like the login page or um if you log in which is also an API request it will then take you to the boot camp page right and then so this is essentially the architecture behind exactly.com but my whole point here is streaming needs to have some type of real-time architecture like this it has to have a way to intercept real time events so there's more to streaming than just kofka right there has to be another layer there has to be the event generation layer and in this case the event generation layer is this HTTP Interceptor and then that and that makes it so we can see every time this is every time an action is taken by a user that's what happens it gets dumped to kofka so this is essentially the architecture that we're kind of working with uh in our boot Camp so okay so so what this code does is it sits between you and when you hit my server and you'll see what it has is uh you this like it's called an API event middleware and essentially what it does is you see has Rec res and next so w is the request res is the response and next is how if I call next that means it passes the request to the server so what happens here is uh first I have this this line here that says okay should it be logged and essentially I'm like I don't want like if I'm in development and I'm doing like a Dev work I don't I my website's on host and I don't want my events my development events to be logged to kofka because I think that that's dumb because it's not real traffic so and then there's like uh there's also I don't want it to be a file request so like every time that it requests like um an image like my logo or like uh any other sort of like like the LinkedIn logo or all the different images that are on my website I also don't want to log back because it's just like excessive and I only care about like the web page requests and the API requests so that so in in C like. netland um when I so so when I've done this in net like I've done this in a couple of different ways so I've either used something called Mass Transit with RIT and Q uh and the way that you can do that is you can either send um to add like to you can do send requests or you can do publish like requests so typically for like events you would do publishing onto the message bus and you wouldn't necessarily care if like um you you aren't necessarily waiting for a response you just want to make sure that it gets onto the bus um and then I've also done it with event hubs where you um have uh like you you you again can can publish those events and then you can have consumers that consume those events uh so it it's very it it feel it feels very similar to that um so did you buy the paid boot camp which kicks off in from January I have actually bought the paid boot camp which kicks off in January I paid I bought that like um like a couple months back literally I bought it 10 days I think before the free boot camp was announced um so but but I'm quite I'm quite happy with that because I think the pre boot camp is going to be a nice um segue into the paid one and then hopefully when I start the one is going to be quite easy that's that's the hope maybe not but that's the hope and then hi I just to your channel and I watched this exact video today oh did you how did you find the video um yeah I I quite like this this um this topic yes I actually have done that already uh so everything that I've done I've already streamed so if you go on to all my live then you'll be able to see the previous like setups and stuff like that so you should be able to view those and watch those then we have this event and this is the schema right that if uh if you remember like when we were working with kofka this is the schema that we work with right and you have the URL the referrer you have the user agent this could be like if it's Google bot or Chrome or Android right and so then you have your schema and then I have like this essentially if it should be logged or not and it's like okay if it passes the should be logged then it sends the message to Kafka and then it calls next which means that then it passes the request to the server and then the server does what it's supposed to do and then otherwise it just passes it to the server anyway like um if like it shouldn't be logged so that's essentially what this piece of code does this is a little tiny piece of code that sits between you and my server that like uh essentially looks at every single request that comes in so I'm not entirely sure the differences between the monthly um paid option and the boot camp I think the boot camp you are with a cot of other people so you can learn with them you are part of a community you have a certification that comes at the end of it whereas I think with the paid like monthly option you get access to a lot of the you get access to the content but I don't know if you get the um assignments and uh the certification at the end of it okay so then we have the producer code so this is the producer code uh or you saw you saw how like here we have that send message to Kafka in the Interceptor well here is that send message to Kafka and then I create this Kafka producer here which uh has a a similar sort of SSL connection like um uh like we do in Flink where we have to connect with like all those like SSL credentials because we need to do that on the producer side as well and then you'll see here I'm saying okay right to uh this porcupine boot camp events so this is that topic or like I want to write this message to this table and then I have my message object which is essentially just the object that I was I have over here this event object I'm going to be writing that and then what we do is we uh start up the producer and then we send the data to the producer and then this producer. send line will um hopefully send the data to kofka if it does then um it will say it will get a log that says this um this uh is yeah it's it's very similar to like a hubs and Transit and stuff um it's funny actually uh um I remember I had an interview like like ages ago like um and I was talking about event hubs in in the interview um and because the person had only dealt with like kfka before they didn't understand when I was trying to say like event HS is very similar to capka but I haven't worked with CA before uh they kept thinking I was talking about HTTP requests and I was like no that's not how um how event hopes works but yeah uh but yeah it's just funny how like you have all these different products that pretty much do very very similar things but if people don't know about that product they they they don't realize that message sent successfully and then it will say result and then it will end otherwise it will um it will it will send an airor message for us and but in and since it's just logging to Kafka I don't want this to throw in air because it's just logs right so I essentially just say okay it's fine if it does if it logs or it doesn't log I don't care I just want um uh because I I don't want that to stop the traffic to my website so anyways those are the two pieces that happen Upstream of events Landing in cof so so that's true in comparison for the free boot camp versus the paid boot camp but but what um leesh is referring to is you have the ability to pay monthly for Zach's paid content which I'm pretty sure gives you that access to the cloud environment but you don't get access to the shared like um like kohar experience and the q&as um and the live sessions so now I want to talk a little bit more about the future architecture of exactly.com and so this is like you'll see is very similar except it has a couple more pieces to the puzzle here right so you'll see um we have our Flink um job down here and then we have another Cofe topic and then you see how we have this like listens for events and then here's that cof consumer that Doug was talking about so this cof consumer is just listening on this uh processed cofk you and then what will happen is this consumer will then push events back to exactly.com and then exactly.com will push those events through a websocket and then the client will be updated in real time so that's the idea I mean obviously like we're not quite there but this is what a DAT this is what a realtime data product looks like is where you have a full loop where it has like uh you have data coming in and then it goes through and then it goes back out back to the client and you have this like real time event stream and the client is very aware of what's going on and so this and all this data would be updated as quickly as it possibly can and so the latency here would be very low like on the order of seconds for sure and uh and so that's kind of the idea of what uh the future will look like and hopefully the future of this boot camp we will be able to implement more of these pieces as well to show people how to build uh their own uh realtime updating UI so that they can have their own kind of oh thank you I really appreciate that I'm really really hoping to start at doing the blogs again I've just been so busy with this boot camp uh that that's kind of taking a backseat for now but once I've got through this boot camp and the next boot camp I am able to focus so much more on the actual content creation side so I've got a good amount of plans for 2025 which is going to be creating um a lot more content both on my blog and on my on on YouTube so that's going to be good and hi Tom how are you you doing so they can see the real power of Flink and be able to build their own analytics dashboards and stuff like that but that's kind of the idea of where the future is going to go with the architecture of the stuff but um for uh today we're just going to be working more with um uh we're going to kind of be still kind of more just in this another kofka topic and your local postgress so anyways one of the big things that I think is going to be a massive massive thing in the future and going forward I've already seen this happen at Netflix and at Airbnb is the whole idea of owning what's called a data product where you have all this data that you're crunching but then that data somehow manages to get back into the app and that's what's gonna like that would be the idea here with these red things that are not quite built yet but that would be the idea is that's how you would build a a closed loop because you see how now there's like a loop here right it's like you can you can like there's like a you can go around in circles as many times as you want right and that's a that whole idea of like a closed loop system is very powerful so um this is kind of the idea behind uh data products I want to talk a little bit more about websockets because I think websockets is something that not very many of you are probably aware of like what a websocket even is so generally speaking when you are building a website you have your client and your server and the client always asks the server for more data and uh the client almost always initiates the request in if you don't have if the server needs to initiate the request like say the server gets new data and then uh like the client usually has to ask for that new data even if the server got it and knows it's got it so the way that and that's like normal HTTP that's how normal HTTP Works um and so if you want to have more of that two-way communication where the server can send stuff to the client as well then you have to do this thing called a websocket and websocket is going to be how you um end up getting that like two-way communication and that those real time updates so I want to talk a little bit more about uh competing architectures here for um uh like how I'm good thank you how I am good um I was just about to ask how you are but you've already said how you are yeah no I'm I'm very good thank you just had a lovely Christmas and hope everyone Els has had who celebrates Christmas has had a good Christmas and for those who haven't I just hope you've had a good chill week hopefully work hasn't been busy with many of um many people celebrating Christmas so yeah anyone who um doesn't celebrate I hope you've also enjoyed your time hi mon how are you doing how you process data so there's one called Lambda one called Kappa architecture these are the two big like comp eting they're like the Pepsi and Coke or the you know uh kind of competition uh different ways of doing things uh and let's go over each one I have I like just to preface this like I have I've almost exclusively worked with Lambda architecture throughout my career so Capa architecture is mostly from Reading uh but I have a pretty good idea about both of them Lambda architecture so imagine you have a pipeline that you want to optimize and what it does is you are so you're creating a pipeline that is like a batch pipeline but you want optimize it for latency um and so you create a separate pipeline that's a streaming pipeline that like lands earlier and is is the data is ready earlier because it's being processed in real time and so how Lambda works is that you just have both you double the code base you have a batch Pipeline and a streaming pipeline that both write the same data and the main reason for that is the batch pipeline is there as kind of like a backup if the streaming pipeline fails that's or the streaming pipeline has problems or issues or um the correctness is a little bit weird right um so the pain here is uh what's my plan for the next year about YouTube content SL my work career so now that so so this year was all about getting more comfortable on the camera it was all get about getting more comfortable coding in front of people and I feel like I've got to a point now where I'm pretty happy with coding inut of people I think this this um this this boot camps actually really helped with that as well because I've been live streaming it so that's been really good um but so for next year it's all about building as much technical content as it can in areas that people are interested in so I've been doing some questions today for example on like what do people want to see so my background is I am very very well versed in aour that is why I got hired by Microsoft I got hired for my knowledge in aour and um so I know that that I need to tie in more into that whilst also making the content that I'm focusing on not like vendor locking if that makes sense so whilst I want to share my knowledge on your a lot of the concepts that I am going to be teaching are going to be things that can help with other areas so I've been asking about the different concepts that people are interested in and I think I'm getting the idea that a lot of people want to know more about how to work with the J devops how to work with G GI Hub actions how to work with um how to build cicd pipelines so that's one area that I'll probably focus on um and then the second one was working with a aour and like data in um data in aure so I'll do a little bit more research on what people are interested in that area so things that I've done like before is I've worked with the data breaks I've worked with um like Microsoft fabric I've worked with uh I've worked with uh like aim and open Ai and connecting to open AI via apim and like using policies to like lock that down so that's an area of interest that I can I can go into there's a lot of different things and there's a lot of different scope that I can go into depending on um where where people are interested in so I just need to I just need to do a little bit more research figure out what it is that people are really really like wanting what what the gaps are so I'm just going to keep asking a few more questions try and dig into that I've definitely got some good ideas up and coming there's been a few things like a few CI CD things that I've done recently where I've where I've felt that that would be really good to do a video on like it's taken me a bit of time to figure it out and I'd be like okay that's a really good thing to do a video on so for example recently I did something with code ql which is a static code analysis Library um it does SK it does static code analysis for various different um different languages such as C and Java and I and and one of the things I needed to do was I needed to get it working both in azour devops but also locally and getting it working locally was a very very like fun challenge because one I had to get it working in a Dev container two I had to get it working um working like with with the same tools that were being used by GitHub Advance Security and that was a journey and it was really really fun to do so things like that is is what I'm hoping to start like sharing a bit more of the the things that I do dayto day that I feel like I'm quite knowledgeable on Jade you are the most lovely that's really nice thank you I really appreciate that double code right that's the big pain of the Lambda architecture whereas uh you get a little bit niceness from like the data quality perspective uh and stuff like that but you the double code base is like one of the things that is a massive massive pain for when you're creating this stuff um so that's essentially Lambda architecture like most companies Facebook Netflix Airbnb they like any of these like Master data sets that they want to land as quickly as possible but they want to be as correct as possible they use Lambda architecture to um solve that problem okay so let's just talk about Capa architecture real quick uh Capa architecture is um where they say you don't need both you just you can just use streaming so like Flink for example you can actually flip Flink from streaming mode to batch mode so you can actually have Flink just read in data and and do the flip itself so then you can have one code base and Flink can just handle the batch job as well uh and that's good um it has some problems when uh like for example backfilling a lot of history is hard because like Flink is still mostly geared towards reading from Kafka and like if you're reading in like many many many days from Kafka like the back fill is just super painful because you have to read in everything like in a row because that's how Kafka works is all the data is in a line and so that is one of the things that is very painful about uh Kappa architecture because those kind of backfills that's why Lambda architecture exists right is to kind of minimize that problem uh one of the things that's nice though and this is something that Netflix is working on is uh iceberg is actually changing that right so it makes it so Flink uh can dump data to Iceberg and then Iceberg you can have a nice partitioned table and so then Flink if you need a backfill with Flink you can back fill days at a time right you don't have to back fill the whole line because cof any plans I'm making my own SE net courses on udemy or on other platforms uh not entirely sure about net like so I would like to think that I have a good amount of knowledge on donet but I don't actively use it all the time like I kind of it's It's like a sporadic thing like in my day-to-day role so like sometimes I use it sometimes I work with other languages I would say that what I work with continuously is um is aure uh is cicd pipelines terraform bicep uh and then the the main like development languages that I work with are python react C so and Java and and and it changes depending on who we're working with and and what the product needs are um and so I feel like I have a lot more like exposure to the other things and I think that they are there's more there's more gaps in those areas whereas I think C and net is very very well covered and there's a lot of great great develop like developers who already do good courses on that so I I kind of want to look at where the gaps are in the market and try and fill fill gaps where I think there are I think if I did do a c course it would be on the testing side like on um how to do good unit testing how to um create good like mocks or stubs or like fakes because I think that's a really big gap in the market I don't see a lot of good cost content on that um in fact a lot of the cuse content is pretty terrible on that so if I was going to do anything it would be on the testing side hka just has like one line of data right whereas Iceberg can have partition data just like uh Hive and all the other kind of data Lake kind of architectures so that's what's really cool about iceberg is Iceberg allows you to do these appends so you can append and create and add more data to your um pipelines as you go whereas Hive you couldn't do that and that's like a very big win for Iceberg and that's like what probably my favorite thing about iceberg is that append because it allows the the streaming world and the batch world to be mostly married and like at least from what I've been reading like Netflix is is starting to be more open-minded about moving from Lambda architecture to Kappa architecture at least for uh a certain percentage of their jobs because of how iceberg is unlocking and minimizing some of the pain that comes with uh like with Kaa architecture and like having to Source all your data from Kafka so that's cap architecture so remember l architecture Capa architecture they're both got out here like for me like the company the poster child of Kappa architecture is Uber because they have uh like they're essentially a 100% realtime company which makes sense right because they have to monitor where all their cars are at at all times and it's like they can't be delayed so they're like uber is a very streaming first company and like it's very in their nature to be streaming first and so uh definitely uh it's which architecture fits which company does depend on like their business model and what they're working on I'm not saying one is better than the other I just think that it should be good for you be aware of both of these and like this question shows up in data architecture interviews all the time I've been asked this question in in data architecture interviews at like every single company I've interviewed at since I became a staff engineer so uh yeah so definitely uh learn more about these two architectures because they are very important so we're going to shift ears here go back to uh so that was just a little aside about architectures that I think is important because it talks a lot about streaming uh we're going to shift gears here to be talking more specifically about Flink so Flink has UD uh udfs are for like custom Transformations Integrations we used the UDF on Tuesday and that UDF what it did was it called an API and then it enriched our data set and we added some new columns that's great um so python udfs are going to be not as performant as uh like Java or Scola udfs and the main reason for that is because they have to have a separate process which is like really gnarly so you have like the the regular Flink process that's running and crunch crunching data and then uh when you hit that UDF call it actually has to push it out of java and push it into python python executes it and then it pushes it back into Java and so it has to I don't know if you'all ever used like a patchy arrow and it uses like essentially it serializes things into do a patchy Arrow moves it over into python calls a separate python process python runs it returns it its result moves it back into arrow and passes it back to Java whereas if you are using pure Java then you don't have to do any of that right and then you don't have like that because in that case you end up having two extra serial ation steps that you two extra serialize and deserialize steps that you wouldn't normally have and so that's where uh you want to be careful this is also uh this uh guidance we're going to talk again about um next week this is generally speaking also uh good practice and guidance uh when talking about Apache spark as well it has the same I hope we cover udfs a little bit more in further lectures just because that's something that one of my colleagues was doing for and and I was like I would I didn't really get to do much of it but but what he did was really cool so hopefully hopefully it covers this a little bit more udfs same issue especially with python udfs specifically because you have this like separate process so let's talk a little bit more about windwing so this is going to get really uh kind of complicated and messy so um there is essentially two types of Windows in Flink you have a count window and then you have time driven window those are going to be I did I did don't worry um it's it's one one of the questions I do wonder is what is it about bit net specifically that you find interesting of other languages what is it that makes you want to be a net specific Dev the two different ways that you can define a window and Flink and you can think of this as kind of like groupby or like a window function in SQL it's kind of that but different so we'll we'll go over a little bit more so when it's a data driven window how that works is like you open a window and then you say this window stays open until I see n number of events um and then we have these time driven Windows tumbling sliding and session and we're going to go over each one of those like in detail and so that y'all will be more aware of like what's going on there so first off let's go with the count window count only have and so that's where uh you want to be careful this is also uh this uh guidance we're going to talk again about um next week this this is generally speaking also uh good practice and guidance uh when talking about Apache spark as well it has the same issue especially with python UDF specifically because you have this like separate process so let's talk a little bit more about window so this is going to get really uh kind of complicated and messy so um there is essentially two types of Windows in Flink you have a count window and then you have time driven Windows those are going to be the two different ways that you can define a window and Flink and you can think of this as kind of like groupby or like a window function in SQL it's kind of like that but different so we'll we'll go over a little bit more so when it's a data driven window how that works is like you open a window and then you say this window stays open until I see n number of events um and then we have these time driven Windows tumbling sliding and session and we're going to go over each one of those like in detail and so that y'all will be more aware of like what's going on there so first off let's go with the count window count window is very straightforward so essentially how count windows work is they open on the first event and then they close on when n number of events happen keeping in mind that you can key here where like you can say like per user so it's like a window per user kind of like um you know in the window functions in SQL you have Partition by uh in Flink you get key by and uh that's going to be so you can have your first event per user and one of the things that's another important part about these windows especially these count Windows is that uh the number of events may never come right say you're like I want 50 events to come and then someone does three and then they never do the other 47 and it's like okay that window doesn't stay open forever so you have you also have to then uh you want to specify a timeout like might be 10 minutes or an hour or some sort like whatever timeout makes sense for your use case and then that will close the window even if the the number of events has not happened so these count windows are very powerful for funnel analytics because funnel analytics often times have a fixed number of events like for example like uh yeah to be fair I should I should do um a YouTube post on that really cuz not a YouTube post a LinkedIn post cuz I think I think for stuff like that you possibly don't need a full video on it but like a LinkedIn post would be quite useful um I'll definitely I I can definitely put something up up together uh I will set a reminder on my phone actually because that's the best thing for me to do cool I shall up that I on here but honestly I have so many good donet resources who people who I I actually know quite a lot of people who do have really good doet knowledge so that that that's not a problem um I'm connected with so many of them on LinkedIn um like you got um f and um Milan and and that they're the two that I think are like really stand out to me but there's so many this m price as well like the notifications funnel at Facebook was generated sent delivered opened clicked uh Downstream action so like I know seven something like that so then like this can be this count can also be a distinct count so if they like click twice or open twice or whatever like that's fine and they can do that and that is not going to be an issue so that's kind of the idea is how you um how you can use these to kind of measure funnels because funnels usually have a fixed number of events and then you also want to a timeout of like well I don't want them to wait around forever it's like they click on a notification I'm not going to wait you know many days to see if they actually end up following through so like you have to that's where the timeout is very useful when you're kind of specifying windows in Flink okay so now let's talk about tumbling Windows tumbling windows are really uh are very interesting these ones are very uh like these tumbling windows are the closest comparison with uh data in batch so a lot of times when you're in the batch world you have like hourly data or daily data and those have like a window and that window is like from Midnight UTC to midnight UTC the next day right and it's like 24 hours or it's from like 1 p.m. to 2 p.m. and it's like fixed it's like from point A to point B and then everyone in that window is going to be in that time right and that will be the the window thank you um yeah I try it's it's it's it's definitely hard when you've got a full-time job and then you're doing um you're doing a lot of different things so content creation learning a car B um and then I've got like my family as well like at home so there's a lot of different things that go on um but I I am trying and um and I I will I will I will prioritize that so um thank you for reminding me that you're looking for and so that is like this one is nice because it has it's just very obviously the same as uh here it's very obviously the same as like what you would expect from uh like batch so tumbling windows and batch are going to be the most similar and you can obviously chunk data this way and a lot of times this is what people do is they make these tumbling windows and then they dump it to hourly data and then you have data that's already processed hourly and then you use batch uh at once it's at hourly and then batch picks up everything else after that that that's a very common pattern at Facebook at least so um that's kind of the idea here this is the most simple type of window we're going to cover this in the lab today as well this kind of tum tumbling window it's uh one of the types of Windows that I think is interesting in flick let's go to the next one so um then you have sliding windows so sliding Windows uh usually have a fixed width but they can be like overlapping right so imagine if you have um a fixed width window of an hour you could have one that's like from 1 to two but you could also have one from 1:30 to 2:30 and both of those windows are an hour and they're both valid right and so and you'll see here how like you get these kind of like sliding overlapping windows and you get a lot of very interesting patterns this way and you get uh obviously you get more duplicates this way as well like because you see how like uh like these records here are in window one and window two so you have to understand like how these sliding Windows like how to manage them Downstream so that you're not like double counting or like what you're looking for so a lot of times that's the whole point here though is that like you you're not looking you're not trying to aggregate all of these windows together you're trying to find the window that has the most data and it's like like if you pick one window there's no d duplicates right because then you're fine you just don't want to look at all of it in aggregate because if you look at all of it in aggregate you're going to have like twice as much data because all like pretty much there's going to be 50% overlap in each window so um one of the things I think about here right is one of the things that this can do though is it can find your peak time because if you have like a window that's an hour long but it's like shifted every 30 minutes then you can know like oh maybe I actually my my Peak usage is like 12:30 to 130 as opposed to like if you looked at 12:1 and 1 to2 those might be kind of flattened out because it's like the second half of that first interval and the first half of that second interval are really high but then the other halves are kind of low so then you can like there's like a spike that's like that crosses the windows and so that could be a it could be very useful for finding like those types of peak uses right um it's also good for like uh when you're crossing the mid the midnight boundary uh this is a this is a very interesting uh question that we had at Facebook when we were talking about counting daily active users because it's like if you have a user that starts their session at 11:58 at night like 1158 U mid or 2 minutes before midnight midnight and then it goes to 2 minutes after midnight so it's like a 4minute session and uh do they count as daily active on both days like because it's one session right and it's like it's only really one continuous use and it's like and it's not very long on either side so it's like is that actually daily active for for both days and uh that's like one of those edge cases around midnight that can happen and uh this sliding window was useful because one of the things it did was it showed us like just how many people we were like essentially double counting um when we picked that time and then we also did this crazy analysis where we were like okay what if we used a different time zone what if we didn't use UTC and we picked like a different time zone that was more centered around a different places midnight and we showed that like you can actually change the number of daily active users by like an upward of like seven or eight% by just like changing the like the 24-hour period that you pick because of that's because of this fact of people who have these like short sessions that are overlapping with UTC midnight so um that's one of the things that these sliding windows can be very powerful at like finding is they can find those kind of uh situations for you and so um anyways sliding windows are great uh I they're kind of Niche like how are you planning hold the New Year night um so I don't have any plans to be fair like I'm pretty boring I haven't had any plans for New Year's Eve for like a few years now um I'll probably I've got some Las that I made a couple days ago so that's in the fridge ready to be cooked properly and then we'll have that and then probably have a Bailey some chocolates and in in the UK um they they have this like fireworks display in in London but they ad they put it on the TV so I'll probably watch that I don't know if they do fireworks anymore I think now they've started doing something with drones so like I'm pretty sure last year they did something with drones instead um to make it a bit more environmentally friendly but it was still pretty cool so I'll probably just watch that when it gets to midnight yeah I'm pretty boring I would have I would I would probably stream as well if I could but um but but I can't because because um I'm spending time with with family but yeah and and to be fair like I doubt anyone would want to watch me they'd probably just want to get drunk so um so yeah but yeah I'm pretty boring I'm not I've not got any plans I used to go like when I was younger I used to go like to to to like to a club or something but i' I've I'm like a 50y old man like I I I found at least in my experience with link I um tumbling windows are what about you actually have you got any nice are going to be a lot more commonly used than sliding Windows sliding Windows have like a very like specific uh use case like it's it's not as much for like a general purpose because of this weird overlap problem that's like why it's like why are we why are we doing this to ourselves right it's more for like a very specific analytical use case as opposed to like building out Master data so um anyways that's slight Windows sliding windows are pretty cool all right then you have session Windows session windows are um this is the fix length one oh I hold up I messed up this is I need to update this slide this slides the slides off um but um so session based windows are different so um how session based windows work is they are based like they're user specific they're not like a fixed window so it's like you can imagine when you sign in uh that and you have your first event that's the start of the window and then the window will last until there's a big enough Gap so there's like a gap definition that you need to specify usually maybe it's 5 10 20 minutes or something like that where there's like a gap between when you have no data and like when you have data and so that is going to be the big thing that you want to look at with sessions this is session sessionization is something that y'all are going to be working on in the homework and it's one of the more powerful ones that I've noticed uh that will because this is the one that can really help you determine like how users are behaving because you can see like when they first start using the app and when they end using the app and so you can get a very clear picture of like how users are using your app in real time because you can get the beginning and end of their session so that's essentially how sessions work and uh that's configurable as well maybe it's one minute or however how however much of a gap You're Expecting from your users when they are um navigating the website but that's kind of the idea okay we're going to cover this one last topic that I think is important uh and we we're going to do a little bit of this in the lab uh it's kind of hard to emulate this stuff in real life uh because of the volumes and like I don't have like the right uh setup for this yet but um so essentially you know you have two ways to deal with out of ordered or late arriving events and the way it kind of works is like if it's a little bit late then waterm marking is going to be good if it's late on the order of a couple seconds usually Watermark is going to be great if it's late on the order of minutes then allowed lateness is going to be great uh this essentially is like if you have a really it all depends on like how late you are it's very similar to like uh like you know when you have a zoom call with someone and like usually if you're like less than 5 minutes late you just don't say anything and you go to the zoom call and like you act as if you're on time but if you're like more than 5 minutes late like you usually want to text them like yo I'm a little bit late and then so that they aren't just sitting around like what the hell's going on so it's similar like Flink has the same sort of idea where like you would think of like the watermark here would be like okay anything as long as they show up in the next five minutes like we're going to count that as like in order but then if it's over five minutes then we're going to count that as late and that's where you have this like allowed lateness which is like another way to do things but one of the things that's rough about the allowed lateness is that if you end up getting that really late data uh it will actually reprocess and it will it will either generate or merge uh data that might have already been flushed because that data might have already the window might have already closed so it might end up reading what was what was in the window that closed and then bringing in this new event and then closing it again and you might get another window like you might get a second record uh or you'll get an updated record like and that that's like where this allow lat is can be really funky and like how it like the actual Behavior behind what it does and like uh it usually does the merge but sometimes it does gener it adds another record so that's the whole thing that's where like for me at least when I've been working with Flink I'm all that water Maring and I usually just don't let like if the record is 5 minutes late the way I I look at it is that like it's similar to like my zoom call stuff where like most of the time if I have a one-on-one Zoom call with someone and they're more than five minutes late I assume that they're not coming and uh and that's like why I have most of the time I have allow lateness set to zero so I'm like whatever if it's super late I don't care but if it's within the watermark that's fine that's uh my perspective on that but obviously it depends on the use case because some use cases like you need to capture 100% of the data even if it is really late so uh that is uh obviously not just don't take that advice for every single streaming oh I didn't realize we moved on to the lab to be oh wait pipeline but probably for most congrats on getting to the end of streaming data pipelines day two lecture if you're taking this class for credit make sure to switch over to the next tab so that you can get credit for the lab and we're almost done with this one guys so cool let's go to the um lessons do you like drinking alcohol or only on the holidays um to be fair I don't really like drinking alcohol that much however I do drink it in Social settings so it's more like a social anxiety thing which I am trying I'm debating whether or not I should stop to be fair because I don't think it's a very healthy habit um yeah I don't think it's a good habit to have so so I think I should try and stop it to be fair it's not it's not good for you so I would like to I would like to try and cut it as much as I can really congrats on getting to the end of streaming data pipelines day two lecture if you're taking this class for credit make sure to switch over to the next tab so that you can get credit for the lab we're almost done with this one guys so with Flink we got uh you saw in like the job folder here we had this start job which we actually still want to work with uh because of the fact that it dumps to Kafka right it because we have this right to Kafka and then this right here will allow us to have a job that will essentially we can have another job that reads from this right and that will make it so that we can aggregate on other things too like we can maybe aggregate along like country as well so we can kind of see uh we can build some kind of tumbling Windows around that kind of stuff so the whole idea here is we will end up using that name right what this process events Kafka that's going to be the name that we're going to be working with but one of the things that we need to do though is we need to create another entry point because I don't know if you'll y'all remember like um in the uh oh wait no wrong one so you know how in here here we have the or in is it possible to see Jade using an iPhone or a Macbook in this life um probably not the only time I would ever probably use it is if the is if the um like if I had to do it for work like if I was told okay you need to start using a Macbook because you have a specific customer who wants you to use a Macbook then I'd have to Su it up and use a Macbook but I have tried to use a Macbook before and every time I've tried to use it I just can't use it because I'm just not used to it so I'm I'm one of those people just like stick to the tools that you're used to rather than try and just learn something for the sake of learning it unless like there's a b benefit like of learning it and I don't see there been a benefit of learning um how to work with Max at the moment maybe in the future we'll see but um I'm pretty comfortable with both windows and Linux based systems so if I was going to use anything different to Windows I'd switch to to to Linux um in the make file we have a make job which is like this big old thing here we need to have like another line here and we're going to call this uh we'll just call this like window job like window underscore job and then uh change this to uh we'll just call it aggregation job because that's the name of the file and I don't want to rename the file so aggregation job so I'm going to uh I'm G send this in the in the chat but obviously this needed to be like actually in the in the file there we go put this in the chat here so y'all can uh put that in the make file just so uh we can have that kind of up and running so obviously first thing I want to do here is I want to say make up so we can um get all of our uh okay so there we go we have uh Flink is now running we have all that up and going and like let's go look at this AG aggregation job real quick because I think that that's something that is going to be important um okay so you saw how uh this name here needs to be different this uh create processed Event Source this needs to match the if we go back to start job it's called process events kofka that's what we're going to actually call it so want to change this name here to process events Kafka in the aggregation jump so one of the things that this allows us to do is Let's uh kind of go step by step through this uh aggregation job and I think that will make sense as to like what is going on here so long oh that's great I'm really glad that you have been inspired to post more in 2025 I think that is a um great goal to have I literally set up with that goal in 2023 in in July 2023 I said I'm just going to start posting daily and see where it takes me and yeah um and then yeah here we are now Jade white Edge and not Chrome because they're pretty much the exact same like they're pretty much the exact same they work pretty much the same so I'm I'm pretty happy with how Edge works yeah they work the same to me um there I the there there are some things I cannot stand so I I will always use Google chome I will always use Google over a bing I just don't think Bing's very good at beinging search results back yet it's it's far less like um it's it's not as good it's just not as good you know and then my to pixel of course you know I'm not going to use um a Microsoft phone and then they're the main ones uh and and and um clipchamp which is Microsoft's video editing product is terrible it's like the worst like editing software for videos I I really shouldn't say this I'll probably get in trouble for saying this I I don't me to slander like I just think like for example you can edit videos offline with it like you have to be connected to the internet and it's very very limited in the functionality that you can actually do with it so I'm like just why do they get rid of like Windows Movie Maker for this for that like it's like come on like so use Da Vinci um Da Vinci was off so yeah probably not um probably unless like um unless I was given it for like work purposes yeah probably not it's hard to say because I don't know like what the future's going to hold for for for Microsoft phones I'd hate to have preconceived notions of oh Microsoft phones suck so I'm never going to use them but I like I like the um ecosystem of both pixel and Android and Samsung phones so I'd probably say that it'd be hard to to to um to switch from those first time here C Flink last live was motivating Oh I'm great um glad that um glad it was motivating uh we have a log aggregation this is going to be our job you see at the bottom here we have the main looking great so in this case we uh again set up our execution environment we say we're in streaming mode and then we create our table environment this is very like standard Flink stuff that you're going to see in like every single job then we go ahead and create our source table which is going to be that process events Kafka topic which is actually dumped to from that other job so what we want to do is on this screen we want to say make job and I don't know what's going to happen if all of us are running these jobs but we're just going to go with it so uh we have this make job which is going to have those it's going to have those two jobs running and what you want to do is you want to keep this going because this is going to dump the geocoded data into kofka into that new Kafka quebe and then what you want to do is open a new tab all right so um right already done the make why is this so funky see it's like it's like it didn't update like it's like okay whatever it's like it's like sad and I don't got my coloring but okay so anyways in here right we could say make aggregation job and it's probably not going to work but it might work and then we'll have our third job running right okay so it's breaking right let's see what it's saying here a group window expects a Time attribute for grouping in a stream environment o so let's uh let's see what's going on there so it's saying that like a Time attribute is the problem here okay so let's go look at our things here right so one of the things that happened here is you'll see uh we have our tumble over and then we have event timestamp and I think in our table yeah see we have event Tim stamp here but it's a varar right and that's uh that's like a problem because that's not what we want we so what we need to do is we need to add another column here uh which we can do it's pretty easy uh like it's actually in the other job we need to do that so what we want to do is um where is it there I have a perfect example here yeah this one so we want to get this uh pattern here because I think we have the because we end up writing it out and uh oh yeah now we want this pattern this is going to be the pattern that we use so but that's fine we can copy this one and just get rid of the Z and the T so go back to the aggregation job okay we have this pattern here but we actually don't have the T and we don't have the Z because this is the pattern that we actually write out I think that that matches that matches the this um format right oh no there's just no SSS though you know we have just the lower we just have the little s's not the uppercase s's okay so I think those well what I'm gonna do I'm just going to copy this it's like make it be exactly the same okay now we know for sure it's exactly the same and so what we want to do here is we want to make a new field here we're going to call this maybe like window time stamp and then uh want to do it the same way they do it in this other job so there like two time stamp right two time stamp so this one you want to grab this say as that so window time stamp as two time stamp and then this is not event time this is event time stamp because like it's the same as this one and then that will use the pattern that we shifted it to when we wrote it out in the Pro in the other in the other job that is currently running so um that looks right that looks good to me um so one of the things here is that should fix our problem for the rest of this code so one of the big differences here though is now you'll see down here um where we're defining the window see okay so we have our source which is great and then we have our aggregated table and our aggregated sync which are working fine but then what we what we have here you see this tumble we have this tumble over so this is going to be our our our we're going to have a window that is opened every five minutes and this is now like this is where we want to update this to window timestamp instead of event timestamp and that should give us um what we're looking for as um kind of the idea of of like the so this is going to give us our window and this is where like you see we actually get group by here and so this is where we're going to get a little bit of a key right so we're going to have windows that are based on both host and based on like time so we have like five minute chunks so what I want to do here is I want to say make up because we need to uh essentially uh start them up and oh wait that did not okay I think we actually have to kill the other job real quick because uh we need to do make down because we need to get the the new the new updates into the into the aggregation job right so make down make up so we can get the actual job running here so there we go now that's running and I want to go back to the other tab here and say make job because that should give us the um the actual data here there's probably more steps that we're going to be missing here to get this aggregation actually up and running but so now that's working that that's the other job but then we can say make aggregation job over here and so this is hopefully going to work now okay a group by window the stream environment isn't it giving it's the same air why why is it mad now is there like another ah it's because it's here you got to do it in both because this is it's the same problem here right because we have another job here that has the same problem so we need to change this to window time stamp and like obviously like if you it's like if Flink is not syntactically correct it's going to make that mistake right and we'll go we'll go over both of these windows here in just one second I just want to get this job up and running so we got to do the same like make down make up like Madness right to get the um the new the new code there so uh then we can do make jobs Z oh oh it just kills it if you do make down cool great and then now we now here we can say make aggregation job uh so I'm just gonna just want to have a look at something I want to see whether or not um I can view the custa um I can I'm definitely receiving records for example so I've got 77 records there and if I go into this one I've got 82 one there so it's definitely getting the records um I'm pretty sure this one is um I'm just exploring um the um C okay okay this one's the cafa say processed events aggregated okay to this one process events aggregated Source okay that's fine hi caros how you \[Music\] doing oh is it g to work it might work how is it still the same bug because now now this is like now this is right because now we have our window Tim stamp and that seems right and we have like everything here that seems to be what we're looking for because then we have event hour window Tim stamp and then this is we shifted it up here right and we have our two time stamp pattern here that seems right to me though because now we have our group which is our table name which we are doing right right here so okay why is it upset um like I want to look back at this so we say valid validation error a group window expects a Time attribute for grouping in a stream environment does it have the actual line oh because this is all in Java right and it's like no we can't do it okay so that means it's actually fail it's it's getting here and then it's not um uh working the way you would expect though so let's um let's kind of go through this a little bit and see like what it could possibly be here because we have the window time stamp here what about this aggregated table there's something here that is okay that's time stamp that's fine and that's event hour which should be fine and then um interesting interesting that's so weird because I I definitely had this working before so we have the window time stamp which is going to be right I mean unless it's like oh that's really awesome um I have to go to your Channel I'm going to go to your channel and I will watch that later um but yeah that's that's cool how you how how did you feel like about um about doing your first video I'll even subscribe actually um how did you feel about doing your first video did you did you enjoy it um how did you find like the videoing and editing process did you find it fun one thing I find in really like disorientating about this lecture and lab is I like the I like the code to be in the format of the lab like like if if a la starts in a particular format I like the lab to start in that format and I also like to have like the so I like to have the start code and I like to have the end code um so that I can if I want to follow along the lab whereas this is really hard to follow along the lab because the code in the like lab is different to the code that I've got locally so I'm find that a little bit disorientating I need to work more on the audio same man same that's for me that was the hardest thing and like one of the things that I found really helpful I I'll quickly show you now actually is if you go into so I've got my word mic which is my word mic video mic but I've also got uh if I go to filters I've added a couple of filters as well so I've got noise suppression which is just good quality Mark CPU usage I've also got a gain which um detects any gain above a certain level and I have this like these are just defaults really that I've set but that's really helped with the reducing any background noise so like for example I think the other day I had like the um the washing machine on and that really helped so this this really just helped with getting rid of any background noise but I just I just Googled like how to reduce the noise like background noise and followed What it told me I'm not an audio person but yeah okay cool that's good sounds like you are on it like not processing it right if it's not this actual like window Tim stamp here like if this is not like if this is not doing the right thing for some reason if this is like parsing it wrong then that could be what's going on and that like it's not actually uh processing that the right way but that seems right to me because because that's how we are dumping it out over here right because I think that that's the problem here is that the date format here is not giving us the time stamp that we're expecting it to give us and that's like why it's like yelling at us right now so anyways um I want to I want to okay there is uh one of the things I want to try here is if we change this to latest just change this to latest real quick and try it with that because that will like essentially have it show up as um uh like we'll only get the latest data and maybe there's like weird data Kafka that isn't parsing and that's that's the reason and so now we'll only get the the latest data mat might see if this if this barfs as well I'll be surprised a a a group window expects a Time attribute for grouping in a stream environment and then this is this has got to be that line it's got to be down here it's got to be like literally right here that's that's the only line that even makes sense we have uh I want to go over the code and I know that there's just like a small like Tim stamp like casting problem here that I like I will figure out but like here's kind of the idea of how tumbling works right so you define a window which in this case we're going to do every five minutes which but this can be every five minutes based on processing time or hi Nisha how you doing I hope you're doing well are just saying about how like I find like the um the lack of con consistency between the lab what I've got locally versus the lab on here quite quite hard like I much prefer having the before and after so I can follow along based on the event time right and in this case we're trying to do the event time which usually is what you want to process on you don't want to process on processing time um and then what we're going to do is we're going to group those windows together in this case we're going to group on the window and the host in this case we have our host here and then uh from there we have our uh event hour host numb hits all that stuff and then uh this is going to essentially do an aggregation every five minutes or hopefully that would be that ideally that's what's going to happen here so um what I want to try I want to just try one thing real quick I want to try using the other um uh like the other pattern here because my I still I still wholeheartedly believe like what is going on here is this pattern here is just not like it's maybe we want to try two two more tries here on using this pattern and then also the pattern with the with the seconds so we'll try the second ones first and then we'll try the one with the Z and everything in it after that and then if both of those don't work then I have failed y'all and like we won't be able to like there's going to be like some other crazy thing going on here but this should uh with the seconds is it going to is it going to give it to us with the with the with the Millies okay with the Millies it still has the same problem but like what about if we change it to the this the exact same uh one that we had uh the exact same pattern that we have from um the other job okay so actually no because it's failing without even reading in data so it's saying it's it's not like this time stamp is not right it's like oh it's saying expects a Time attribute for a job right and so one thing I find um interesting is how come he um how come he like has to re start every time like the make up and the make down because his job's not his job isn't running because it's failing so I'm not I'm not sure maybe it's just a safety thing but I don't think he needs to do that I'm not I'm not convinced he needs to do that um but yeah uh all right thanks I am good I was not expecting you to come back so early from your holidays I was expecting holidays until the 2 of Jan yeah well I've got a lot of things I need to get through I need to get through this car so so that's why um I thought I'd come back a little bit early I wonder if uh it's like it's not quite it's not like time this is like a time like maybe there's difference between time time yeah Kyle do you have any thoughts on this curious like o Dude that dude if that's what it is like oh man that that that that actually freaking probably is what it is dude let's just throw in the watermark real quick uh we'll call this uh we're GNA for we're gonna call this window Tim stamp and then as we GNA say window Tim stamp dude that's dude that's dude that's totally what it is that's totally what it is and then this is going to like my this is going to just be the that right it's going to be that regular one that matches the other pattern you're totally right 100% that's totally what it is it's like totally forgot thek dude if this just works now like it probably will just work and then then I'll feel better I oh oh it's not failing it's it's definitely getting F Yeah Boy let's go thank you Kyle you saved my life right there that was this is almost a demo fail okay so um let's go look at like what is going on in uh in Flink here because we're going to have a lot more jobs running right so um we also oh yeah we need to run the other job right so we need to like make job over here this might actually not let us do all four um so I will uh I'm going to send yall that Watermark definition here in just one second too I just want to see if there we go so now I think the problem here is that we don't all right I don't know for when you are thinking about distractions you mean like notifications emails messages from people on teams that sort of thing or do you mean like actual physical distractions like in the home and and if you remote work or like in the office for example we don't have slots for four yeah we only have slots for three but okay but but so one of the things I want to walk through here real quick in like these window jobs is that they are more complicated so you'll see in this case we actually end up having two there's like two records here right you have the um or there's like two steps right because you have the um like you have the the first step of like reading in the data and then you have the group window aggregate step which ends up so this is where you this is where watermarks are useful this is where like uh because marks like if you are getting data and you're just reading and writing data as like rows and you're not doing any grouping then the ordering doesn't matter that much right but now you'll see we have a watermark and Kyle's totally right in that like we needed to have that Watermark right that was exactly why we couldn't window because you can't window without a watermark in Flink so what we need to do is we need to go and create that table real quick so we can say create table processed events aggregated and in this case uh this table has three columns I think there's uh oh there's two tables here yeah there's processed events aggregated and processed events aggregated source so these are two separate tables uh but like you can essentially just create them from the ddls and Flink uh I will pass this to y'all as well so that y'all can have this but so this is going to be our first one where we have our processed events aggregated and then we also have one for Source because we wanted to also look at um who is winning based on like not just uh like uh like who's winning from Twitter who's winning from which place right so so say create table this is processed events aggregated source this has uh these three columns in it I want to run this game so now these tables should be already processing though like like once you make them like it should just immediately uh be uh pumping out data right there we go so now you'll see we have uh essentially data coming in um every uh we have data for like every five minutes right essentially for all of these you'll see that a lot of this data is for like exactly and um Tech Creator which makes sense but we should be able to say where host equals we can say Zack wilson. Tech creator okay so I'm just going to go into here I don't think I have an events aggregated table though I didn't make one I don't have a processed event aggregate do I need to change do I need to create a table for um I'm not sure if I'm like missing something here but I'm struggling to see where this gets uploaded to here okay do I need to go here do I need to log into cafka like directly okay I willo because this should give us other data there we go because now you'll see this this's see this 1855 this okay so I don't remember him creating this table anywhere that's all like I see I remember him creating this process event table but I don't remember him creating the aggregation job table let me just I'll just create that now and make show see if if something actually gets populated okay so I've created that table now and and then what I'm going to do is I'm going to do makeup I'm going to do what he suggested yeah okay yeah there are definitely like I'm I can procrastinate with things that I find boring as well so you're not the only one is what I'm saying but there are definitely things I can I do that I I don't know like I've already said that I'm pretty sure like I've got like ADHD I'm getting I'm waiting for an assessment and the way that I make myself get through stuff is I give myself Rewards for doing it that's I do I give myself a w I'm like okay if you do this thing then then you can you can have that thing which um like like for for example like um when I get myself to to to clean up or or something like that I'm like okay if you do that then you get to relax and do this thing um and you just get to like relax for for an hour and and I don't know that that's a way of tricking my brain it's doing it this is going to be the one that really is the yeah but you don't know who I am on the day today um so you already see a small fraction of who I am as a person so that might be your opinion I'm pretty sure I don't have ASD based on who who I am and just going to empty this okay so it did it did start doing it it's just um I'm I'm going to kill it cuz it it gets it just gets really slow but um watch we'll see I'll just run the aggregation job for now I think and see how that goes but um yeah uh we'll see um I have um I have my ad I have my ASD assessment on the 7th of January actually so we'll see what happens in that one but like I said I'm pretty sure it's not that I'm pretty sure it is ADHD but I'll let a professional determine whether it's one or the other I haven't had a date for ADHD one yet um but I thinking I might as well do both based on comments from different people and having symptoms in both I guess you would say it doesn't feel like ASD though just because from what I've read like a lot of the symptoms for ASD are like from childhood and I don't fit into a lot of like like when I was filling out the form for like for both of them everything was like for for ADHD I was like yeah that makes sense like that that that like yes yes yes whereas when I was doing ASD there was a lot of things that I'm funny with but then like all of the childhood questions like I was just like none of these really relate to me so I don't know the one voting so then you'll see we have three just that now three three hits there so really we want to put an in here so we're want to say Zack Wilson we say lulu. Tech creator. and we want um Dutch engineer so Dutch just gonna do the job as well because it seems that we need both I think we need both unforunately it's going to be slow for a little bit e e for um so that's interesting yeah they don't do um any sort of brain scans in the UK they do entirely um like behavioral assessments so you have a assessment with you on your own and then you have an assessment with your informant who someone who has known you for a long time but yeah there's no there's no um brain like scanning or anything like that here you also like if you have someone in your family that has it as well it's easier to for the assessment I think but um but I don't have anyone who's been diagnosed with autism like in my immediate family I have people who have been diagnosed with um other conditions in my like not not my immediate family so I have like cousin who has been diagnosed with ADHD for example and I'm pretty sure my like I'm pretty sure like like my like one of my parents has it as well but um but no one's being diagnosed with it so okay so that looks like it's been I keep going on that one this is one engineer is Sarah so have D Dutch engineer. Tech creator. and so then we have our three here uh it's not as easy as just going to a dock um so if you want to get it done on NHS they require you to get referral from your GP you can't like just be like okay well I want um I want an ASD assessment you have to give me one you have to go to your general practitioner and they have to determine whether or not you have enough symptoms to be able to go to a specialist who can then diagnose you so you have to you generally have to fill in the um like aq10 and ASD like for for for for like this there's like diagnostic like checklist that you have to fill in and if you get a certain score then I think they then refer you to um then they refer you to um like a clinic that that you choose I do have a separate video on what what I did but um but like for example I asked for a referral like a year and a half ago and and it was May 2023 that I asked that I first asked for a assessment and they did they did an initial triage and they said you and then I didn't hear anything and then about I'm just going to cancel this job here I didn't hear anything for a year I rang back up and they said that they rejected it passed it back to the GP because they didn't have enough information the GP never bothered to get back to me asking for that information and so I had to go through the process again which was in like July this year so it's not like as easy as okay I can just go to the doctor and get get assessed you have to go through like the motions and then we should be able to run this and there should be more data than that there we go so you'll see here we have our data coming in and this is like for the ninth though so uh you'll see in our in Docker compose here in this Docker compose at the very bottom you'll see this task manager number of tast slots I'm going to bump this up to five and then I'm going to just we're going to do make down and then we'll essentially make up and then uh make it so that we don't have any jobs that are uh like failing because they don't have enough task slots say make aggregation job but like that's a great thing and we're about to find out if we need to increase parallelism or not and so uh this should work okay so if we go back to running here we should have four yeah we're good we're good because like you would you should see some like purple here but you don't you see how there's no purple like it's all running now it's all looking pretty good and so now I mean we should we should be seeing way more than this though like this is trying to like say that like freaking nobody has like nobody has clicked on my links like I don't believe that because like that doesn't make any sense to me at all like there's uh Because unless uh the data in it's still like chugging through all the day so you do have the ability to pay for a private assessment and that typically will make it quicker so it's about £2,000 to get like private assessment but the thing is if you get a private assessment they don't always class it as a valid assessment on like the general like the GPS don't necessarily class as a valid assessment so you could go through all of that and get the get get diagnosed and then when you go to GP they like that we're not we're not classifying that as an assessment um data which okay so if we go to the what processed events aggregated Source if we go into here this should be like pumping data right okay it is so one of the things you'll see here is like it actually does give out like data sets here right so you see it says like okay we've sent 500 records and we've received 200 records oh I bet it's because of the like the five minute marks like these like 5 minute marks is probably the thing because we have that window and it's like needing to open and close these windows but a lot of those windows should be done like a lot of those windows like like like that's it's still seems very strange to me that there's no what if we look at the source table processed aggregated events Source okay this is looking again though again we like we okay see there there's someone there is someone who's coming from Twitter like that's the t.co is doing right there like but uh this is still like seems wrong though this still seems like this is off by like a day like the the time is not like because it's not the ninth right it's actually the 10th so um that seems really weird like that might be like a latest thing but like I swear we are processing on each thing here we're only processing latest if we go to the aggregation drop I put in in here right there's just we use latest offset and then in the start job we okay we're doing earliest offset in the start job so that might mean that it's still pumping out new data in Kafka there should be way more records here than than eight there should be like because it's like this is like for every five minutes like there should be like like way more than just like that's so so strange to me that like uh because it's weird that it's like processing data but like it only has like this many records right and it's like and this must just be like spare traffic that like it's not actually okay what about the other job though because this other job should be um what this process events C this job oh ah there it is that's why this guy is struggling that's why this guy is definitely struggling this guy is not writing data right now that's what's going on I think you're totally right Kyle that like this guy is like he's um oh you're is based that's cool um my um I'm from east yorshire too where abouts in East yorshire are you based um there isn't in the UK there's no medication for a ASD um you can have like you can have medication for things that ASD can cause so for things like anxiety and depression but I I've already been diagnosed since I was 18 with anxiety so I can already get any the the medications needed for anxiety if I wanted to I don't I'm not actually on any anxiety medication at the moment but I used to be I used to be on um blatin and then I then switched to citran and culine so I've had the various different ones and I used to have propanol as well um but but yeah in the UK you don't really get any medications for ASD it's more like coping mechanisms and therapy I think that you can have access to yeah it's it's a bit it's it's not it's not the best but we'll see we'll see how um we'll see um how Cho take go like he has like he's kind of backed up right he's kind of backed up I wish I could like so this is like I think this is a great example of where like so what we should probably do here and this like this is an easy fix I feel like which will make it so we can at least get some new data from people who are coming in is so I'm going to stop the the start job and then um what I'm going to do is let's go back into that job and change this earliest offset to latest offset so that like we aren't like processing like everything right we just process the new data and that should that should make a pretty big difference oh wait I got to do the do the whole thing I wish that like it would just put it up there and I don't have to like Docker all this all the time but this should make it so that we have more of a like actual kind of data set that could pull in more like it will only pull in like new data as it comes in and then and then then we have to like have that like kind of those five minute sort of interval things but at least then we'll have data that's like it's not just like struggling but you're you're totally right that's exactly what was going on is that like it was uh not uh like it was totally not like it was struggling to read all the data because of the fact that I don't think that's because I'm so popular I think a part of it is because like a lot of y'all have been running the jobs right and so that hka que actually has a lot of duplicate data in it I want to see real quick that's actually like a uh that's a good um little thing that we can add to this lab because now okay all my jobs are up and running now right we got okay they're all up and running and so now this should uh to be fair though like I don't even know what l t is I don't think I think in in the US I know it especially that it's much easier to get access to different drugs whereas in the UK they're very like it's very regulated and it's very difficult to get like when I go to the US I can get like loads of different things like you wouldn't believe like how weak the cold and flu medication is in the UK compared to the US it's it's mad like but yeah um is it anxiety or OCD anxiety uh I would say it is I wouldn't say it's OCD anxiety I would say there's definitely traits but I also think that's another thing of you know why it's like why I think maybe it's ADHD like um rather than OCD because when I look at OCD like it's definitely not that like but like I definitely have like impulsive thoughts and impulsive um like like for example um this sounds really bad but like whenever I'm near like stairs that like have like a banister like oh you know what if you fall off like and you know you know just jump off and things like that even though like even though like it's not like I would never do that and I would never feel like I would do that but like that's the kind of just like your brain thinks about like just doing it like like oh my God like don't do it but like think about like what if you did that like oh my God there' be load of like G and stuff like that that's that's what it thinks about it's really weird but yeah I live in Market weon oh cool um I live in Cottingham so not too far from you I drive through Market waiting to go to The Bu station quite a lot when I go to the office yeah we definitely do not have a huge Tech scene do you work locally then have more upto dat dat right or it's just going to not have oh no it's cuz it hasn't been 5 minutes it hasn't been 5 minutes we have to wait till 7:40 and then uh then we will see because that that's when that wind the the tumbling window will close but I'm like in the meantime we can go see how like messed up kfka is like and see how much data y'all like dumped into kofka because I bet it's like I bet it's gnarly okay so here we are um here's kofka oh look at that what are we whoa this is um not what I'm looking at I want to look at process events cof okay it's saying that there's just like 25 mags in there not like obviously something Happ here with this iox right and I think it might be because I have just like a really like tiny cka cluster and that's what's do I need to for I'm just signing up with for all right I'm just um having to sign up to authenticator on mying I'm just gonna keep watching this what's going on here and that's why it can't like handle handle it right um because because Boot Camp events I'm guessing is probably going to be fine oh yeah but you can see that there is a step up here from like uh when I made that post so there is more data in the queue for sure for sure for sure I'm just trying to get it like to have so we have like Flink into Flink that's like the idea that's what I wanted to like really show in this class there we go there we go okay we're good we're good fam it's there we hit this we hit the five minute Mark so now we have that the 7:35 window closed and you'll see uh Sarah is beating me by one and then Lulu has 16 hits uh and so that's kind of the idea and you'll see if we put in like Source here then uh that the F minute window here closed as well and then you can kind of get the breakdown and you'll see like like Twitter hates me I literally have no votes from Twitter Twitter none not even one all my votes came from LinkedIn that's so sad Lulu's got some love some Twitter love and then someone someone uh someone went directly there too you see there's like null so like uh so TW Lulu is like uh like mostly LinkedIn 12 from LinkedIn three from Twitter and then one direct so in this kind of uh competition that we're doing right so this is working now this is working and um I'm glad it's working and we kind of discovered like what's going on and why this is happening that's where this like latest offset versus earliest offset does make a pretty big difference especially like when you're processing things CU like you might end up like trying to read in all the data sequentially and like if you're doing window on that that just becomes like even more gnarly right so um that's essentially what's going to happen here and we're going to build out some more like stuff here well one of the things I wanted to go back over here with uh oh wait not that one with in in the aggregation job so let's let's go back into the code now now that we actually see it working and I'll show you essentially like what is actually happening here right so we are reading our Source table we're doing a 5 minute window we're grouping on the window and the host and then we're essentially uh this is giving us our time and then we have the host and then the number of hits that's what is in here when we look at when we get a source here if we look at this table you'll see uh that's exactly what's going on here we have the time and the number of hits and obviously this does have that little bit of latency but you'll see like in in two minutes we're going to get more data and we'll have like more data for every 5 minute interval and that's like what's really powerful about this is that it just like automatically shows up it's pretty cool um and so uh and then the only difference between that one and this next one is we also group on referrer which is going to be like did they come from Twitter did they come from LinkedIn like where are they coming from and so it does this aggregation kind of in real time which is pretty cool like uh do we need to do this in real time that's a great question because sometimes aggregations are things that we don't maybe necessarily need to do in real time uh so this is kind of how tumble windows work and like we're going to be able to see as these windows close like how they kind of process over time which I think will be pretty cool to see one of the other things that we could do here that like would probably you saw how that choked right that the the last job kind of choked uh because of the fact that it um I one of the things that we can do and uh I want to try this actually I think this is something that I think would be pretty cool to see just to see like if it actually does the right thing I think that this will be a nice little experiment so what what I want youall to try out is like maybe change the the the offset back to earliest offset for the aggregation job so what I'm going to do is I'm going to kill the aggregation job real quick and then what I want to do is uh I want to you'll see how we have this am. set parallelism so let's set that to three so that like maybe we can have because uh that's how many uh if you saw in uh in this process events Kafka you'll see that we actually have three partitions of data so we know like that the data is split up in three buckets already and so that that will be something that will help us get more uh like parallelism and we should be able to Crunch this data a lot faster if we have three tasks versus one so then what I want to do is want say make down and then uh make up and then here I want to say make job and here I want to say make aggregation job and then I just also want to show youall what that looks like differently in the Flink UI because this is essentially how you set the parallelism so that Flink knows how many tasks that kind of spin up I I I'm wondering if we might need to have more task slots but I don't think so because I think uh in this case the task and the parallelism separate I think it's like workers but um that I want to be more sure about okay so we did okay so you see does anyone know how I can um view the cka tables e okay so part is's this here which is a caki right I'm just going to open up another V \[Music\] code I'm just going to get this set up for do you do lives while studying yes I do um there is a Discord but it's for UK um there is a Discord that I'm a part of but it's for UK um It's a UK Discord I'm not sure what your base right I'm just going to use this yeah for for e watch this see here how um this one is uh we're running out of uh see how we get these are scheduled tasks right so you'll see in here how we now have parallelism of three but this is three on each uh on each task here so the number of tasks slots here we probably need to bump this up quite a bit because there's actually going to be uh so because here you see how this changed to six that's because there's three T there's there's the default parallelism is there's three tasks per stage or and so we have six and six here so that's 12 that's 14 I want to change this to 15 real quick I'm G to change the uh this guy to 15 just so we have like enough spots and then let's try make down one more time just to kind of show youall how like because you really do want to learn how to work with uh parallelism and all this kind of stuff because this is these are the real problems that happen in streaming right this happens all the time so like uh and this is the kind of things that will like just kind of playing around with this stuff will hopefully this is going to give us what we're looking for so and then this should be able to just crunch the data way faster like we can have like this like beast mode kind of crunching of the data okay there we go now we're getting some more more jobs in here okay initializing initializing there we go now now see now we got the six and sixes running and so that's great that's going to uh that's going to make it so that these things are running way faster right we should be able to see like way more data coming in through here and it will uh it should just um kind of crunch all through it and we should see a lot more data showing up in postest as well so if I okay so there okay so you see I did process the next one down here you see the this is the the the 740 slot and um I did a little bit better there I was able to to swing and get 25 in that in that case so um this this seems to be working actually I think like I'm guessing there's actually like a lot of data here okay there we go see now see now we were able to crank it up and get the parallelism up to like what you would expect and you see now it's just like it's making quick work of that data like it's just like it was able instead of feeling clogged right it's just murdering it now because it it's splitting it all up and it's reading each partition at once so we're getting at least a 3X speed up here and so um that's what these parallelism that's what this parallelism is going to do seems like this is working pretty well now um and so you definitely want there's like this kind of off right between like parallelism and the stuff like one of the things that's nice about Flink versus spark in this way is that the parallelism numbers are usually lower right because like you see how I moved it from one to three like in spark you're always like thinking like oh should this be 200 or 100 or how many tasks or how should this split up right and uh because you know in spark the default is 200 but in Flink the default is one because and and that's like one of the fundamental differences between batch and streaming though right is because in streaming it's like okay we're going to process the data as it comes in and that's going to give us what we're looking for and now this is going to essentially just work and uh it will make super quick work of it but obviously one of the things here that is can be uh kind of tricky is now like this job is going to be running all the time and there's going to be times when there's not a spike in the data that and we we don't need six we really don't need six uh tasks to process the data we need one or two and that's where uh it's a trade-off right where I I bumped it up the six because I wanted to show y'all in this lab and I was like impatient right and I wanted to show y'all like how this actually is going to work so um because you'll see in here you can see uh how the the votes are coming in and oh you can oh we can actually even see one let's see that the source we can uh look at source too okay so why is it like not all ordered okay there we go did Zach get did I get any Twitter love I hope I got some Twitter love right oh like okay good three people one of the things that I think is uh important here is that there is so many just like jobs that you can run right is the is the sound not coming at the moment did you say and that like should I think one of the other important questions here is like should these jobs be ran should this aggregation be ran in Flink or should it be ran in spark or like should it be ran in batch should it be ran in streaming like how do you know like because they both add latency right this adds five seconds of lat or this adds five minutes of latency right and so it's not like you're just uh like you get this for free right in terms of latency so um one of the things to think about there is okay like what kind of aggregation are you doing over what time frame so one of the reasons why this aggregation is really nice to do in Flink is because it's a five minute aggregation and so and since it's a five minute aggregation like doing that like imagine running a a running a spark job that ran a group buy every five minutes like that jobs a joke because it's like the time it takes to start up freaking spark to then run the group buy is going to take more than five minutes so it's going to take like three minutes just to start up and then it's going to run its query and then it's going to it's going to essentially be run the whole time anyway except like you're going to have all this start up and tear down cost where Flink is actually going to run more efficiently in those cases and so like there is a line where I think that it it flips and I think that that's generally at about an hour so if you have a group buy that's like an hour or longer then that's when batch kind of wins and kind of takes over like if you have a if you're trying to group and DD over like longer periods than an hour then uh you probably want to do this you don't want to do a tumble window that's an hour you do a tumble window that is I don't know 30 minutes or something like that and then you do another aggregation in spark for the hour or the 5 hour or what however big of a aggregation window you want to do like generally that that's kind of like what I've noticed when I've been working with streaming that that's kind of there's a line there between like hourly and like sub hourly where where that's where it kind of transitions from like you should do the aggregation in streaming to you should do the aggregation in batch uh obviously this only matters like that rule of thumb really only matters when you're working at very large scale uh like you can like Flink is totally possible like at smaller scales you can have Flink do a daily aggregation and just hold on to all the data like this and kind of aggregate it down because this is not that much data like it really isn't so like and since this is a FIV minute interval it actually writes it out and it doesn't hold it on to memory anymore right it once it once it once the window closes it writes it out and it go it persists to dis at that point so you don't have to like have tons and tons of ram that's why batch is better for bigger Windows though because Flink just needs a weird amount of ram in order to like hold on to so many windows for such a so many windows have so many like data points for such a long time whereas uh like spark is going to be able to handle that much better so um yeah I'd say that that's kind of the perspective of like how to do these aggregations congrats on making it to the end of the streaming data pipelines course it's one of the most complex things that you can do in data engineering so I'm proud of you so I'm just basically what I'm trying trying to do after this now is I'm trying to see if I can look at the data I'm just going I've just basically um found a UI for kfka and I've just got the docker up and running um and all I've done to do that is you can go to you can go to this here this capka UI and you can just run um this command here to to run it that's all I'm doing and I'm just going to add um the username and password which is going to be the traffic key and the secet to see if this is going to work it might not work cool so that's working so now I've got that working I'm just going to see if I can get any information from it go go to topics I should have this one here yeah okay I want to add a time stamp of today basically Okay cool so it's definitely um running which is cool sorry I just wanted to make sure that like I could see the capka data um cool I'm happy with that cool that's the current stuff that's running ium like it's just because yeah I assume it's I wonder if like if I run this now I wonder if like the this will change State at all I don't know I'm not quite sure I'll be interested to try my own cap actually this is pretty cool um for I'll definitely need to do some more um understanding of the the code and just to see how it's connecting to cafa cuz I don't feel like is doing anything what's see but I'm happy that I've got it I can look at the data and I can see the data and um I like that so I'll I'll have a bit of a play with that and see if um there's anything else I can do yeah know that I like that that's cool cool right I'm going to end that there I'm going to do a make down now and then I'm going to end the video here because I think it's quite late but I quite like that um and we'll see what the next episode been like I said with this one the thing that was awkward is the data is or the the code is different to the the lab and so if you're looking at it and you don't quite understand how events work you're kind of looking at the you're looking at the Cur and you're like well this doesn't match like what what am I meant to be looking at so I think that's my only criticism really for this lab it's just they they're a bit mismatched um but not a problem we'll see what the homework brings which I will get started on probably like tomorrow or the next day thank you for watching for those who have watched and I will see you next time goodbye everyone


 > [!info]
> - Real-time architecture requires a way to intercept real-time events; there's more to streaming than just Kafka. There needs to be an event generation layer (3:33)
> - Lambda architecture involves both batch and streaming pipelines, doubling the codebase. Batch pipelines serve as a backup if the streaming pipeline fails (17:40)
> - Kappa architecture uses only streaming. Flink can switch between streaming and batch modes, but backfilling history can be difficult (23:06)
> - Iceberg allows Flink to dump data to partitioned tables, enabling backfilling days at a time (23:55)
> - UDFs (User-Defined Functions) in Flink are used for custom transformations. Python UDFs are less performant than Java/Scala UDFs due to separate processes (27:52)
> - Watermarking is good for slightly late events, while allow lateness is good for events late by minutes. Allow lateness can reprocess and merge data (43:57)
> -  Tumbling windows are similar to batch processing, involving fixed time windows (35:16).
> - Sliding Windows overlap and can identify peak times. These are good for edge cases around midnight (37:06).
> - Session Windows are user-specific, starting with the first event and lasting until a gap is detected (42:35)
> - Set parallelism to three, to have more parallelism and have the power to crush the data (53:00)