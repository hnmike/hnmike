---
title: "DataExpert.io - Fact Data Modelling Day 3 Lab & Lecture - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2024-11-30
source: "https://www.youtube.com/watch?v=dJZ6h0ISQVo&t=4078s"
image: "https://i.ytimg.com/vi/dJZ6h0ISQVo/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGDwgWihyMA8=&rs=AOn4CLAc273FjLiMNBv1HvugnjQM1v15xQ"
created: 2025-03-23
tags:
  - "youtube"
  - "Data_Engineering"
  - "Fact_Data_Modeling"
  - "Data_Bootcamp"
summary: "DataExpert.io Bootcamp Day 3 covers Fact Data Modeling, minimizing Shuffle, and using array metrics for efficient long-term analysis. Practical lab included!"
---
# DataExpert.io - Fact Data Modelling Day 3 Lab & Lecture - Data Engineering Bootcamp

![DataExpert.io - Fact Data Modelling Day 3 Lab & Lecture - Data Engineering Bootcamp](https://www.youtube.com/embed/dJZ6h0ISQVo&t=4078s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> for e for just having some audio Kinks that I'm working out right now just so that you can hear the um the lab with me bear with me cool I think hopefully that should work now uh Shuffle the thing that sucks about Shuffle is that it's it's the bottleneck for parallelism so if you have um a pipeline that doesn't do use Shuffle it can be as pretty much as parallel as you want it to be uh because of the fact that you don't need a specific chunk of data to all be on one machine you can have it also I've had to jump straight into it today um because I am on a tight time crunch so um hi for for those who are who are viewing but um yeah I'm on a tight time crunch so we just jumping straight into the lesson today have it where like all of it split out and we're going to talk more about like what that means and the parallelism of stuff in another slide but keeping in mind that like some steps in your big data pipeline are going to have more parallelism than other steps in your big data Pipeline and because of that like the usually the more parallel parallelism that we can have the more effectively we can crunch Big Data because that is going to be what makes it possible for us to use more machines and to have less bottlenecks because Shuffle you can think of Shuffle as the bottleneck it almost always is the bottleneck for um uh if you were working with very high volume data so there's a couple different ways that we can you know think about minimizing um Shuffle but uh we'll be going into that there's like a there's like a modeling way to do it there's like also like more um kind of tactical ways to do it that we'll talk about uh so yeah let's let's talk more about Shuffle shuffling is so fun I think we're talking about um why shuffling can um like disrupts par or makes it hard to parallelize and I I'm pretty sure that's just because you cannot do like like changes to a lot of data from two pipelines um at the same time so like by um doing the techniques he's going to go go through we can reduce that by saying okay this P this pipeline will work on this amount of data and this pipeline can work on this this um set data um I think that's what he's going to talk about I've I've definitely R into this issue before where like you've had to have things run consec con um consecutively because of the fact that it changes um because a table count the table that we're working with and the way that it was being done um could not be altered at the same time when you are working with big data uh there is you have in SQL right you have a couple different keywords select from where Group by join having order by um these uh these keywords uh affect Shuffle in different ways um for example uh if you have a query that is just SE that just uses select from and where and importantly your select doesn't use a window function but like you so you have select from and wear without a window function that is what is considered a query that is infinitely scalable you could essentially have it where like if you think about it this way like say you had a billion like let's think about it in like kind of the most extreme sense here right so say you had a billion rows and you had a billion machines and each machine had one row um the thing is is like with select from and where that's fine right because one machine doesn't need to have more data or less data because it all it needs to know is like okay here's the data I have does this data fit this condition or not and if it does okay bring it out otherwise don't bring it out and that's that's it that's all like you don't have to have like a specific chunk of data on a specific machine that's the big thing to remember about select from and where is that like if you're using those three like you can go as parallel as you want and that is uh very very powerful if you can understand that fact because you don't you can use if you have queries with those three keywords and only those three keywords like uh the job is going to essentially come back instantly the job is going to be no Shuffle the job's going to be cheap the job's going to be all those things right so now let's talk about the ones in the middle uh here we have group by join and having so why are these not extremely parallel well the main reason for that is because so say you have group bu right and let's go back to that example a billion rows and a billion machines is that better is that is my volume better now I'll just bring my microphone closer to me is that better oh is it Zach's volume that's the issue right and there's one row on each machine well with group ey the problem is is like in order to actually do the right aggregation all of the rows for that key so say there's me and I'm someone who has 30 rows in this billion row data set right and um all of all 30 of my rows need to be on one machine they all need to be moved over into onto one machine in order to correctly count them right because if they're all separated and my 30 rows are all on different machines then we how do we know that there's 30 we don't know like that's like and that whole process of collecting like so the data is in a bunch of chunks and it's all separated out into like these this comically chunked out like one row um per machine kind of situation uh and in that case we'd have to move all that all those into one machine where then it has all 30 rows of data so then it can count at that point and that's why groupby triggers Shuffle and that's what all Shuffle is right is Shuffle happens when you need to have all the data from a specific key on a specific machine that's all it is and so um and that's the big thing to remember when you're doing shuffling and so that's why Group by works in this case right is then you would reshuffle and then uh and and essentially what it does is you get a pick you get a pick the parallelism there right because uh in spark there's a thing called spark. SQL do shuffle. partitions which is a configuration that you can pick uh which is something we'll talk about in week five as well but uh the default is 200 so we would have our billion machines they would pass all of their data to 200 machines based on the kind of the key right so say you have user ID and the user ID is an integer and then what you would do is you would take the like the modulus so you would uh take the you would divide the user ID by 200 and whatever the remainder is that's the machine it goes to right so say it's like uh user ID is 207 so that would go to the seventh machine right and user ID 4002 would go to the second machine right etc etc and that's like how they kind of like split it out and uh and you can change that number that 200 you can change it to whatever you want 200 is just the default but anyways Group by is going to do that it's G to kind of bring everything back together and it will do things where you have those 200 uh kind of machines that will process and then aggregate the data once it's all collected um along whatever whatever you're grouping on then it will then do it that way uh what about join uh join is actually even trickier uh in some regards because join you have to do this not once but twice so what you do in the join case is you have all of the keys on the left side and all of the keys on the right side and then the keys on the left and the keys on the right all have to be pushed to one machine and that's going to be one partition in your Shuffle partitions which again defaults the 200 but then after that then the comparison happens to see like oh do these match or not and then if they do match it keeps that record and if they don't match it Chucks it away or maybe not if it's a left joint or a right joint or whatever it depends on the joint I'm talking about inner joint in this case um then having so having I I I I was having difficulty having difficulty putting it into a a bucket here I put it in kind of parallel because of the fact that you have to have having having and group by kind of go hand in hand so technically having is like as parallel as select from and where because it's just again a filter condition but it's a step after group bu so you can only really apply having after Shuffle so um that can be a big thing right uh and then let's talk about the last one here uh in distributed compute uh which you you should almost never use right uh is order bu order bu is the most painful and least parallelizable uh keyword in SQL and here's why so if you think about order by like um especially like I'm talking about order by at the end of the query not order by in a window function order by in a window function is a little different and we'll talk about that in a second but if you have order by at the end of a query um what ends up happening so say you order by user ID and you need to do a global sort of all the data and it's like on and say it's on 200 Mach or it's on a billion we'll go back to theal example where we have one one row of data on a billion machines and then we need to make sure that all the data in all those systems is sorted the only way that we can make sure that that is the case is if all of the data goes on one machine and it all gets passed through one machine and so uh which is I don't know if you all know but that's like the opposite of parallel right because that's that's one machine that's sequential at that point and that is where a lot of times you'll see uh this can be a very big symptom in big data jobs if you have order bu like you can use order bu but use it at the end after you like aggregated and the number of rows that is left is like in the thousands or tens of thousands if you're using orderby and there's millions of Records like your job's G to have a bad time it's gonna have a really bad time so how is this different in a window function and like why is a window function not going to be like if you use orderby in a window function uh the main reason for that is a window function doesn't do a global sort well it can it can right if you don't put any Partition by in there and if you do like a if you're doing like a global like a global rank function so if you use like Rank and you don't Partition by anywhere in the um if you don't use the Partition by keyword then you're gonna have the same nasty terrible parallelism problem exact same problem like uh so like but if you're using partition bite which you should then uh then it's no longer a global problem and you don't have then then it goes back to the shuffle problem where you end up sorting based on the number of Shuffle partitions which is usually 200 but you can change that if you want and because that Partition by is going to create that same sort of thing where like we need to cut up the data and pass all of that same data to one machine and that's the so partition Buys in that way it's really weird because Partition by in a window function and group by in uh like a regular query like in the distributed compute World those things are those things behave very very similarly so especially in terms of how it affects Shuffle so that's where orderby is an interesting one like you can use orderby in window functions only if you also have Partition by like if you do it by itself like you're going to have a bad time so why am I talking about this like some of this you're probably like Zack like I thought we were talking about fact data modeling like why this doesn't seem like anything that's related to fact data modeling at all I'm just like I'm going off on this rant about spark and distributed compute and one of the things though that I think is very important to remember about this stuff is that how the data is structured determines which of these keywords you have to use and if you have uh if you have data that's structured in a certain format you can skip out on using some of the hi Uma how you doing these keywords so that's a big thing that you want to think about when you're like kind of going through and building out uh your fact data modeling is that so remember this uh at the very least just remember this uh this slide is very powerful it has more of an impact besides just on data modeling exercises it can help you troubleshoot uh spark problems and other problems as well so um it's it's great so let's go to the next slide okay so group bu right we talked about how group bu cause a shuffle so um how can you fix that problem so there's two two ways that you can fix this problem uh and here's here's essentially how it works like you can in spark or in uh not not just spark but like with S3 or any um kind of like or Iceberg or huie or Delta lake or whatever they they all support this is you can bucket your data so instead what you can say is like you can essentially do this you can pre-show the data uh for spark so like say like you're like okay I want to put my data into eight buckets and then um when you and then you bucket based on a key I I mostly write them down here and then um I've been writing them down and then I've been summarizing them posting them on LinkedIn um so that's that's how I've been doing it uh I don't know what about I don't know I'm not sure what to do about the homework so because I have done the homework and I'm not like I don't really want to post it onto my um um GitHub I don't know I might do I might do I might do like when like I've got through everything and and stuff but um yeah I I'm happy for people to like follow along the homework with me but it feels kind of like cheating if they just get to copy and paste everything that I've done probably it's going to be like a user ID or a device ID usually it's some sort of like high cardinality integer field and then you Buck it on that field and then you and then it does that whole modulus grouping when you write the data out so if we have the modules grouping when it writes the data out then when we do group by we don't have to shuffle uh because it's already it's already in the buckets are already set and we already have the guarantees that this data is here and this data is here and uh so then the group bu can just leverage the buckets and it can just crunch it down and that can be very powerful that can be a very powerful way to do group by without um uh without Shuffle and so definitely we're going to talk more about in the spark week uh week five and that's one way to do it um but today we're going to be talking more about the other way which is all in caps here which is reduce the data volume as much as you can and like because if you can reduce the data volume and you can uh uh also reduce that that also reduces shuffle a lot and you can really create uh way more efficient queries how can we represent fact data in a way that really is efficient and wonderful and can do really magical things so let's let's get into a little bit bit reduced fact data so we talked about fact Data before fact data often has a schema where you have like a user and then you have a time an action and a date right this data is very very high volume incredibly high volume then you have the next layer down is you can aggregate this up right so instead you could have like a user ID and then you can have like an action count so in this case you could say like I to I made Seven likes on January 3rd right and that could be that daily aggregate um that is great the the the volume is dramatically reduced that way right it's like it gets cut by the number of actions per user so a lot of times that might be 100x smaller um that's good but even that data can be too big and I'll talk more about like kind of the problematic things that can happen there and that's where we I honestly love how logical like data engineering is it's just so it's it's like pure logic it's great we're going to go one yeah I do the same thing really um and I try and like write down withad the Keynotes um and it helps it helps it stick better for me step further here where we talk about this reduced fact take this one more step right so in this case what you have is you have a user ID and then you have an array of action counts so you can think about it this way it essentially cuts the daily data to uh it divides it by 30 if you use month but it can also divide it by 365 if you use year so it's like you can pick which way you want to do it but this data ends up being like you can either have one row per month per user or one row per uh year per user and those two things those are uh a very powerful way to get the data essentially as small as you possibly can get the data and that is what what can really minimize the amount of shuffling that can occur and really minimize the the whole thing like it's it's really powerful and we're going to talk more about like how this unlocks a lot of new capabilities but the key thing to remember here is you got three flavors here you have like just normal fact data very high volume daily Aggregates medium volume and then you have reduced facts which are the lowest volume um they come with come with some tradeoffs though especially between the the second two like between daily Aggregate and reduced fact they there's some interesting trade-offs that we're going to kind of dive into as well because as the data gets smaller you lose some of the flexibility around what type of analytics you can do on it but that like worth the tradeoff of being able to do analytics more quickly so that's one of the things that you'll find in data engineering a lot of the time is that when you are working with data and you aggregate it up the data gets smaller easier to work with in terms of like speed of queries but then it's like you're like oh what if I want to do this or what if I want to do that and then like those some of the options are taken away from you because the data is too aggregated up so let's go a little bit deeper into each one of these you see that all the time with um with things like YouTube and stuff like that where like you know you get quite a lot of analytics for the like the previous month but then as you get like further into like the previous years the analytics get less and less um I see with my Fitbit as well there's only like I get a lot of data for like the previous like couple of weeks but then over the last few years it's it's a bit it's less um okay so this is what a fact schema might look like where you have like a user ID you have a Tim stamp you have the action that they took you have the date the date partition and then there might be like other um properties like for example here it's like I made a like on this post on my on an Android phone at this time and here's like I commented on this post on an iPhone at this time and so like the thing about fact data like when you're here it's very very granular and you can ask very very specific questions of like whatever question Under the Sun that you want to ask but the trade-off is like you can only ask questions usually for like what has happened recently or like within a couple days you can you can look at very small time Horizons and the reason for that is because you end up missing uh like the like the volume ends up being unwieldy if you have too if if your time frame gets too big so you can't like the fact data like especially if you're looking at more than a month of fact data like if it's high volume enough then like a month of fact data is going to cause like all sorts of Mayhem and all sorts of problems and it's going to cause like it's just going to cause your queries to be really slow and like they might not even run you might get some weird Adam memory problems like there's all sorts of interesting things that can happen when you do uh your fact data modeling and or your analytics just right on top of fact data right but at the same time like this is great like for other questions right if you're answering very specific questions like what happened today or what happened in a week what happened in two weeks right um one of the things that I noticed that Facebook is uh like so they had this thing where like after 90 days everything had to be anonymized so like that policy also really impacted analytics a lot whereas like uh it made it so every everyone was just looking at the last 90 days of data and like they didn't like look back for and they weren't trying to figure out like okay how do we get like the other days of data in here as well but um anyways this is uh a crazy problem that can happen but uh so fact data the key thing I'm trying to take get you guys to take away from this slide is that this schema is very flexible very good at answering very specific questions but it's very not good at answering questions over a longer time Horizon once you have a longer time Horizon you're going to have a hard time using this schema I promise okay so let's go to the next schema that kind of that daily aggregate right that was the next one that we were talking about so here's daily aggregate so uh daily aggregate you have a couple things here right you have again you have user ID then you have like a metric name in this case likes given you have the date and then this is how many met or how many likes we're given so now we have our daily Aggregates uh this is going to look a little bit nicer um now in this case uh you have one row per user per metric per day this is a little bit nicer uh instead of having like you know for likes like for example this first row here there was 34 records in the old table and now there's one and so that is a big difference right that's a that aggregation is going to make this table a lot easier to work with and you can do a lot of really powerful things with this table as well so one of the things that's really nice about this daily aggregate table is you get um a lot longer time Horizons now like I noticed at least when I was working in big Tech that like if you have this schema this works for like if you want to look at like a year or two years this works great this works great and uh this like can work really well and the thing that's really nice about it is you get these nice joins uh because say you have like an STD right so you wanted to know like what my primary device operating system was like whether I was on Android or iPhone and like which was my favorite and that might change over time but the thing that's nice about these Aggregates is you have that at least down to the Daily granularity obviously you don't have like the hours you lose you lose a little bit there like if you have something that's changing like on an hourly basis then it's not going to work but like most scds are on a daily basis anyways so you can still join this table withd Yeah I think and I think that's what you says about how like it do lose specifics but the benefits of having less data to work with outweigh that for certain scenarios so when they decide on like you know after 90 days they they they're going to aggregate the data into like monthly for example um it makes sense why they do that because if they had like if they had like 5 years worth of data where you know they knew exactly everything that was happening every activity that's just going to be really really like unper performant and you can still do like um Aggregates at the higher level of bringing in other dimensions and that could be a very powerful way to use this schema so and it works great um those uh those yearlong or two year long queries they do take a little bit of time though they take like I mean at least back then when I we were working with Hive and uh those queries would take like like two days right is two days to work with and that's like kind of a showstopper it's kind of not the way to go and you can't analyze Dimensions very well uh if it takes that long like or obviously two days is not that long but it's like still long enough to be painful so one of the things about this uh aggregated data is it's very powerful so this this schema here this Daily aggregated Facts is um has another name um it's called like a a metric repository to give it that way where like in this case uh your partition on this table is actually not date your partition is date and Metric name I think it's also important to kind of remember as well like this is a lot more for like the analytics side it's not really focusing on the the governance side so if the Govern side is still needed I think there is still the need to store like that data but like maybe somewhere else so that you can go through it if you need to um but like this is about just being able to get those quick analytics it's a you have a sub partition on Metric name so that you have two partitions for this table so that you can you can dump in all your metrics into this daily daily table and this schema is very very powerful like so many of the um like I don't know if y'all heard of deltoid deltoid is experimentation framework at Facebook that powers a lot of like the AB testing and all sorts of different powerful things it uses this schema to determine if an AB test impacts uh different sort of metric values and all that kind of stuff so um one of the things I noticed when I was uh working at Facebook though was uh this schema could still be improved on we can still make this smaller and not lose anything right there's still one more layer we can do so this schema is what was very powerful when I was working on Facebook so this schema probably looks a little bit crazy but we are going to be working with this schema today and we're going to be building out literally this exact uh data model uh in our lab today but let's kind of go over like how this works right so you have a user ID you have likes given and then in this case there's only one row per month right and then you have your value array so one of the things that I recognized when I was working on this problem because a lot of this stuff like why I even had to come up with this model right was I was working on these problems of like uh there was the first Decline and growth in in in at Facebook in the history of the company and they were like what's going on what's going on we need to look at all the metrics we need to look at all the metrics over the whole period of Facebook like how do we like figure out what the hell is going on here and like how do we can like troubleshoot this right and that's where like we were mostly working with these like daily metrics and I was like wow these daily metrics are pretty fast but like we had that two day 3-day backfill problem and I was like that's not going to work right that's not like that's not going to work so then we moved to this like uh we move to these monthly metrics right which essentially reduces the volume again 30X because of the fact that now we and but the thing is is like keep in mind this is not a monthly aggregate that's the thing that I hope y'all keep in mind here and that's why this is a beautiful thing if it was a monthly aggregate this would be boring and like y'all would like not like this and this would be kind of stupid this is actually not a monthly aggregate so how this works right is we have this month start and then we have this array of values so the date is an index so in this case 34 is how many likes user three gave on July 1st and then three is July 2 3 July 3rd and then nine is July 31st so what this does is instead of storing the date as a string right in multiple rows we store the date as an offset or as an index so you do date math based on the index the position of the position in the array is equal to what what the date was so this is very similar like in a lot of ways this is similar to um yesterday's lab on uh date lists like if you all remember like how we had that crazy like bits of ones and zeros and the position based on like where if that bit was the first bit that meant it was for today and then the last bit meant it was like 30 days ago or whatever this is a similar concept except you are doing it for um uh nonbinary things right you're doing it for uh things that are either like a decimal number or uh like a count an integer value I I talked about a lot of the stuff already where I said like the first index is for the date at the beginning of the month and the last index is for the date at the end of the month right and I was talking about how dimensional joins get weird if you want things to stay performant because you don't want to like have to bring in a dimension in the middle of the month that was actually one of the things that like I was trying to figure out like that was like one of the last things I was looking at at Facebook was like how to do that like how to actually make it so that uh it the dimension gets assigned to the right value based on the date and like based on the index in the array and then the sum happens the right way without exploding it and I couldn't figure it out that was like I was like it's too crazy too crazy and I was like I and that's one of the beautiful things about data is that like if you explain to people the constraints of what is like what are The Oddities and what are the weirdness of things that can happen with this and then there and they can accept that as like okay that is inconsistency but like it's fine for like what I'm trying for the purposes of what I'm trying to use this for like it's great right and so that can be um a really powerful thing that will work right so what this did was we were able to en enable long-term analyses that were a lot faster so go to the next slide so here's the impact from this new schema right so before if we wanted to do a 10-year backfill with that you know that intermediate layer that daily data a 10-year backfill would take about a week a week to maybe a little bit more and once you got the data into this format instead of taking a week it took a couple hours it took like three or four hours to run because it's just dramatically smaller dramatically smaller data and um and it made it so we could cut up the data and look at look at all every possible Dimension Under the Sun like for the entire history of Facebook as quick as we wanted and that's like I talk about this like unlock the decades long slow bur analysis at Facebook which is like okay is there a certain type of user that is not doing a certain type of thing over a long period of time but the decline is so slow that our alerts don't pick it up and our alerts don't see it right so then what we were able to do is just like kind of look at all that stuff and like I don't know if y'all have ever heard of like root cause analysis or RCA so RCA is a very powerful thing to try to troubleshoot what was going wrong and what happened right and that is a big thing that this framework unlocked and so uh yeah this is very powerful I felt very proud of building this out and a lot of people in the analytics space at Facebook were like wow Zach you're actually doing stuff that is kind of innovative and new things here that are kind of changing the company and so this is really powerful and uh I think I I think I have one more slide here so I I I loaded this uh framework up with 50 50 core metrics and 15 core dimensions and this uh enabled uh you know tenure analyses to happen in hours instead of weeks and uh they didn't promote me they uh they said that that was not enough uh of an impact to be a senior engineer and I was like very angry about that because I thought that that was because I thought I solved some really insane problems with this framework but yeah that was uh so the fact that Facebook did not recognize the impact of this but it was crazy because then later on they did because like a couple of my teammates who stayed they kept working on this project right and uh they got this thing called discretional equity which is where uh Facebook will give people like another big Equity Grant to keep them and maintain them and like uh and so they they they made millions of dollars off of all my hard work and then like I and they're grateful for it I'm still good friends with these people but I'm always like I tell them sometimes I'm like y'all owe me y'all owe me y'all owe me money dude like so yeah um I think that is what we got for the the the lecture today congrats on getting to the end of the day three lecture reduced facts are crazy right I'm excited for you to check out the lab and how to minimize Shuffle and get in there and get handson if you're in the platform make sure to skip to the next link so that you get the credit that you deserve so we can say create the um it is like as someone who also epic tech there is so much politics that can kind of go um into promotions as well like because you have to kind of prove that you're working at the level like above you for like two consecutive um like like periods which is a period usually like six months um so even if you are say like you know doing something really really cool in like the last six months it doesn't matter if like that you you know um didn't the the six months before and um and yeah and then also you have to have like diff various people advocate for you and stuff like that so it can be a little bit weird um like and then if if it it there's a lot of politics basically create table I'm just going to get my um PG admin back up which is already here I'm just going to open a new query for um day three so um well day six I think it is um I keep spelling data modeling the UK way and then I have to remember that most people are from the US so I need to like update it when when I'm doing like my um summaries I change it to um to the like americ button uh what's it called this uh well let's call it uh let's call it array metrics I think that's a good name um and then we have a user ID right and apparently want to call it numeric cuz numeric will work because I remember in yesterday's class it was like weird weird cuz we'll just match it with whatever's in events here even though I don't like the word numeric I don't even know what that means like but like it it's fine because we used big in last time and it didn't work but I think numeric will work because I don't want to use text okay so we have us already then we have uh like month start uh we'll call this a date um then we have metric name uh metric name is going to be a text and then uh we need the array right so uh uh I'm just call it metric array and then so there there's a big debate here about like okay like what type should this be right is this like a real array or is this an integer array or is this uh like you could say this is like a scoring class array right if you want to do like array of struct right but like I I'm not about that life um I think we're going to do integer array here I guess like you could so for anyone curious um numer basically you can have 131,072 digits that's a lot of digits say integer or real right because if you use real like real and iner like they they work like you can put an integer in a real but you can't put a real in an integer so uh okay fine you guys convince me or I convince myself we're going to use real um in this case we have a primary key here primary key is going to be I love um I love ZX little like quirky personally the uh user ID month start metric name right this is going to be our uh table we're going to be working with today uh that we will be building up slowly but surely so let's go ahead and uh create this bad boy cool so one of the things about this that is kind of tricky is that you have to uh think about this in terms of partitions and like Hive and uh or like partitions and things like that and that's where this like can be a little bit messy compared to like in postgress whereas it's a lot cleaner like when you have like that insert overwrite sort of mentality right because in this case we need to have month start but it'll make more sense like I we'll cross that bridge when we I'm not sure but I think I think poost is a really accessible database for anyone so um it's open source it's free to use it's quick to get set up so I think that might be a reason um and I find it quite easy to work with as well so and I know a lot of companies that use it so um I guess I guess that's why I don't know um but it is quite accessible for various people from like you know starting um from you know finish University to um to people like me who are like already like in in their careers so it's a good tool I think as as a Teach as a teaching tool when we get there but I I'll show you what I mean so what we want to start with here is we want to create that daily aggregate um function all right that's actually not too hard so let's go ahead and say with daily aggregate as so we're going to be pulling from events here I'm going to comment this out we pull from events we're gon to say select star from events and then well what do we want from events well we want uh prob want user ID and then we probably want a count one as numb sight hits and then we're going to have a where here and then we should be able to do date of event time equals date then we'll say like 2023 0101 so we're just going to do we're going to do like the month of January and that's what how we're going to build up this array so if we do right now and we say daily aggregate uh we're missing though we're missing we're missing the group by so we got this guy we run him cool see we have all of our hits okay that's a problem we need to get rid of that uh this null so and uh I'm gonna paste this to y'all like because obviously I just like jumped immediately into coding all right I was wondering how you are typing so fast while speaking so good okay so we have have our aggregate for the day right so like what we want to do is we need like yesterday's Aggregate and so this is where like like for the purpose of this lab I would change this because you you don't get this is one of the things I hate about post I'm not sure because I I I used it in my old um I used it in my old company and it I never had a problem with it it was really good um but to to be fair I think I think from what I understand it's not it's probably not the best thing for like big tech companies but like for the for the size of our company which was like a startup of um it was like a second stage startup like where they weren't yet making like Revenue but they were getting and they were still being um like SE seeded by investors but um they only had 50 employee employees kind of they had 50 about 50 employees um but um we use postgress and yes I am from the UK that I wish had postgress had was like a merge because postgress doesn't have merge right yeah it doesn't have merge I think it has like I think you could do on conflict on conflict update though right I think you get that I think we can use on conflict update we'll try I I'll um I I might end up Googling a little bit here but so in this case we have our daily aggregate and we need uh last month's aggregate as well like we need like uh we we need like what was yesterday because if you think about this in production when this is running um uh like on January 1 the array will have one value then it will have two values then we have three values do all the way up right so that's something that we need to consider when we are building this out so let's go and get like yesterday array as um and then in this case we just going to say uh select star from array metrics where uh in this case we're going to say uh month start equals date 2023 0101 because we that's all we care about um so now in this case we can do like a have our daily aggregate full outer join yesterday array one of the things that I hope y'all like uh at the end of this data modeling stuff is that like you'll recognize that every data modeling problem is actually the same problem where you just uh where you full outer join all the time and obviously like if if I said that on LinkedIn people would be like Zack did you have a stroke but um anyways uh let's just run this now and I think this will make more sense so we have our site hits and then we have NES across the board on the other side like we would expect right so now what we want to do is uh we have all this then what we want to do is we want to uh essentially create an array or we want to create a new array for this daily aggregate if it doesn't exist and then we have to fill backwards like essentially from that date so that's where we actually do need to pull in this date here we so uh because this is going to be needed we say as current date because we need this uh oh we'll call this fun we'll just call it as date I hate how postp doesn't like like current date's like a keyword so we're going to need this and we're also going to need to group on it right because we'll need this to do the the date math later for like there's an edge case where we need that I promise so initially what we want right is we're gonna have I can't see the um I can't see the the blue flag I think the the green black and yellow flag is Jamaica is that right and then I don't know is that a leaf in the middle of it um on the blue one um those let's go yeah it's funny it made me me that like every data modeling problem is the same problem it kind of feels true you do a lot of the same things don't you um but um how to learn better the SQL queries uh because there is a lot of um oh nice CU it's really cool how like people who are watching are from completely different parts of the world I really love that like um it's it's it's really great you realize just just how many people um from various parts of the world just um at like you know on on on LinkedIn and stuff like that and and connected and following and stuff like that which is cool um cuz have you like tried like you know kurd Academy or anything like that so to get get kind of started with just basic like SQL a lot of the time they're just free courses on on code academy let me just link um code academy it might be worth just seeing as there's any good SQL courses on there have a coals here of da. user ID and ya. user ID this is always the best part when you're just like yeah I got my cols figured out and then now like okay so then month start right month start can just be uh like another cols here so the cols here is going to be between and this cols is really really gnarly actually so in this case we have um you have ya. Monon start comma and then you have da. date but as a month start but this is not quite right because uh this day moves forward so you got to like truncate this right so you got to do like date trunk month uh da. dat so that like as we kind of cumulate up this will still stay month start okay so now we have month start then we have metric name uh good thing uh good old metric I'm starting to wonder if um if um other people like are doing the actual Labs or just watching them cuz I'm doing every single one of them I'm writing it out myself and everything but um I don't know if other people are name here is we call the sight hits that's a metric name this is something that like is usually hardcoded but it's good now we have the hard part building the damn array so uh you can think about it in this first case when yesterday array is completely null and completely empty it's like pretty straightforward because we know all the users are on the other side so or they're only in the daily aggregate but they're not in the yesterday array so we're going to essentially fill in the first one and uh because I want to I want to really illustrate to y'all the pitfall here that can happen so in this case uh we just want to do array and then um so we want to say uh case when ya. metric array is not null then uh what we want to do is if the metric array is not null we want to say why. metric array but we want to um so this is the next day though so this is the opposite like from yesterday's date list one where we put the most recent data first this is actually the other way around because we want everything to line up right that's one of the things we want to do here is we want to have everything line up so what we want to do is we want to do a concat here of and then we want array and then we have da. num sight hits but there is uh this is uh another one of those edges where this could be um null and that might be okay y'all might be okay with null like I think for this case I don't like null I want to do uh zero instead of null so that will be um so then we have else okay so then then we have uh so if the metric array is not null that means the user already exists but then we have when ya. metric array is null then for now what we're going to do is we're just going to put put in the uh this value here but this is actually wrong and um I will explain why this is wrong here in just a second but you'll see with uh this kind of daily aggregate this is getting us pretty close so you see we have our user ID and we have our case when statement and it looks really nice W someone someone hit my website 60 times on New Year's wow someone needs to get a life or maybe they just love my data engineering maybe he's just a Super Fan I'm sorry um okay so then in this case this is metric array right want to say as metric array in this case we need to put like an insert into here insert into array metrics right and then okay then the um site hit I just where did I just am I just losing it where did I get sight hits from to get life or maybe no I did put that right as okay \[Music\] \[Music\] why do I keep putting Deer uh cuz he's yeah I did De trunk um as month start column de does not exist I miss something there purpose of this lab I would change this because you you don't get this is one of the things I hate about postgress that I wish had postgress had was like a merge because postrest doesn't have merge right yeah it doesn't have merge I think it has like I think you could do on conflict on conflict update though right I think you get that I think we can use onc conflict to update we'll try I I'll um I might end up Googling a little bit here but so in this case we have our daily Aggregate and we need uh last month's aggregate as well like we need like uh we we need like what was yesterday because if you think about this in production when this is opposite like from yesterday's date list one where we put the most recent data first this is actually the other way around because we want everything to line up right that's one of the things we want to do here is we want to have everything line up so what we want to do is we want to do a concat here of and then we want array and then we have da do num sight hits but there is so we have on conflict to get a life or maybe they just love my data engineering maybe he's just a Super Fan I'm sorry um okay so then in this case this is okay he did put date there must have missed that from somewhere um I sorry I was missing something but and I imagine I'll have to do a group BU as well yeah thought time type again metric array right I want to say as metric array in this case we need to put like an insert into here insert into array metrics right and then on conflict so in this case on conflict so our primary key here array metric yeah we got to put all of them here so we have on conflict then we have all of those right and then we say set and then we say metric array and then in this case we say equal to what is this even doing okay because we want it to be I think we want it to be the excluded one because this is going to be the um the the other record right and so I think this is just dot metric array I think that's all we do so uh we'll see if this actually works uh but uh there is one more bug here with this guy which I will uh kind of show here in a second but we had to get this on conflict right I'm glad I got the on conflict stuff to work so we don't we can do it the right way um uh in a big data world like you don't have to worry about this because you get overwrite right and overwrite just will just like you don't have to worry about how to set the updates of things like I'm like I don't know I maybe I was I've been spoiled and I've just been using overwrite for so long that like I just expect it out of every technology I work with now and every time I'm like why do I have to update I don't like the update keyword so um anyways uh let's go ahead and we should be able to run this query now okay so we ran the query for day for the first day all I want to do here is I want to run the query for the second day and just so I can illustrate the problem and and and check if this conflict thing works okay well something worked so if we say uh select star from array metrics we should have some here that have two values okay there we go perfect yes everything everything is exactly what I was thinking was going to happen Okay so remember in uh okay so you see how like for some of these metrics like you see this first guy here he's had six on January 1 and zero on January 2nd so he didn't show up the second day right um this person was three and three just very consistently going to three pages right and so um you'll see though like remember one of the things I said was that every for every iteration of this for every data set here regardless of when a user shows up everyone should have the same number of elements in the array in this case they should have uh like these guys should have one more more they should have a zero at the front because this person essentially hello how you doing mayor didn't exist until January 2nd and that's what is going on here so what we need to do is there is uh so in this case we have the okay if the metric array is null then we need to have this array but there's also uh it's really awesome so we have an array fill function right and we need a concat here so the array fill function here is actually going to be equal to uh so in this case there's you we have month start so then we have uh we have date and minus month start so this probably looks really funky but uh like this is what this like so what this does array fill what this is going to do is going to so for the second or or or let's imagine we're on the seventh of the month and a new user shows up then what this will do is uh date will be the 7th of January and month start will be the 1 so then this will be uh six right you'll have six values that are there that need to be um kind of uh and so what this will do is this will create an array of six zeros right just it'll be 0000000000 six times so one of the things I want to do real quick is I want to like clear out array metrics though uh we're just going to we're just going to clear them out because like and it's going to yell at me because it's saying there's no you see I love I love that data grip does this so that like every for every iteration of this for every data set here regardless of when have an array fill function right and we need a concat here so the array fill function here is actually going to be equal to uh so in this case there's you we have month start so then we have uh we have date and minus month start so this probably looks really funky but uh like this is what this like so what this does array fill what this is going to do is it's G to so for the second or or or let's imagine we're on the seventh of the month and a new user shows up then what this will do is uh date will be the 7th of January and month start will be the 1 so then this will be uh six right you'll have six values that are there that need to be um kind of uh and so what this will do is this will create an array of six zeros right this it'll be 000000 Z six times so one of the things I want to do real quick is I want to like clear out array metrics though uh we're just going to we're just going to clear them out because like and it's going to yell at me because it's saying there's no he I love I love that data grip does this so that like because you don't want to delete all the data but we do want to delete all the data so um uh this that's what this array fill is going to do and what we're going to do is we're just going to move this back to January 1 and then we're going to run this two times so we're going to we're going to run it for array fill integer integer does not exist okay so this is so I should really just wait until he's actually ran the query to make sure that it runs correctly rather than trying to book it myself shouldn't I weird so you actually give it an array like that that is so weird but okay right was that like dimensional values cannot be null oh is it because month's start is null right interesting because that is null then this is null because it doesn't exist yet but oh this is this is an interesting Edge right so in that case we have uh I think there's like a third condition here actually so when when it's completely empty this is not going to work right so when it's completely empty though we can just have this first array because we know that that date hasn't happened yet so what we have here is it's like when ya. Monon start is null then we have that array okay I I think this this should run now okay we're just going to we're just going to clear them out because like is it's so weird so you actually give it an array like that that is so weird but okay right was that like dimensional values cannot be null oh is it because month's start is null right in interesting because that is null then this is null because it doesn't exist yet but oh this is this is an interesting Edge right so in that case we have uh I think there's like a third condition here actually so when when it's completely empty this is not going to work right so when it's completely empty though we can just have this first array because we know that that date hasn't happened yet so what we have here is like when ya. Monon start is null then we have that array okay I think this this should run now okay it ran so if we look look at it let say like if we search here this should perfect so the first day ran that worked great but then let's move it to the second day so that we can uh just see this working and then I will definitely send query to y'all okay so that should now we should get our filled that did not give us the field zeros oh oh oh oh oh oh oh oh oh oh because no that actually makes sense because oh because it's not matching here right so that's still gonna yeah this is wrong actually like so we gotta like essentially coales this cuz one of these values is going to always be there right because essentially what we want is like if both of these values are the same so that's so weird I didn't even like I thought I ran into this problem before I I I know I'm like kind of fumbling here a second but like let me let me go over what we actually needed to do here and what's going on so the problem here is this array can't accept a null value so what we want to do is we just need to coales this to zero so like if either of these is null then we just don't fill because we don't need to fill because that means it's the first day of the month right and that will fix our problem okay I am going to trust you this time though I don't know if I should because but now uh we have bad data again so we have to delete from the array metrics but that will um we will be uh we'll be good to go here just a second okay so that will fix our problem that's why you have to you have to put a CO Les there because you can't put array bracket null because postgress apparently doesn't like that which is again like one of those like today I learned sort of moments so I think this query should run now okay but then if we change this to two this should run now okay now we should be good how is that still not like okay now I'm wondering if like the update isn't working if if it's something with the update actually that is cuz here we are getting our the array fill because because if you have the new date oh oh oh oh oh oh oh I know what it is I know what it is it's because month start is still null because what we need to do is this month start is not actually in this yesterday array this is a hardcoded value this is actually not here because what's happening right now what's just kind I'm going kind of go over what's going on right here right so we have the date here and uh we pull it in from the array but we have a full outer join here right and so when I have this month start value here this is not the right one because this is this is going to be null so on the second if someone shows up and they don't exist yet this is going to be null but really in the pipeline this is not like this value is fixed right so this is actually date 2023 0101 and it never changes that date will never change that date's always the same so like that's why we're still getting buggy data wow that's a that's a very interesting uh uh a very interesting change okay there we go now now we'll be good with just like one I was right not to trust I was right that is a interesting one though for sure for sure I always like feel like I'm going to accidentally delete the wrong dat that would be bad on it one more delete and I think we I think we got it here that's even better I love that I love that yep but we're going to move it to that right because they so you're saying date trunk month of date right like that yeah I like that better I like that better because then it's it's not hardcoded right because then like you don't have to like if I wanted to change it to a new month I only I still only have to edit it up here right so okay now keeping in mind that like this kind of array fill stuff it this should work I'm okay we got to change this back to one though so you you you'll see like this has the same uh pitfalls that uh cumulation does ooh types interval and integer cannot match what coales date trunk because this is is it because this needs to be cast as a date as well that's post Chris is so weird well because it worked before okay no yeah there we go it's because date trunk returns a time stamp that's why so you got to wrap that in another date right like cuz that's like so dumb okay some of this stuff like like all this silly little data engineering stuff is okay now like my whole point is I just wanted to get it to be where everything in the metrics array there we go there we go I know that was painful y'all but there we go we got it so one of the things to prove it out right is we can say cardinality of metric array and then we can say count one and then we can see like how like everyone should have this should be two right everyone should be two yep there we go 138 users everyone has two value we do one is always better than delete but in this case it's okay operation should yeah but it's not like I'm using it on production just for playing with I can do what I want I can do what I \[Music\] want cool that's that is good values right and then this just keeps working too though like you'll see if we uh if we go to three right and then everyone will have three values now right if we kind of someone was saying about them getting duplicates with this data I'm just going to check that whether I do um m it count this that work \[Music\] Where do I okay I'm not getting duplicates that's cool where did that query go there we go put that back and then change that to three but you'll see now if we run this query now everyone has three so that's kind of the idea here here let me paste this to y'all because that there was that weird date cast that I think we missed that this query essentially does it where we can build these things up and uh run all of these queries at once and we can get all of the data obviously like uh this this line this line is this line is absolutely nuts though I don't know if y'all like if you look at this line of code you're like what is this guy doing here like this is so crazy but that gives us our our code for for that right so one of the things that I wanted to show though that I think y'all will really appreciate is um how to do the aggregation of this so that you can see how we can go we can Aggregate and I'm just going to show how to do it with uh with metric name and we can group on Metric name but then it will be obvious how like because you can join on user ID and bring in other dimensions if you want but you can group on Metric name and that's going to make more sense for now so I'm gonna I'm just going to open a new uh query console here so if we say select star from array metrics right this has all of our data and we have three three three days of data right now right but we want to aggregate this and what we want back here is dimensional analysis on three days and I'm just G to illustrate how this works kind of for and then it will make more sense how this works like for like a month so we don't have to do the whole cumulation thing but uh so what we want to do is we want to say metric name and in this case we want to say uh sum and then we want to say metric array at one right or and then we can say and then we we put this back into an array this is it's so weird like this is another thing this is another problem that I noticed that uh a lot of SQL uh syntax stuff doesn't work the right way for this so now you'll see and then we can say Group by Metric array so this query works right there we go so you see now how we have and then we have a month start here right so oh we gotta put it in the group by too month start so you see how now we have like this is all added up though like how this uh this is like we have one record here right so one of the things that you can do right is this part can be like you can do like a like there should be like a unest like function here that gives this should okay that worked so what you do is okay I know how to do it though so you have uh the array not unnested right and we're going to call this as summed array and we with um say a a call this an aggregate for now and then what we can do is we can say um select star from from tag we can say cross joint join unest and then in this case we have a do summed array with ordinality right okay I think it works this way with ordinality this is like this is getting absurdly fancy but I I I assure you that this is important why we have this like index here we're going to call this index though so if we query if we run this query you'll see okay so now we have this index here right and uh postgress is dumb and we have to do minus one because it does one Bas bye nice to nice to see you indexing but we need to add one day to our month's start so if we say um so we have metric name then we say month start plus there should be like plus interval can you do like one day or like is an interval in yeah we can say day index minus one is it like that what there should be a way to do that like add one day so the idea here is you can take this month and add one day to it like what is the there's does anyone know what the add one day function is like like like from an INT it's like you got like date ad or like I always freaking get this stuff like date in date part date out date trunk okay so problem here is uh remember it's zero based right so this is actually the wrong day so we need to do index minus one because postgress and okay perfect so now this is now we have our um and then we have um LM as value right and then this is uh okay there we go so now we have our date and we have you see how now this is back to um done the wrong thing I need to put that here and then I need to move that from there then I should be able to run that um uh daily aggregate right um but the thing is is like I want to show how this works really well though because of like how uh how like if we just load in one more day into the metric right then this pops in and then over here all we got to do is add it to the array here and then uh this part was like I actually ended up writing python that generated this there needs to be a UDF that essentially just does this sum for you but uh so you see now we have the fourth and that just added it uh very efficiently cuz I already added um um uh I already added um the fourth day just from testing so I didn't need to then redo it which I'm pretty happy with ah yes I saw that thank you um that's really helpful the thing is is like this explode here is like we we aren't like every user only has one record right and so we can sum everyone up and it's like very fast because we just sum them up in the array and we don't ever explode the array we only explode the array after everything is aggregated and so that's why like we have like one record for each metric name but that's where like in here when you do when you have this um this sum here this is where you could join in users to like get some other value right you could get like I don't know there's other values that you could get here to like explode out the dimensions and so that you would have more here but then you would have daily data with that dimensional value as well and so that's how you can go from monthly uh kind of array metrics back to daily Aggregates but it's very fast because you have uh it's the minimal set of data that you need and um that's uh like this idea like saved Facebook so much time and so much effort and energy congrats on finishing this 5H hour course doing all the Hands-On exercises and getting to theend end and sticking it out I'm really proud of you congratulations not very many people get this far if you really like this content make sure to like comment and subscribe and good job I'm excited for you to check out week three just thinking as well like um I \[Music\] just the um I'm just thinking as well like the way that Zach's done this is actually really smart like I'm I'm thinking from a Content creation point of view rather than a a data engineering point of view but he's factioned out the lessons into like you know what is it like six lessons one two three four five six so six separate um like it's all one video but six separate you have to go into different pages um to view the next bit and I'm pretty sure like I can't go find I'm pretty sure that that would classify as a new view each time so like he's cating all those views for like this the the various different lessons by breaking them out anyway I think that's pretty pretty smart like from a from a content creator Point View um that's just me just thinking about the like content creation side though uh but yeah no that that was that was cool um I I really like um one thing I really like about data engineering is how like you kind of you have all these different layers and you're just kind of building on top of them so like you you get your data into this format and that allows you to then get your data into the next format and that dat then allows you to get the data into the next form and it's like like it's like an onion like you just got layers and layers and layers that you add upon and I think that's really cool as as an engineer you just kind of slowly see it coming together um yeah I like that cool I like that one I'm looking forward to doing the homework actually uh I think I'm going to spend a bit of time just to understand it first rather than jumping straight into it so that when I do do the homework with like when when I'm streaming it is a little bit quicker cuz um I think that would be more beneficial beneficial for people who are watching anyway that is me today thank you for those who have stayed on and thank you for those who have interacted um but yeah um I am going to stop there uh and I'll see you next time don't know why YouTubers decided to stop responding for some reason um okay that's fine I am going to go yes data governance definitely does become difficult when you have layers but I would anticipate for the government governance perspective to still store that record of data as it was originally there but you wouldn't be querying on it like for analytics um for like the general um like the general like um doing daily analytics on it uh so I would still expect you to be storing that data just not um using it like for for analytical purposes um yeah okay so I'm going to go now thank you for everyone who has watched thank you for everyone who has interacted with me nice to see you and I hope to see you next time goodbye everyone and yeah I'm going to try and get like a little layer um overlay in my OBS studio so that it's a little bit more christmy for the next time now that we're heading into December so I'll see you then and yeah goodbye everyone


 > [!info]
> - **Data Modeling & Parallelism (2:31):** Shuffle is a bottleneck for parallelism in big data pipelines. Minimize shuffle for efficient data crunching.
> - **SQL Keywords & Shuffle (5:03):**  `SELECT FROM WHERE` (without window functions) is infinitely scalable. `GROUP BY`, `JOIN`, `HAVING` cause shuffle.
> - **ORDER BY Considerations (10:27):** Avoid `ORDER BY` in distributed compute, as it's the least parallelizable. Use it after aggregation on smaller datasets. `PARTITION BY` in window functions behaves similarly to `GROUP BY`.
> - **Reducing Shuffle (13:55):** Bucket data or reduce data volume to fix shuffle problems. Reduce data volume for efficient queries.
> - **Fact Data Modeling (16:16):**  Fact data can be normal, daily aggregates, or reduced facts. Trade-offs exist between data size and analytical flexibility.
> - **Reduced Fact Data (26:32):** Reduced fact data uses arrays to store daily metrics within a monthly or yearly row. This can drastically improve query performance.
> - **Long-Term Analysis (30:09):** Reduced facts enable faster long-term analysis, unlocking insights into trends over years or decades.
> - **Array Metrics Implementation (36:58):**  The lab demonstrates building a system using Postgres to create array metrics. Shows the complexities of managing the arrays and updates.
> - **Handling Missing Data (57:11):** When new users appear mid-month, fill previous days with zero values to maintain consistent array length.