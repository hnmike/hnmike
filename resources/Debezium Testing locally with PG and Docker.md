---
title: "Debezium Testing locally with PG and Docker"
author:
  - "Juan"
published: 2024-08-25
source: "https://www.youtube.com/watch?v=V_RjthHydKs&list=LL&index=165"
image: "https://i.ytimg.com/vi/V_RjthHydKs/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGEAgVChlMA8=&rs=AOn4CLBDdPlpPCOLSdPml4oIsxmsE7WmSw"
created: 2025-03-20
tags:
  - "youtube"
  - "debezium_cdc"
  - "docker_testing"
  - "kafka_postgresql"
summary: "Run Debezium locally for testing with Docker, Kafka, PostgreSQL, and a Java consumer. Capture data changes and stream them to Kafka. Easy setup guide!"
---
# Debezium Testing locally with PG and Docker

![Debezium Testing locally with PG and Docker](https://www.youtube.com/embed/V_RjthHydKs&list=LL&index=165)

> [!summary]- Description
> In this video, I'll show you how to run Debezium locally using Docker, Kafka, PostgreSQL, Redpanda Console, and a simple Java consumer. If you're new to Change Data Capture (CDC) or just curious about Debezium, this tutorial might help you. I'm still learning and playing around with these tools myself, but I'm really impressed with what Debezium can do to enable CDC in a straightforward and powerful way.
> 
> We'll walk through setting up everything step-by-step so you can easily test and explore Debezium's capabilities in your local environment. Even though I'm not an expert, I hope you find this guide helpful as you start your journey into CDC with Debezium. Let's dive in and start capturing some data changes!
> 
> Topics Covered:
> 
> Setting up Docker containers for Kafka, PostgreSQL, Zookeeper, and Debezium Connect
> Exploring Redpanda Console for managing Kafka topics
> Writing a simple Java consumer to process CDC events
> Tips for troubleshooting and learning along the way
> If you find this video useful, don't forget to like, subscribe, and share with others who might be interested in CDC and Debezium. Let's learn and grow together!
> 
> \#Debezium \#CDC \#Kafka \#PostgreSQL \#Docker \#Java \#Redpanda \#ChangeDataCapture \#LearningJourney

> [!note]- Transcript (Youtube)
> all right so I'm going to cover pretty quick on this video um how to run uh for testing divisum using postgress and um obviously for uh so division is is a this tool that allows you to um to do CDC uh which is uh capture data change in the in you know a bunch of different database it's an incredible Tool uh I'm just learning about it I started today uh but I find it really interesting uh so the idea of theum is that um you basically you have a you know you have a database such as pogress or MySQL or something else and uh let's see it's taking a little bit uh and and the idea the idea is that every time you you you know you wantan to Let's suppose you have something like a some some analytic database um like a Apache Druid or uh something like click house and you want to capture you want to get the latest records in order to do um in order to do some sort of enrichment data uh or anything you know it can be you know any use case that you have so the idea of divisum is that divisum kind of like register the sort of connector the sort of um process that listen inside your database so let's use postgress and so the idea of that every time there is a change right there is a there is a a capture thata change it would send that to to divisum um and what is going to happen is that divisum then will stream those changes into something like Kafka right and and this is pretty handy because from C Kafka you know you can send it to your olab database or to I don't know snowflake right uh it can be it can be to your data waterhous to your uh data Lake whatever that is you can send this from uh from divisum so pretty handy tool so in this video what I'm going to cover is um how to run this locally okay uh I haven't I haven't get to the part yet where I run on the cloud um you know my organization uses uh RDS in a WS we use posters um but I haven't I haven't get there yet so this is just testing locally so but the idea is that um you know I have uh this uh dogger dogger compos file uh where I I'm going to run cob Services as the division connector I'm running postgress I'm also running in uh this uh red panda console um and uh gafka inside this here too you know and uh probably something else and then so when postgress insert something uh delete something update something division is going to sort of pick those changes and send those to Kafka and then we are just going to validate through another Java consumer uh a c summer that uh you know the changes are uh being sort of you know inserted into Kafka properly uh from the connector right so all right so let's see um so let's let's see uh so in order to run this let me start by removing some of the stuff I had so in order to start division offers a tutorial uh gor REO so if you do here you can see that out of here you know you can get you can get this divisum examples there is a lot of different examples uh you know for postgress for uh different databases you know open shift etc etc and so by you get this thing here and then follow the uh follow the Remy uh inside the tutorial you know there is um there's a tutorial folder in uh somewhere here and there is different examples you know different doger compos files pretty handy to run this and learn how to use it um so like I was saying you know we have um we have this uh Docker this postgress Docker compos I updated a little bit uh in order to include um the red panda console used to see you know what was going on inside the Kafka cluster and all of that red panda is also another great tool that you can use um you can get it right here red panda uh ddata console in giop it's a great project so um you know it will let you it's a UI for that streaming pretty much so it kind of looks like this this is my local host and I'm going to walk you through this in a little bit um so like I was saying go to the you can visit the division website and learn more about the connector you know some of the uh constraints some of the important details that you have to have in consideration when connecting the visium uh to uh postgress and you know it's sort of it's a very extensive documentation really deep excellent is this is one of the best documents I have ever read um super Wellcraft um okay so with this docker compos file that is going to run um postgress is going to run sueper for Kafka um it's going to run Kafka the console that I was telling you about the red panda console there is postgress and then there is the division connect right we're going to run this we're going to run this by used um just going to use the latest version here and I just tear down so we're going to run it and it's going to run a bunch of different things all of the stuff that we have defined inside the um inside the docker composer compos file something to know here I had to add I was running into some issues so I ended up uh adding these networks here afka Network to each one of the services and also like defining that uh using the um Bridge as a driver and you can see I had to list it for each one of these uh I guess while I keep playing with this and troubleshooting and stuff I would figure you know what what was going on but uh this was just to get it working the other thing was um let me see I had to define the kafa uh adverti listeners in order you know for clients to connect to know where to connect to kfka and uh you know the c c listeners here too so I had to sort of update um some of these things some these details uh that you might have to dat it too you know so try to follow uh pretty much you know try to grasp uh this details on the video to to get it to work um all right so with that you know we we just run the uh doger compos app it run all of these Services right and there is a lot there and we can directly just go to Red Panda right so we can go here the cluster is running we see the status of the cluster here and we can see some of the topics so let's delete this one um it it will usually with this setup that I just this doer compos it will look just like this right we just have uh my connect config pretty pretty simple nothing relevant just yet okay so the next thing I'm going to do is I'm going now to provision that uh division connector um to uh be able to you know create to be able to create the CDC um connector right so every time we insert something into a database every time we update or we delete something you know division is going to change it's going to stream those changes into Kafka and then from there we can subscribe and consume from Kafka and get the messages so uh Coral is this tool for uh it's an HTTP client on the terminal uh we just going to send uh you know uh this request this post request to uh this endpoint connectors this is offered by the division API internally and we're going to call uh this register po post so if you see this file it's also it is also provided in the um in the uh tutorial uh GitHub repository folder so you're going to see that you know we're going to use the connector which database is pointing um username password DB name yada y something here very important is the schema uh and the also uh you know if you change in your doger compost any of your uh you know username password you also want to date that in here um so okay with that so let's run it and the moment we run it you know it's going to say that you know should say it should see that right so I created with a 2011 uh and you know we can check the uh we can tell okay now give me back the connectors and there is that example connector defined on that uh you know uh the name that we Define right here so you can change it and you can also get the status by calling the connector name and then the slash status and then we tell you uh running and then you can see some of the details about the worker ID you know to uh for some of the internals of this division stuff all right so with that once you go to uh rep Panda now with there is a little bit there is more topics uh you know there's new topics created and you know this is you know so every time we sort of work in any of these databases um you know you're going to get different messages i' I've been already playing with it so um right now it's just going to stream back some of this data uh all right so part this is sort of like the um I guess the uh first part of this um so basically you know we set up the visum there is CA running right uh and uh we got the or postgress database running there you know and some of the topics for Kafka have been created and now you know we're going to connect we're going to consume uh out of Kafka uh using this Java consumer here right so this consumer is going to register there just right now it's pretty simple you just prints uh new messages right so for that um what I have created here is um there is this uh so there's this Kafka consumer um that it connects to the host uh the docker this is I'm using Mac uh but this is also available I think in Linux uh I'm not sure in Windows but probably it is available too um so basically this map to whatever Docker IP you know it's for IP resolution and uh you know different networking stuff to be able to connect and then uh this is the actual um table or sort of you know uh topic that I'm interested on listening so I'm going to subscribe to that uh you know specific topic here and every time there is a message you know so I'm just going to do uh run this forever and you know just pull P do a poll to get the messages you know and every time there's a message it's just going to print it on the terminal so uh it's just a pretty simple way to connect uh from outside um and uh this is using Maven um there is uh you know the gafka connector here uh the map the gafka dependency to connect from java um um and uh there is also Docker file that will let us run this project you know inside Docker so um I'm I am already running this uh but the the way you compile that is by uh you know you you build it and then uh you know you just run it something here is that because we have uh those um those networks created defined on the um on the doger compos file for the for this part one you know setting up Kafka and postgress and theum um you know you have to pass the specific Network that uh you know you are going to be connecting uh through this so this is the actual um dogger email after you compile right it will it will compile into your consumer app uh dogger uh file and then you connect to it right you run it you run that uh container you you you boost Tru um you know run this container so uh attached to this network okay so all right so we have we have everything now we can start producing messages I'm going to use uh this example you know to send messages to the uh make sure you have that schema selected it's called inventory right you know there is a public schema which is kind of the default um so if we insert something right you're going to see that the message you show up uh so Java here the Java consumer uh is connected to Kafka and now it's receiving the message so every time we do an update in postest right we insert something right or we update something the post connector is going to start uh sending that division is going to read those messages and it's going to sort of send those to um to the CF Copic and there is also um pretty handy the red panda um console it's pretty cool for that too so you can get into the topic and you can see you know the messages that are uh getting through you know there's a pay low uh portion down here on this Json message where you can see the specific data um you know that is that is coming by uh and it's telling you you know what what was there before you know and uh what change was made um you know to to days so for example let's do this to see now uh right so it stream something there and now we should get a new message um down here the payload that it should capture sometimes the data I seen it you know it doesn't stream all the time um but yeah so so this is it I guess um but you saw pretty simple you know how to download the um divisum um the divisum uh repo you know with the examples um you know sort of how to where to find that documentation for division to connect to postgress um you know how to use R Panda uh this console is super handy uh you know it's you know it helped me troubleshoot things uh really quick and this Java consumer that we can connect to it you know and sort of uh sort of get the messages that are coming through um yeah I mean this it let me see um any other particular details I guess something here um initially when you're using um when you're using this slf uh 4J uh this Java logs thing uh it produces a lot of a lot of messages so you might want to you know set up this logb back file with only uh certain information you know and Silent some of the things so it doesn't you don't see so many um events coming through uh so much uh data coming through and uh what else I guess is relevant here um yeah make sure you connect to the right uh you might not see it the right schema you know and you're going to see the tables depending on what database client you use you know and and uh right play with it so you can you can get uh you know different messages um and uh yeah so hopefully this was uh helpful for you and uh it sort of help you also to get a starter testing Division and um yeah hope uh hope this um bring some uh Clarity for those that are starting just like I am um using some some of these uh data uh tools um all right see you in the next one uh take care


 > [!info]
> - **Run Debezium locally using Docker:** This tutorial shows how to set up Debezium for testing in a local environment. (0:02)
> - **Tools used:** Docker, Kafka, PostgreSQL, Redpanda Console, and a simple Java consumer. (0:02)
> - **Debezium:** A tool for Change Data Capture (CDC) that streams database changes to Kafka. (0:17)
> - **Data Enrichment:** Use Kafka to send data to OLAP databases or data warehouses for enrichment. (2:00)
> - **Docker Compose File:** Includes services for Debezium, PostgreSQL, Kafka, and Redpanda Console. (2:45)
> - **Networking Configuration:** Adding Kafka Network to each service in the Docker Compose file and defining Kafka advertised listeners. (6:47)
> - **Red Panda Console:** can be used to get into the topic and you can see you know the messages that are getting through (15:04)
> - **Java Consumer:** A simple way to connect from outside and get the messages from Kafka. (11:32)
> - **Schema Selection:** Make sure to select the correct schema when producing messages to avoid issues. (14:17)