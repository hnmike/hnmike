---
title: "DataExpert.io - Day 1 Lecture & Lab - Data Engineering Bootcamp"
author:
  - "Jade Codes"
published: 2024-11-20
source: "https://www.youtube.com/watch?v=ppBq0amchiI&t=9s"
image: "https://i.ytimg.com/vi/ppBq0amchiI/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgZShlMA8=&rs=AOn4CLCsoZxPQJLZe-qMkzWvz3jfu98jYA"
created: 2025-03-23
tags:
  - "youtube"
  - "Data_Engineering"
  - "Data_Modeling"
  - "Cumulative_Tables"
summary: "Lecture and lab on data modeling principles, covering OLTP vs. OLAP, cumulative tables, and run-length encoding. Day 1 of Data Engineering Bootcamp."
---
# DataExpert.io - Day 1 Lecture & Lab - Data Engineering Bootcamp

![DataExpert.io - Day 1 Lecture & Lab - Data Engineering Bootcamp](https://www.youtube.com/embed/ppBq0amchiI&t=9s)

> [!summary]- Description
> Thank you for joining me on my Data Engineering Bootcamp journey!
> 
> I'm so excited to have you here as we work through this incredible free bootcamp together. A huge shoutout to Zach for creating such an amazing resource!  
> 
> Before we dive in, let’s go over a few quick house rules:  
> 1️⃣ If you’re also following along with the bootcamp, make sure you check out the official site at:
> https://bootcamp.techcreator.io/ 
> 2️⃣ Don’t forget to watch Zach's videos directly too! He deserves the watch hours, and you deserve the credit for completing the bootcamp.  
> 
> This bootcamp is completely free, but I also want to let you know I’ll be joining Zach's paid bootcamp in January. 
> 
> If you're interested in leveling up your skills even further, you can use my link for 20% off: 
> https://www.dataexpert.io/jade
> 
> Thank you again for being here, let’s make the most of this bootcamp and learn some amazing skills together!

> [!note]- Transcript (Youtube)
> for hello that that all \[Music\] hi um I'm just going to do a lecture first just to see how it is going to do but then I will then do a live stream of the lab afterwards once I've done the lecture um so the idea is I just live stream everything that I'm doing basically um and then and then see how if people like that how's the audio today is the audio better it's still really bad I've plugged in my my headphone at the moment I've plugged in my microphone so hopefully it's better um but I just want to check that it's okay for other people \[Music\] okay so thank you everyone for joining me so this live stream is going to be watching the data modeling lecture um if people think that I should just do the labs rather than the lecture okay um there's still some Echo okay um let me just see if I can sort out the echo um just going to watch this okay well I think that's the best I'm going to get it so hopefully that's okay right so I'm just going to watch the diamond data modeling lecture today I don't know if everyone's caught up with the actual um like kickoff event but that really didn't go into any detail it was just basically uh showing you what what to expect from the boot camp if you join the boot camp but it wasn't any like technical content so I didn't buy the the live streaming that because I didn't think people would find it very useful can you hear Zach I have to watch things at like 1.5 speed because it if I don't I have to watch things at like 1.5 speed because if I don't then I will like get really really bored and that's that's nothing to do with Zach at all it's just to do with any sort of lecture that I watch I like find it really really hard to listen to something when it's not at like a fast pace e just checking something um okay see this is like a user ID or it might be something like a social security number or it could be like I know there's all sorts of ways to uniquely identify things like maybe a device ID if you're tracking your phone it could be another great place where you can see different device IDs that can go on uh then you have uh you know what I I attributes and attributes uh they aren't really part they aren't critical to the the the identification but they help you know do other sorts of analyses or all sorts of stuff like that and these attributes generally speaking come in two flavors the first flavor is slowly changing so I'll give an example of a slowly changing Dimension slowly changing dimension in this case is going to be like my favorite food right like back when I was a a child right uh my my mom would always cook lasagna and I love lasagna I thought lasagna was so good and like that was my favorite food for a long time um but then like when I moved to the Bay Area I realized that I actually really like spicy food I like I like f it just like slaps me in the face you know and it's just like wow like it should be like like my favorite foods now are like a little bit painful to eat because I like that pain I like to I like to suffer while eating my food it's very delicious so anyways what I'm saying though is that my favorite food changed at one moment it was lasagna and then it was like a spicy curry so and then maybe later on I'm going to discover another food and it will just keep changing and keep changing and so what that means is this attribute is time dependent and since it's time dependent uh it's slowly changing and like uh that's one type of so one of the things that I've not this is before I've um watched ja before but I have worked with and dimension tables before and one of the things I always try and think of when I think of is something a dimension table is is it something that isn't going to be changed very often or very frequently that but it is going to be queried quite a lot um that's what I I think of and similar to what I think um uh Zach saying um like in my old in my old company we did this reports table um and one of the things that we were reporting on a lot was um events but um but the events you would typically find that you wouldn't have a lot of different events like but there would be in a lot of different queries so that was in in that particular example uh Dimension and then the other type Dimension is fixed uh a great example of a fixed Dimension is my birthday right uh I can't really change my birthday that's like kind of stuck right it is the day that it is I was born at the time I was born do I use Windows yes yes I use Windows I I do code I do use um WL um for when I work but I use Windows so I can't go back and like change that right there's there's other kind of other fixed Dimensions that are the case like for example for a phone right the the manufacturer it's like this phone was made by Apple or this phone was made by Samsung like they can't just like go back and destroy the phone and like recreate the phone and be like okay this phone is now not made by Apple like it's it's a fixed Dimension right you can't change that Dimension it's like set in stone so that's one of the things that's interesting when you're modeling Dimensions is knowing whether they're fixed or slowly changing obviously fixed dimensions are easy they are just one value that never change so like they are a lot easier to model slowly changing dimensions are uh kind of a pain they're of all of the dimensional data modeling problems they're probably the the most annoying to deal with so keeping in mind that those are the two different types of dimensions and we're going to be going deeper into this this is this this uh whole lecture gets way deep so let's get going okay so we're going to be covering six topics today and some of these topics we are going to be doing in the lab as well as we go more into that kind of postgress stuff I hope most of y'all have that set up if not like we're going to have like another uh time tomorrow to get people set up because we really want you to get set up on postgress at some point in the next couple days so that you because we're going to be using the same post stuff for next week as well so if you can't query stuff in that post instance that's going to be a big blocker for you to get so if anyone's um like still not got their environment set up I really recommend that if they are going to use po um they use Docker to do that rather than installing it on their machine it's so much easier I think to get set up and to get running um I actually like added on GitHub if I go to GitHub uh where is my repositories I don't want to show you all my repositories but um uh if I go here I actually added PG admin as well to my docket compos file which I thought was really useful so I definitely recommend going the docker compos route so much better so much easier to to work with rather than having the um having to use like it on your machine I just think it's it keeps your environment clean and and I really like that a lot of value out of the first two weeks here um so first thing knowing your data customer you have to have empathy right we're going going over that we're going to talk about oltp versus oap like it's transactional versus analytical processing then we have cumulative table design massively important we're going to be going going over cumulative table design in quite a lot of detail in the lab today uh then we have the compactness versus usability tradeoff which is an important consideration when you are doing your data modeling exercises uh then we have a very fun one here called a temporal card expion can defitely when you're mod Dimensions that a temporal component so out of curiosity for people who study what is your like favorite way of studying do you like have um do you have like not a notebook that you have online or do you are you like me and you you have to write stuff in a notepad to learn I found like um found like I've tried loads of different online versions so like one note and and notion anything like that but nothing beats being able to draw and write down my thoughts as I'm doing it component to them we will go into that and how there's a various different ways you can model that and then the very last thing we're going to talk about is run length en coding uh runlength en coding is one of the most powerful ways to compress your data in the Big Data World especially if you're using what's called Park file format which is the standard across industry now so we will be going deeper into each one of these and uh I hope youall enjoy this presentation as we go deeper into this all right one of the first things you got to know about when you are modeling your data is who who's using this like who who's going to be actually trying to get insights from this like this is the part of data modeling this is one of the things where I think that data modeling is like one of the will be one of the hardest things for chat GPT or llms to figure out because of the fact that it has this kind of exercise and empathy of understanding okay am I delivering data to an analyst to another engineer to a model to a customer to an executive like what is this data and how are we going to be moving it around right and how are we to beining this data and so we're going to go into each one of these real Qui so with data analysts and data scientists this is where you want to have data that is I think I think this is a really key point that um Zach touches on here about knowing who it is that you're modeling for because one thing that I've noticed like quite often is like um or what I've seen is where people just collect loads and loads of data and then they they just it's just too much information for people people are just like well that means nothing to me what what does that data mean and you have to be able to find a way to create a story with the data that is that that you've got rather than um just uh providing loads of data because that data doesn't really mean a lot of to people who aren't like data oriented so finding ways to tell a story and giving the data that is actually needed or like wanted by um your audience is really really important pretty easy like you should have mostly uh decimal numbers uh maybe a string data types it should be very like flat like all the columns should be very easy to work with you can just easily sum or average or do whatever you want to do with these columns you don't want to like make the data analyst or data scientist job hard with these data sets so that's going to be the important thing when you building analytical data sets and a lot of times another name for that is like called like an olap Cube and we'll be going a little bit deeper into that in another slide sometimes like your Downstream consumer are actually other data Engineers that are going to take your data set and then join it with other data sets to make don't you think there's just so many acronyms as well like in like the industry like I'm like what is what is do that mean there's been a couple I've seen when like I'm just like I need to research the actual meaning of that just something um just another another acym you have to remember this is the case especially as you get further up the ladder in data engineering you will start to develop this stuff called Master data and Master data is data that other data engineers and other people in the company depend on and then there's like many many pipelines that read from your data and produce other data sets and the these data sets are going to be different and we are going to be producing one of these Master data sets in the lab today and a big thing to remember here is that using nested types like structs and arrays is okay don't worry about it like because that's the whole idea you're working with data engineers and they're going to be able to understand how to work with your stru and eras because that's part of their job is they need they understand how to work with complex data um obviously there's other types of uh consumers another big here is a machine learning model um machine learning models generally speaking they want the identifier and then a bunch of flattened decimal columns like the feature value columns they're usually but they might be categorical values too but usually they want it to be pretty flat but it depends on the model some models can have you can feed them more complex data structures but most models they want just like identifier and then flat primed types that's what they want so it's kind of similar to what data analyst and data scientists expect but you probably don't have to be as concise about the naming conventions of things because a lot of times the model doesn't care what the name of the column is and then obviously the last one is custom customers you shouldn't that you you know you never want to give like a customer like an Excel spreadsheet you want uh the customers to mostly be uh uh looking at charts and looking at uh geometric patterns and you really they you shouldn't be serving your customer like data data modeling stuff right that's not obviously there are some small edge cases there like if you're working in some sort of analytical product and you know that your customer is also an analytical person then maybe there is some cases where you can do that but generally speaking you should give them a chart that's easy to read that has a lot of good annotations in it and if if you have that that will really fit the needs right that's one of the most important things to remember when you're doing data modeling throughout all of these exercises that we're going to be doing over the next two weeks is you have to understand how the data is being used because if you don't like you're going to mess up you're going to mess up you're going to cause a bunch of expensive mistakes and that is going to cost the business money or you know cost a lot of time or whatever there's going to be a lot of issues if you uh don't think about the downstream users so yeah let's go to the next slide so I'm curious if anyone has done a lot of data Engineering in the past like um what um mistakes have you found that you've encountered by not following like this advice and not having a clear idea of who your audience is okay so there's three types of data modeling uh for the most part when I think about it and they kind of exist on a Continuum and keeping in mind here we're talking about dimensional data modeling uh fact data modeling is slightly different and we will talk about that later but so you havep which is your online transaction processing uh these are going to be uh mostly outside of the realm of data engineering this is how software Engineers do their data modeling to make their online systems run as quickly as possible this is where you're going to run into things like third normal form and minimizing data duplication and you're going to have all sorts of like Linker tables and primary key um just general day-to-day for for like users and consumers where um you sign up to a site and and you're using the site whereas online analytical processing is putting your data in a format that can then be used for things like reporting and Gathering um like big insights into different areas I think I think that's what he means um on the other side you have olap or online analytical processing this is the most common uh data modeling that data Engineers use and this is mostly used for like you're not you're optimizing for something completely different like we're not talk like we don't care as much about data duplication we mostly care about can we run a query that's fast and we don't have to join a bunch of things together because when you're doing like a big join between two tables that can be very slow and that's where if you model the data the right way then maybe you don't even ever need to do the join and you can just group right and because olap is always going to be looking at the population it's looking at the entire data set or or a subset of the entire data set but usually like a good trunk of the data set whereas olp is looking at like one user and one person right it's already filtered all the way down to just like one entity so that's why like these two ways of data modeling are different and they have their benefits and their strengths and their risks and they uh they both work but like if you try to do them uh if you try to model like your transactional systems like your analytical systems or the other way around you're going to have a bad time and that's why you kind of have to have this middle ground and like I call you know this Middle Ground is called like the master data middle ground and uh in this case you still are working in the area where you want to have your entities deduced and you want to have uh very complete definitions for your entities and that's where the master data kind of sits in the middle right so you have your transactional data that is like very uh kind of split apart into a bunch of different tables then you have the master data layer and then you have the olap layer and those those layers kind of like all work together and if you model like if you have a master data table that you model as an olap table you're going to have some problems as well and so we're going to be working um in the lab today on like how to model A Certain Master data table that will kind of combine some of these things together and I think yall will like it so yeah so I think I think what you're saying is what I was thinking um I remember like before when I worked at the my previous company we had to model um the report in very very differently to the way that we were inputting data into the actual system and it was all due to it was all related to the fact that like quering the data is very different to actually like processing the data and storing the data and um when when we wanted to actually do the reporting it was we we we didn't necessarily care about having that um like uh like strong consistency see it was all about having the eventual consistency for the reporting side there's a Continuum here we're going to keep talking about this but first let's go back to mismatching needs right so in the case of uh say say you are going the other way where you don't like you have a transactional system and you model it like an analytical system so you optimize for the um the population but you really only need the single row your symptoms there are going to be your online app is going to be slow like because it's going to be pulling in all of this data that it probably doesn't need because it's um it's modeled where it has all these extra columns like that are probably duplicated across tables and that could be a very painful thing to see but on the other side right if you do it the other way where you just take your transactional system and dump it to the lake and then you're trying to model uh your analytics like your transactions the pain that happens there is different the pain that happens there is what in Joins where you're going to have a lot of joins that happen and these joins uh will if you have to do all these joins for every single analytical query that's going to just be very expensive and you're going to have a lot of Shuffle and a lot of pain and a lot of frustration and so that's where having good Master data can be a very very critical role for where you can match your needs to the right spot and like you're not trying to force one layer into the other layer and you have this middle ground that gives you essentially uh the agility to go wherever you want right you and that's where Master data and that completeness is really really powerful I think as well this like what he's saying here is why it's so important to know who your users are because if you don't know who your users are and you don't know what the format of the data that you're providing needs to be in then you can't create those like um the data and or do the data modeling and the way to get that efficiency um so by knowing who your users are you knowing what formats your data needs to be in that's where you can then do the data modeling to actually um be like have those efficient queries um so here's the Continuum that I was talking about so at most companies they have production data because they have like an app of some kind like for example I worked at Airbnb for a while they had an app and uh inside the app like you have all datas around like hosts and guests and listings and prices and availability settings and all sorts of different like you know Dimensions right and all those dimensions are modeled in a transactional way and what one one of my jobs was was to take all of those transactional datas and merge them together to build master data that made it easy to understand pricing and availability at Airbnb so that's where uh I built more like Master data which is like taking all these kind of production data sets that are like you know I I remember like for that for that pipeline it was like 40 there was like 40 transactional data sets that all came together to create this master data set which was one table right so that's a that could be a big difference like where instead of having to query imagine like if we didn't have the master data layer there and then like for the analysts it was like oh you want to learn about uh pricing okay just join these 40 tables together and uh good luck right and so that's where you can have a lot of value as as a data engineer and um but it wasn't an olap Cube either because of the fact that like it was still normalized and we're going to talk a little bit more about that in another yeah but I had had to have a bit of a chuckle that just because I remember like when we were using um data modeling at my last um there there was about 17 tables that needed to be like combined to to to produce some uh more efficient reporting and it was really really slow like before we actually did the the um the changes and yeah like you just end you could just end up with loads of loads of drawings so it it it wasn't very pretty at before before like doing the sort of Designing to to to um to make a report in um like table for where it was still duped and normalized but it still had it was very complete so then you have olap cubes right which is where you kind of flatten the data out and you might have multiple rows per entity at this point and that's when you can then Aggregate and do like group buys and like figure out like oh buy country by age group by you know device operating system or you can do all like Ola cubes is the area of like another word that is often used in analytic spaces slice and dice and if you could do slice and dice then that is where you're going to really live a lot is this olap Cube olap Cube cube is the the space that data analysts and data scientists they love it when you give them an olap Cube because you can they will be able to do whatever analysis they want very quickly and then uh you have one more layer here which is metrics which is essentially you take the olap Cube and you aggregate it even further down and then you have like one number right which you can think about it this way where it's like you take the 40 transactional tables you have the master data that describes price and availability and then the olap cube is a a flattened uh table that describes all the listings and all their prices and then you can imagine aggregating that all the way up where then you have one number which might be like the average listing price for all of Airbnb and then you you from 40 tables to one number right and it's kind of like where you distill the data where it's like you have like all this data that's like all over the place and it's all messy and then then you com then you put Humpty Dumpty together that's what uh the master data does and then then you kind of split them out into as pieces like but but now they're all like they make more sense and that's the olap Cube and then you smash those pieces together and that's how you get the metrics so those are kind of the like the four layers of data modeling that are very common in uh data engineering and like if you can understand each layer here in dimensional data modeling you will have a much better time as a data engineer because this is a very common pattern that you'll see over over and over again so yeah let's go to the next slide this Continuum like I definitely try to let this sink in and come back to this um so we're going to shift gears here a little bit we're going to talk about cumulative table design so one of the things that can happen when you're building Master data is uh some days not every user is going to show up because maybe they're only active once a week or something like that but the master data should still hold on to that user you should have a complete history that's what cumulative is all about so if you have a cumulative table what it's all about is holding on to all of the dimensions that ever existed right up until a point you might need to filter out users that are like deleted or like hibernated or something like that and and we're going to talk more about that we're we're definitely going to see like how that can be uh used um uh so what we're going to do is like so cumulative tables are all about holding on to history and if you're not holding on to history then like that's not cumulative right and so cumulative has two the way that cumulative works is you have two data frames or two data tables you have today's data table and yesterday's data table and those two data tables come together and you full outer join them and the reason why you full outer join is because uh there might be data in yesterday isn't in today and there might be data in today that isn't in yesterday so then you can kind of merge these together so that you can uh get the whole set and then uh then you need to coales the values because uh they might match or they might not match depending on uh like if they if they exist in both tables or not and uh then you do this coales and and then that's what how you hang on to all of history and if this doesn't make a lot of sense yet don't worry we're going to be covering this in the lab uh one of the big places where I saw this in my career was at Facebook doing growth analytics so they had a table called Dim All users and what that table was was it looked at every users activity every day so that we could see who was daily monthly weekly active and we can like pull in all those users and we can like look at them and we can pull that history forward every day with like full aage joins and it's a very powerful pattern um another place that this happens is uh State transition tracking like for example this is another thing that I learned a lot at Facebook was they had this thing called growth accounting so how growth accounting worked was you have today and yesterday right so if a user was active yesterday but they're not active today they're churned right they churn and but say they were um not active yesterday and then today they were active then that's uh resurrected but then you have other ones where maybe they didn't exist yesterday and then now they're active today and that's new right and there's all sorts of different like transitions that can happen from today or yesterday to today and that can be um modeled and those patterns are something that we are covering um in very uh very intense detail in the analytics track like we have a one thing I always think of like when I think of like how on when I think of like companies like Facebook and stuff it's just you realize how much data they actually store in you and how much like how much information that actually gets stored and and everything that you do is is being trapped basically um from you know whether or not you like went on to the application from whether or not you um like clicked a particular button there's so much data even like if I go on to my Facebook I can get so much information about what I've done um over the last um you know I can retrace all my steps from you know page I've visited and yeah that that that's when you when you think about it it's quite creepy to think about just how much data they actually do have on you a week on applying analytical patterns so definitely stay tuned for that if you are in that track uh it should be a pretty great time this whole cumulative table design is very common to for creating Master data that's like so you see this Dim All users table that I was talking about like that table at Facebook had 10,000 Downstream pipelines right so it was it was used not just for like managing you know the immediate growth analytics but it was also used as the source of Truth for all user data like all users in the in the company and then it's like if you needed to join to the user table you use dem all users like and that's what everybody used right and that's like where you can get a lot of value out of these Master data tables is that way so yeah but yeah like definitely uh you should only be using two data frames uh for doing this cumulative table design because the columns in those two data frames should be essentially the same they should have like all the same columns where one is just yesterday's data and one is today's data right and so that's how you can keep building the stuff up and so that's how uh I would definitely recommend uh like doing this design there's more uh in the next SL so if I understand this quickly um like you you out jooin from yesterday and today but yesterday's one has out a joined from the previous day and yesterday and and that's how you cumulatively add more and more to the um the the the table um so yesterday's technically is actually the previous days and the previous days and the previous days and the previous days until basically the start or until of which they've decided they don't want to stop information from um I think I think if someone if that's wrong please correct me but I think that that's what he's saying Li so there is uh here's like the kind of diagram for how cumulative table design works right so you have today and yesterday so today is just like we're only looking at the data for today and then yesterday might be uh all of like the cumulated history up until that point or it might be null right so yeah yeah I think I I jumped the gun there to to say um it was that because um he's he's basically confirmed that yesterday might be null if you're just starting if you if you're starting cumulation today then yesterday will be null right and it will just have no no values obviously like how many users you hold on to and like if you're going to hold on to users for a long time and then like they're stale and you have all this data like you know Facebook has a couple billion users a billion active users but the number of inactive users they have right is like tens of billions right the number of people who have churned on Facebook who have made an account and left right so you don't want to like and a lot of those users are never coming back so they're not like really relevant to your analysis so there is a point in this cumulative table design where you do want to like like get rid of people and kind of prune them out like there's got to be like some sort of line and I think at Facebook like they pick like if the user hasn't been active in 180 days kick them out of the table right and that's essentially like but that all depends on the company and the business and the requirements of like what your compan is looking for so like that like and how to make this efficient like and how when you're pulling all all of history forward because that means that generally I guess one thing that's interesting is what do they Define as an active user so for example I have Facebook look on my phone but you know um what do they like determine is an active thing like if I like you know swipe a notification or if I click on it what's what's the actual attribute for for someone being active speaking this table gets bigger and bigger and bigger every single day right that's the general trend of this but like uh you're totally right that there needs to be other things involved here like filtration things or something like that if like otherwise these tables can get kind of unwieldy definitely so one of the things about this right is we do full average join and then we coales the IDS right uh so because yesterday and today we'll have a user ID um they might not one might be null the they both might exist um if someone was uh in both tables they might only exist in one or they might only exist in the other and that's where you coales and that's how you you get back to just one ID and we'll be going over all this in the lab so don't worry about it too much then we need to compute the accumulation metrics right the thing that's nice about this is if you were holding on to all of history you can see things like okay how long has it been since they were last active right we could be like oh it's been seven or eight days since they were last active great and then you can increment it by one if they still aren't active and you can do all sorts of interesting cumulative when you have these tables and these they're on the same row uh and you can do all sorts of really cool things like that and also you have this collect functionality that you can do where you can keep adding new things to like an array of values or uh different changes computed stuff we'll be going over a lot of that in the lab like if this doesn't make quite a lot of sense quite yet and then at the end of that you have the accumulated output and the accumulated output of today will become yesterday's input like so for tomorrow it will be yesterday's input right essentially feeds back into this and so that could be a very that's how yesterday is defined um and we'll be going over the kind of the pros and cons here for the Cula table design so cumula table design let's talk about the um uh strengths so you get this amazing ability to do hisorical analysis and you don't even need a group bu because you can collect all of the history of a user as they keep going on you can see like when they were last active or you can have like their last 30 days of data in one row and it's just an array and then you can see exactly like oh like they were active eight days ago right and you don't have to use group ey because it's all in the table it's just a select so you can do a massively scalable uh queries when you use this and it's on historical data which a lot of times that those queries can be very very slow because you like say you're like oh has this user been active in the last 90 days if you look at the daily data you're going to have to query all 90 days of data and that's going to be a very expensive query and you're going to have to group buy and there's going to be Shuffle so that's going to be a very slow query if you query all the daily data but if you move to this cumulative table design you just select from the latest date and you can get all this data directed and obviously I was talking about the transition analysis that's like that churned resurrected like active today inactive yesterday like uh all those different ways that users can transition from one state to another and you can easily do those so I assume based on that though you would take the like last active from the latest table um rather than from um when you're doing the join you would take the latest one rather than the previous the previous one analyses with cumula Pap design now let's talk about the drawbacks because this is not I'm not saying that this is like the the Silver Bullet solution to all of your dimensional problems because it's totally not uh one of the things is is like because it relies on yesterday's data when you backfill you can only backfill um each day after each other you have to do um today then tomorrow then the next day then the next day which is not with with daily data you can run all of them at the same time and you can backfill them all in parallel and so that's one of the things that can be very painful about the cumulative table design is that since it relies on yesterday's data you can't backfill in parallel so that's going to be a very painful thing when you're looking at and then obviously a pii can get kind of messy where like if you uh you have to filter out those users you have to be intentional about removing the users that have been deleted or inactive for too long so you have to bring in another table a lot of times to like filter those users out be like oh this user was deleted so we have to we got to remove them from the table and so that like that can be a mess as well like handling that kind of pii mess okay so let's talk about the compactness versus usability tradeoff so there is a trade-off here where um and a lot of this goes back to like olp and olap and the differen is there but the most usable tables are straightforward they have identifier for the dimension and then they might have like uh some strings or some uh decimal numbers or some integers and it's very easy to use where and group by in them awesome great and then on the flip side you have the most compact tables the most compact tables are going to have like maybe the identifier and then just a Big Blob of bites because what they're trying to do is they're trying to compact the data down as much as they can and they use like compression codec and you can't even read the data at all like you have to use you have to decompress the data and decode it in order to do that and uh for example at Airbnb when when I was working there in like pricing and availability the um in the app they actually send you down like for the calendar the availability calendar they actually send you down um a very compressed data set that has to be decoded in the app and the reason why they do that is they want to minimize the amount of IO for you when you request that data because the network uh IO is going to be one of the most expensive pieces there and then your app can just decode it and then display the calendar to you but for an analytical purpose it's very bad because you have to decode the calendar and then you're going the if if the analyst has to do that decode every single time that's not going to work and so like I had to build my own tables that were essentially that decoded calendar and then there's a middleground here between the most compact and the most usable which is where you use um arrays and maps and structs and you're able to kind of crunch the data down a little bit but it's a little bit harder to query and so you can have all three here and they all have their own use cases where you can kind of think about the very usable tables are going to be more analytics focused and then the very compact tables are going to be more software engineering focused and more like uh production data focused so those are going to be the kind of the big uh differences for the compactness versus usability trade-off and who the consumers are so the most compact is all about online systems like if you're serving an app that has thousands or millions of users then you want to minimize data as much as possible that's going to be a very important thing um and then on uh the middle ground is kind of that Master data layer that's where you can really use a little bit more uh complex data types like strs and arrays and then most usable is that olap um Cube layer that like analysts love to work with and that's going to be your most usable and like each one of these has its own place like the one thing I want to talk about a little bit here is that middle ground layer if your Downstream consumers are other data Engineers who are joining your table and then creating more Downstream data sets that is where you're going to uh like really benefit from using these more complex data types at least in my experience I've seen that be I've seen that ring true um Okay so we've talked a little bit about struct and array and map right so let's uh let's talk about the um kind of the trade-offs of these right so struct is essentially where that's like almost like a table inside of a table that's the way I like to think of struct sometimes because it's you you have a list of of keys and values but they're all defined and uh for me I think it'd be interesting to see how like um at the moment it's quite uh it's quite abstract but it'd be interesting to see like the like performance like trade-offs between having something very combat packed versus having something that very usable um yeah that would be interesting to determine one of the things that's nice about structs is that the the keys and values the data types of the values can be different uh that's one of the things about map that is is hard right is for a map the all the values have to be the same type they have to all be the same data type and that can cause all sorts of weird casting problems like if you're using map but map has some other things that are nice in the fact that the keys of a map don't have to be determined like you can have n number of keys you can have an infinite number of keys in a map well not technically infinite or there like there some I think it's like 65,000 Keys it's like some depending on like what uh framework you're using it's usually like either 65,000 or 32,000 or some sort of like power of two number of keys otherwise like you can't have more than that but like you can have a lot like you have you can have more than you would ever need um and then obviously array array is very good for order data sets like if they in some sort of list and then uh inside the array each thing in the list does have to be the same data type but that data type could be a map or that data type could be a struct so you can actually Nest these where you can have array of struct or array of map and uh you can do all sorts of different kind of modeling exercises like that and we uh in in our um lab today we are going to do array of struct in the lab today so that would be pretty cool this is uh one of the things that I worked on at Airbnb that was like probably one of the most uh uh like impactful things I worked on was looking at how you can manage when you have a dimension that has a Time component to it like for example like at Airbnb uh you have a listing but then a listing has a calendar right and that calendar has a bunch of nights on it and then it's like so then you have what's called like a listing night which is it's own dimension in some ways it's like its own entity the Knight itself is an entity and so uh when when I was looking how to model this like uh do you model this as like six million listings and then uh or do you model this as two billion nights because we needed to look at the next year the next the next year forward and so do you keep everything at the listing level or do you um Compact and then you have like listing ID and then an array of Knight or do you you uh explode it out and then you have two billion rows where you have listing ID and Knight on the same row and then uh and there's benefits and trade-offs here that are interesting and one of the things that can happen here is like if you use Park the right way and everything is sorted these two data sets are actually the exact same size because of the fact that uh the listing ID will like uh if if there's 365 kns but they're all sorted in in in an order then the listing ID will be compressed down because you'll have 365 IDs in a row and that's and that's what run length encoding is all about you have duplicates of the same value in a column they can be smashed up together as like one and then the rest of them can be nullified and just the first value it'll be like okay we have this ID 365 times and then the all the rest of them can be removed from the data set and that's one of the most powerful ways to compress data with par and uh yeah let's I think the next slide is where we talk about that um oh yeah so let's talk a little bit about like how this works we have the Badness of denormalized temporal Dimensions so if we have it be at the listing Night level if we do a join shuffling is going to mess with the Sorting because if it's at the listing light level and it's all nice and sorted but if we need to do a join spark is going to break the sorting and that can make the compression a lot worse and because then the Run length and coding is not going to compress stuff down as much and uh I have an example here in the next slide here so here's run length and coding we're going to talk about it is so the big thing about run length and coding is if you have a bunch of duplicate data then it gets nullified right so you see all these like AC greens uh like how you have like 1 2 3 four five like you have five in a row here like you'll see like run length en coding what it does is it essentially nullifies all of them except for the first one and then it puts a five and then so when you actually the DAT don't have to all the duplicates you just okay we have this value five times and this be a very powerful way to um compress your data down and especially if you have this temporal component to it you see how we have like this season value in this data set here and so you have this temporal component that will uh kind of make it so you have a lot of duplicate um um entity information so how do you manage that right so like let's talk about how the joins would work if we don't uh if we don't use complex data types so imagine if we just have this like flattened out table and it's nice and sorted just like we have here we have AC Green all nice and sorted um but then we do a join and Spark when we do a join what it will do is we get this table now and it's all mixed up so now you see how AC Green is not in the order it's not it doesn't have the same order as it did before right so now we only get one compression we only have like this one value that ends up getting compressed and then all like we end up getting all these extra values in here and this can be a very very big difference so like for example at Airbnb when I was working like one of the things I noticed about this was that this breaking of the Sorting was something that a lot of Engineers didn't even notice because it just happens and then uh their output data set is there but it's way bigger and but they don't even really notice it and then there's two things there's two ways to solve this problem one way to solve this problem would be like hey if you join on this data set make sure to Resort it and I I'm not about that I'm all about like you should sort your data once like and then like everyone Downstream should just know how to work with it and that's where uh if we keep everything in an array right so imagine instead of having all of these uh player names and Seasons broken out like as rows but instead we had one row with a player name and then all of the seasons then what we can do is we can join on player name works great and then after the join we can explode out the seasons array and then it keeps the Sorting because then all of the all of the rows that are associated with that player name get kept together after the explode and um if that like doesn't uh like make sense uh don't worry we're going to be covering a lot of that in the lab today we're going to be using this exact data set in the lab today and covering how we can keep the sorting and how this ends up working together so that's essentially how Shuffle can break things and that's where like if you model this data with complex data types then you don't have to tell your Downstream consumers oh if you do a join make sure to Resort it because it will already be sorted for them they will like in all of their queries it will just work and that is where like that's where leveraging this especially for master data can be very very powerful for your Downstream data Engineers who are working on your stuff CU then then you aren't giving them a landmine to step on of like when they create their Downstream data set and it's like a lot bigger than like they were expecting because you modeled your data the right way so that they they can't make that mistake and so that's a big thing that I've noticed I've seen this happened a couple times in my career now with these like temporal Dimensions that you want to be careful with so that's a big thing to remember about spark Shuffle is like how it messes up the ordering of the data it's a good thing because it makes uh the join fast but you got to be careful right and like spark Shuffle is a big thing to to watch out for okay so basically um if if I understand correctly the the way we dat do our data modeling it is is key for that and then also as well as that um you want to make sure that you aren't doing queries that end up making you having to shuffle your data um so try and avoid that first still a little bit OB to me I've not I've not worked much with um PB before um only once I've worked with it but I am looking forward to seeing how how he does that in the next video I think the next video is going to be the lab all right everyone so that is it for the lab that's what we have for the presentation \[Music\] today so one of the things that I don't understand from signing up to this is I thought that if I I watched the lesson it would say that I've watched it and I pretty sure I watched the full thing then um did I not and that's where like if you model this data with complex maybe because I've got it on 1.5 speed I wonder if that's how it's determining whether or not I've watched the full thing um data types then you don't have to tell your Downstream consum well like in all of their queries it will just work and that is where like that's where leveraging this especially for master data can be very very powerful for your Downstream data who are working on your I'm just testing whether or not this percentage here is basically recording how long you're watching the video for because if I've watch it on 1.5% like speed then it it's going to get to that point um I'll have to figure that out another day okay um I'm going to watch the the lab as well so I'll probably rename this to be the the lab but um yeah let's watch the lab welcome to dimensional data modeling day one lab so today we're going to be doing more of a Hands-On exercise to really Master the concepts of struct an array inside of postgress uh to be ready for this lab make sure to flone the GitHub repo in the description below you'll be able to see the homework there and follow the lab there make sure that you have Docker installed and be able to spin up your post-ess instance via Docker and make sure you also have a client a SQL client installed like we're going to be using data grip in the lab today but data grip is paid after a 30-day free trial so you might want to use something else uh the other options are not as good you have like DB visualizer or D Beaver those are two other options that are free forever that you could possibly pick from to if you don't want to spend any money at all and if you want to learn how to do this type of data modeling with Hands-On exercises in the cloud I recommend joining the data expert community and there's a link in the description below so the main table we're going to be working with today is um we have a table called player Seasons okay so I'm just going to get my my um repository up and running pretty sure all right okay so hopefully um the percentage should be fine and should hopefully resolve itself what do you mean by I have to refresh the page okay why is this not opened in just okay that's fine okay get to Lo has 5050 okay um how did everyone find getting their um setup up and running is it okay you'll see we have um Let me refresh this this is like my connection there like broken we should have okay there we go so you'll see in player Seasons we have uh every row here is one player in the NBA and then they'll have like their seasons and their stats um one of the things that is rough about this right is that we have that temporal problem with this table where uh if we ended up joining this table with a downstream table it would cause that uh shuffling to happen and then we would lose compression so what we want to do is we want to create a table that has one row per player um and and then it has an array of all of their Seasons so that we essentially remove the temporal component or we push it into uh its own kind of data type inside right so let's let's just go through this data table real quick to look at what things are actually part of the season and what things are part that are not really changing right so obviously player name is not player name is an attribute of the player so that one will stay the same um I think that uh height obviously is probably not going to change College not going to change country draft year draft round draft number all these are going to be uh like these are the same every time right you see how like they're just duplicated so these are things that like you see how like this this data table actually has a lot of duplicate data in it that we can we can probably work with to get rid of if we model things more correctly and then you'll see we have all these other kind of uh but then once we get to this GP which is games played now we are in the actual attributes of the season and these are going to be all of these at the end here these are all the different attributes of the Season including the season itself the season um name so what we want to do is we need to create um a struct right we're going to call this season we'll call it season stats so um in post you can create types so the first thing we're going to have here is a season and this is going to be an integer right is it is it like oh yeah I forget you got to do um oh yeah there's no colon there we go so we have season integer right um we can look at some of these other ones that are probably good uh like what I really love is like evenly you know um Z you know it's so easy just to get the um the syntax Mak stuff like we all do it like I'm the same I always like oh how how do I write that again especially when you have to like move between different languages and different instances \[Music\] um I can't even spell today probably want to get uh I'm not going to pull in all of this because that's just going to make the writing the sequel really obnoxious I think that really the only ones we care about are these like first three games played points rebounds assists the rest of this is like super nerdy and like we don't really need it so we're going to put in all the rest of these so games games played is also an integer right because it's a you can't play half a game and then you have points right which is going to be a real you have Reb which is a real you have Assist which is a real and I think I think that's it it I think that that's all we really have right is um for this uh data table is or for this struct so this is going to be its own data type right um that we are going to have here is going to the season stats struct and we're going to create this data type awesome now we have our season stats struct now what we want to do is we want to consider a new table that what it will be is um hold up yeah yeah what it will be is it will be all of the columns that are the player at the player level so that we don't duplicate them and then we will have the array of season stats that will be the other kind of big piece of this puzzle right so let's go ahead and we're going to say create table players and so in this case um we can I'm going to comment this out comment this out so um the first obvious column we have here is player name which is going to be a text right um so age age is an interesting one right because age is technically actually uh a property of the season right it's not actually a property of the player because it really depends on the um uh the season value and that will give us our age but we can also figure that out based on what their first age was so um but like we're just going to ignore age for now but we can also keep height Heights a text I hate the heights a text but it is and then we have college right which is also a text right we'll just uh we'll just ignore weight as well because I think that's one that can change um then we have country text draft year so draft year is also a text even though it looks like an INT because you have undrafted here right draft year is a text and then you have um draft round which is a text and draft number also a text because you have undrafted is also a Val okay so we're creating like um a player table for things that are likely to be fixed for that one one person so that we don't get um those things do not need to be duplicated because they're going to be the same for each person sorry I will um be I do not use a mechanical keyword so but I will make sure when I type I pause the actual thing rather than um rather than um and I can I can reduce my microphone audio as well so I can I will do that um what have I got proper this um is that better is the mic better now it um okay so I've adjusted the mic settings um it' be great if I could just get some feedback just to see if that's better um and I'll tight and see if if that's better as well um but if if it's not let me know and I can do a few different things why is this not yeah okay increase my mic volume it is too low um okay is that better um hello hello is that better sorry this like I've not done a live stream before and usually with this mic that I've got which is the road microphone I do I I increase the audio after the um like as suppose setting is that is that okay now is that better oh is it too loud now um let me change it to I feel like I'm one two three testing two testing I I've put put it in the middle between before and now is that better okay perfect cool and let me know if the typing is too much as well I can reduce that as well if it's better um if it's too much yeah like no one told me in the last um in the last one that the audio was bad and so it's like it was only until afterwards that like I was like oh God that's bad that audio is terrible but um at least everyone's telling me now which makes it a lot easier to edit and and figure it out because then I've got people saying okay yep that's good not good um is the typing okay so that is going to be our essentially these are the values that don't really change in this player data set then we have uh we have one more column here which is uh we have season stats right and this is a this is a season stats array right so this is uh where um you might be seeing things that are a little bit new to you um there's one more column in here that I like to talk about which is going to be uh it'll be season but it's actually what I call current season and this is going to be integer um because we are going to be developing this table cumulatively and so the the current season is what like as we like do the full outer joins between the tables this current season will just be whatever the latest value in season in The Season's Table is and uh that will make sense as we kind of build this up uh so we have our uh but like what's the primary key of this table the primary key here is going to be player name comma current season it's going to be the um cuz uh like it will build up and build up and build up and so this will be our unique identifier on this table and what we're going to do is we're going toh go ahead and create this so we have our players table and it is um good to go now what we want to do is we want to think about how we do this full outer join logic because I think that that's one of the things that is going to be interesting so first off let's let's figure out what the first year of in this table is right so let's do select M season from play poou joins between the tables this current season will just be whatever the latest value in season in The Season's Table is and uh that will make sense as we kind of build this up uh so we have our what's the primary key of this table I miss whether or not he actually created that table now or whether he just out um the primary key here is going to be player name Comm a current season it's going to be the um Cu uh like it will build up and build up and build up and so this will be our unique identifier on this table and what we're going to do is we're going to go ahead and create this so we have our players table and it is um good to go now what we want to do is we want to think about how we do this I missed the table that's fine it's going to be the um like it will build up and build up and build up and so this this will be our unique identifier on this table and what we're going to do is we're going to go ahead and create this so we have our players table and it is um good to go now what we want to do is we want to think about how we do this full outer join I think that that's one of the things that is going to be interesting so first off let's let's figure out what the first year of in this table is right so let's do select M season from player Seasons so this is going to give us like whatever our first year is okay so we can go back to 1996 awesome um that's going to be our first year that we work with so let's go ahead and I'm going to show you how this today and yesterday query ends up working right so we can say with um we're going to say yesterday as and then we're going to say um select star from players where current season equals 1995 I know this is going to be strange but just just go with me on it right now you'll see and then in today we're going to say select star from player Seasons where season equals 1996 so this is going to give us the cumulation between today and yesterday so what we can do here right is we can say select star from today full we're going to say today T full outer join yesterday Y and then in this case we're going to say on tplayer name equals y. player name that's going to give us the um the values that we're looking for so let's go ahead and like do this query because I think that okay so this is basically a way of um populating the data that we want um for the the currencies and using the um cumulative approach that um that Zach mentioned in the previous video what you never guys before um okay and then today Seas Seas 96 okay so we're going to be getting the from the previous table and then this the table um is the this is like probably a lot so you'll see one of the things you'll notice is like as expected everything from yesterday is just null you see that just null like as we would expect because it's just all null um which is great okay \[Music\] did you say that the um the AR join did you say that these queries are on Zach's site did you say okay that's that's like where that means that uh like now what we want to do is remember the first the first thing you want to do is you want to coales the values that are uh not temporal like essentially the values that aren't changing so in this case the first thing we want to do here we want to say cols say t. player name y. player name this is as player name and then what we want to do and then we basically want to do that for all the things that are there what type of music do I like to listen to um it depends actually so sometimes I will listen to um like pop music so um you got things like uh who like um sa Carpenter s MCC like oliv that but then then I also like um I'm currently listening to Hy's new album which I really really like at the moment um I really like Hy uh and then I really like my favorite band is probably de Havana I don't know if you um I don't know what you mean by my Str not created it's there you don't need to um have an as before on your tables I'm pretty sure uh so I really like de that's probably one of my favorite artists at the moment um well it's one of my favorite artists and then I love you at six and then um King us is uh there I like L par too linol par is one of my favorite ones too um so yeah a bit of a variety really it's kind of a mixture between um pop pop punk um and like alternative music I would say is my favorite kind of genas is essentially copy this guy a lot of times cuz we're going to do boom boom boom boom and then we're going to do we got to do all of these right we have like height I've never been a mess fan of Queen day to be fair um I don't know what it is it's just never been I don't mind know but I much prer um Like My Chemical Romance and fall Bo from Thursdays basically um I did say to someone actually um that um if I hit like um if I hit like 10,000 subscribers on YouTube maybe I do like a little like song video but I don't think I think I forgot about that so I'll probably um like hopefully like hide that height height then we have uh what college College college then draft year draft year draft year and then draft draft round draft round draft round and then one more here and then that's a draft number and then draft number and then draft number so let's look at this query now and uh just see like what this does right oh we got to actually do the whole whole thing here so you'll see now we have essentially the same query like you'll see like this is now like you're probably like Zack that was the most that was like the lamest thing you've ever done there like you just essentially queried uh that's just today right this like it only manages today's data but I promise that's because this is what's called the seed query for so I just join them together oh if I was going to pick a if I was going to pick a favorite song to be that it would be if um Prett I don't know I just I really like it it's it's it's death band is the kind of band that I listen to when um I'm in a really crap mood even though it's like it's quite like depressing the music but it it makes me feel better I don't know why accumulation and you know it's the seed query because this first players is null right you see how there's nothing in there right so since it's null that's that's what that's what makes it the seed query and um that's why the full lad join here is just taking everything from today but that won't be the case as we build this up as we cumulate more and more so the next piece that we need to put into this query is we need to add in the um the seasons array right so what we want to do here is we want to say uh we're going to have an array uh and then in this case what we want what we want to do is we want to say we want to see if the um if yesterday's value is there right so let's do that first we're going to say case one y. season stats is null then right and then we're going to have a an else here uh and then I'm just going to put an array and we're going to put like we're going to work with this for now so why we need to do this is um we're going to be doing an array concat here and what this will do is it's going to slowly build up the array of all the values here so in this case what we want to do is we're going to build up an array of row and in this case we need this row to be these values season GP points rebounds assists right so in this case we can do t. season t. GP t. points to. RB okay yeah so I think um thought ear like if you probably want to be taking the the latest results if the previous is n then it'll be interesting to see which results you take if if if you've got both populated I know that if like you're looking at last active you probably want to take the latest one but um T do assist right so and then in here we need to also do a um you got to cast it as like a um a colon so this colon colon here will cast it as the type that you're looking for it will take this row and turn it into the actual stru type type that we defined so this is only if it's null right if it's not null then what we need to do is we need to say y. season stats um and then we need to concat this right that's going to give us um the current value right uh or this will give us if uh if it's not null right so let's go ahead and run run this query move this so he doing the same thing um do you know one thing that really bothers me about um like SQL in general um it's just that like it's the indentation that you have to do the indentation yourself it really bothers me really triggers me okay so you'll see now everyone has this uh this nice little uh struct thing here we're going to call this as season stats right and then um the last piece here right is oh wait yeah okay there's actually one more case here because of the fact that like we're only managing if uh because we don't want to add to the array if uh if today's value is null for like a player that do that has like retired okay I'm assuming that this basically puts it into the um stroke that we want it to be in because we want to hold on to that data but they might we don't want to keep adding more n to the array right so what this is not an else here this is actually uh we're going to say when uh this is uh t. season is not null then and then we have one last else here and the else here is y do season steps so that we don't add a bunch of nulles that we don't want to add to right okay yeah that's fine um so like I know this isn't very like um riveting to watch um but I always find that actually doing the lab alongside the tutorial makes it a lot easier to like absorb the information so I could just watch this lab and be like yeah that looks easy fine um but I find that when you do like the like actual following along it stays inside your head more so when I do my homework which I will be doing soon I should hopefully have this all fresh in my mind and be able to do the homework properly that would be because we want to so this is going to cover all three cases right so it's like if it's null we we create the initial array with one value if um if today is not null uh then we create the we create the new value right and then otherwise we just we carry the history forward and this means this is like a retired player who doesn't have any new seasons so the last thing we want right is this current season value so current season is an interesting one right so in this case we want to say um um case when and we're going to say t. season is not null then t. season and then we want say else y. current season plus one and so what this will do is this is going to give us our current season value uh because it will either take the current season or um oh there's actually a nicer way to do this that's that's that's an ugly way do it so what we can say is col. season y. current season uh plus one as current season right this will this is much nicer so this is now matching right this is this query here is now matching what we're expecting value uh because it will either take the current season or um oh there's actually a nicer way to do this that's that's the ugly way to do it so what we're going say is I quite like about um what SE does is that like he um and this is guess what you find that things like Ai and stuff like that won't necessarily teach you the best way to do it is that like you learn when someone has a lot of experience how to do things in a nice way in in a concise way and I really like that he kind of shows you the the different ways that you can do it so so yeah I just thought that was nice co as t. Cs and Y do current season plus one as current season right this will this is much nicer so this is now matching right this is this query here is now matching what we're expecting let's just run this kind to show you so you see now we have that 1996 for everybody which makes sense because it's the first uh it's the first record right and so um everyone should be in here and they should all have 1996 so now what we need to do is we need to turn this into like a little pipeline Let's do an insert into here we're going to say insert into players right and this this query should just run column season stats is of type season stats array but expression is of type integer what did I miss a I must have missed something College hi College oh it's because I missed country ah okay so now uh now we have all the columns I think country draft D okay okay so now we can just we can just run it I didn't have I think it ran right yeah so yeah it ran because now it's it's yelling because there's a duplicate so if we say select star from players now you'll see players now has uh a player and it has uh their height college and then it has their season stats and then the current season right and then this is all exactly like what we're expecting to um I'm wondering if there's um like a a like editing like error or something here or like uh because I amet and I do not see um country at the beginning of this like not that it's a problem maybe I'm just I'm just going to go back a minute oh there was country that I just did not add it so that's on me it's just weird that um that it works fine for me but not um right I'm just going to go back delete table drop and then I'm just going to create this with then I'm going do yeah so weird though that that cool that's fine seven so what this is going to do is now we're going to insert into play is you'll see players now has a a player and it has uh their height college and then it has their season stats and then the current season right and then this is all exactly like what we're expecting to um expecting to see here and so this is uh but now what we can do is you can really you're going to witness the power of cumulation and uh what we do now is we just fiddle with these two values and I think you're going to really see what we do so you saw how we loaded in 1996 that's all work so now what we're going to do is we're going to change yesterday to 1996 and today to 1997 so what this is going to do is now we're going to inert into players but we're going to have the um this is going to create now if we say like select star from players where current season equals 1997 right so now in here you'll see we have a bunch of players right um and you'll see now you see the you see how this now has an array of two values that and we we pull these players forward right and see this guy here this guy 1997 guy he joined this is his first year right and so this guy just just joined right so he's not even um like he doesn't even have like like he's like a rookie probably or something like that right and you'll see some players will have two records some players will have one right and that's pretty common right and that's because we're holding on to the players there's going to be some people in here who like will have just a 1996 record because they retired right because they played last year but they haven't played in 1997 so but now we're holding on to more players and we're going to keep holding on to more of History so what I want to do real quick is let's go ahead and change this 97 to 98 right and we're just going to we're going to do this like two or three more times because I want to illustrate uh there's like I don't know if y'all uh follow the NBA at all but there's um okay good that worked so like you know Michael Jordan he did this weird thing where like he like retired and then like he came back and played for the Wizards right and um and like he wasn't that good on the wizards but like um but he was like one of the best basketball players of all time right so um I just finished loading in 1999 so let's do 1999 to 2000 so Michael Jordan came back in 2001 so uh let's just load up like two more data sets here so I change this to 2000 and 2001 so now uh let's go ahead and load this up okay so now let's go ahead and select star F star from players um and then we're going to say where current season equals 2001 so you'll see in here how we have all sorts of players in here and they're going to have all sorts of columns right and you'll see like this guy this Houston he just joined in 2001 right some people are going to have many seasons some people are not going to have that many seasons so well let me show you how this is really powerful though cuz we can say if we say I'm player name equals Michael Jordan right because he uh okay so you see here's Michael Jordan North Carolina Country so um uh one thing I really love and is is is is when people from American America say y'all like I don't know I just really like it y'all like Zach says it quite a lot and I just think I like I like that PR draft year all that stuff but if you look at his um you'll see how he has that Gap right so you see like 1996 1997 and then 2001 so he had a gap right where he um he was like gone for a couple years right and um so that can be a big kind of metric that you can also add into this um kind of like like you can cumulate and add more metrics into this table that like can can kind of illustrate like how long it's been since someone retired or like years since retirement and stuff like that you don't just have to collect this array right that can be another big thing that you can add into this table so that's one of the things that I think would be a powerful thing that you can also add so I want to slow down here just for a second so that we can really kind of take in how powerful this actually is because now if we joined this right so say we uh like join this table with another table that gives us more information about michelan then what you can do right this can eily become so this table here is now flattened out right and we it can easily become player Seasons again this table here I'm just going to show how that works real quick so what we're going to do is we're going to say select player name and then what we're going to put in here is I don't know if you ever used unest but there's a thing called unest then we're gonna say season stats and we're say as season stats all right so this for so just enjoy okay so you see now how we have three records we have the 1996 1997 and 2001 records that will all they're all there right so this is one powerful way um that you can get this back right I think does does do can you put a dot star here I mean in post or in Presto you can do that okay maybe can you do it this way because then can you do season stats this okay one second I want to see if this actually works okay but okay so you have the season stats here and then this gives you the unest right I think okay I think postgress is strange and like because what I want to do is I want to show how you can actually explode it back and get get the columns here so I think what you can do is if you put this in a CTE so we're going say this like unnested okay so in this case what I want to do is I want to say um season stats. star yeah it does work awesome okay so this query does work missing from wait do you get okay you okay you have to do it like I think you just have to do it this way say season stats. assist does it give you that it doesn't give you that that what that's so weird that actually should work one second like I know I'm really close here there's like um uh I want to just check something real quick on the um here there's like a I have the unest query does it does it actually show here no that this is just saying seasons though because one of these has it one of these has the um the way to do this second let's see here because this has all the coales stuff is it one of these season stats oh and then you have to like cast it so I think what you actually have to do here then is if you do this I think that will make it work if you if you cast it like that give us the um okay there we go so post is strange but if you do it that way can you do a doar like that though because really you should be able to do do star like I know in um in like spark and Presto and stuff like that you can do do star like this and it will ah it does work okay cool that's the query that's the syntax right so this is how you get back ah I did not mean to do that um okay so if you do is it like that that will give us the um okay there we go so post is strange but if you do it that way can you do a dotar like that though because really you should be able to do dotar like I know in um in like spark and Presto and stuff like that you can do dotar like this and it will ah it does work okay cool that's the query that's a syntax right so this is how you get back to um the the old schema right so you can see how like you can go back and forth easily going from uh the unest stuff back to the the regular stuff and that is definitely one of the powerful ways that this query and these syntaxes can like essentially you get the versatility of knowing like also of other facts about that player but then you also get to know like okay like um like if they if they want to do a join they can go back to just this player's table and it's already at that grain and then now like for example let me just remove Michael Jordan here real quick and then you'll see that this is going to always be sorted right you'll see that like every player you see how it keeps all of them together so now all of these player names at least for that like you know that runlength encoding problem that I was talking about you'll see how there's all these players in here and then like all their years and they're all kept together um and so that is the powerful thing that you get access to with cumulative table design is now if you have this temporal component right in this case is this season component that now can be if you have a join or anything like that you do the join awesome get the new things that you want for player but then after that you can unest this and then everything will stay nice and compressed because it keeps all of the temporal pieces together and you don't have to worry about uh messing up the Sorting because the Sorting will all stay and you'll be able to keep all the Sorting together so that could be a very powerful thing that um I highly recommend that yall kind of like look at and look into so but like what if you want to like figure out other stuff here right I mean there's all sorts of other things here that I think could be really uh powerful to look at right and and this is one of the things that I like I also want I like this I like the fact that basically if there are things that you want to um like call together um they're not the things you're likely going to be um searching on so um you keep those col together um or I say call us like in one structure and then um you only separate them out when you need to you're likely not going to be filtering on those and if you are filtering on those like we are doing with the seasons then that is put into it own table as well all column as well I want to illustrate is how you can use these analytical queries and this cumulative table design um let's go over this player table again like uh let me just I'm going to just stomp this now so say select St from players where um current season equals 20 2001 right what we're going to do is we're going to go ahead and we're going to uh we're going to just drop drop drop the players table real quick I know this is like kind of sad because we just loaded up all this data but we want to add in two more columns into this table so let's go ahead and uncomment this and then so we want to create um essentially um what's called the scoring class column uh which is like essentially like is this a good player a bad player and we're going to base it all on the points column but like what we want to do is we're going to say create type um scoring class right let as enum and then enum values here I'm going to say star good average and we'll say bad go with that so this enumerated type is going to be all of and we will come up with a definition here of like what each one of these things mean we have scoring class but we want to put scoring class in here as well so let's put scoring class into this table and then we also want like years since last season because I think that's another great um a great uh metric that we can also compute um so this is going to be our new players table and we're going to add two more columns into that players table now we have our players table but we need to add two more columns in it right so we have um and then we have a current season is still the last column so it's after the season stats we're going to like what each one of these things mean we have scoring class but we want to put scoring class in here as well so let's put scoring class into this table and then we also want like years since last season because I think that's another great um a great uh metric that we can also compute um so this is going to be our new players table and we're going to add two more columns into that players table now we have our players table but we need to add two more columns in it right so um scoring class right this going to be as enum and then enum values here I'm going to say star good average and we'll say bad go with that so this enumerated type is going to be all of and we will come up with a definition here of like what each one of these things mean we have scoring class but we want to put scoring class in here as well so let's put scoring class into this table and then we also want like years since last season because I think that's another great um a great uh metric that we can also compute um so this is going to be our new players table and we're going to add two more columns into that players table now we have our players table but we need to add two more columns in it right so we have um and then we have a current season is still the last column so it's after the season stats we're going to put two more columns in here so so in this case we're going to say case when um it's going to be t. season is not null then we have the case this that means they were active this season then um we're going to put then uh we'll put something right we'll put our values here and then oh wait it's going to be a cols my bad this is a cols put a cols here um and then uh wait no no that that was right because we need the case one there okay so we have an end well we'll come back to this in just a second then we're just going to put like a that then we have a the last one let's do the years since last season first right so in this case we're going to say um case when T do season is not null um then zero else this is going to be um cols y do um this is y. years since last season for can't spell anym comma 0 plus one and this is end as years since last season so I'll go over this real quick so and then we'll go we'll do the scoring class here in just a second so how this works right is if the season is not null then it's zero easy right um otherwise uh they um the year since last season this might be null uh actually wait it it should never be null right because it should it because the first year that they're in it should be there and then it will always be populated after that so yeah actually this this is not necessary you can just put the plus one and that will work so how this works right is like okay if the current season is not null that means they they they played this season and then otherwise it's been um one more year since they've played so this might and as as they've retired this number will keep incrementing and incrementing and then um if they come back though it will go back to zero and so that's how this like cumulation works right so um in this case uh for the um scoring class we essentially want to uh we have three values here we have star good average and bad I like to use uh points for that right so then we can say then case when uh t. points is greater than 20 then um star um when T do points is greater than 15 and good when T do points is greater than 10 and aage okay so that's um so adding some casing here of when T season is not all equal zero okay um when 20 is it bad is it bad yeah I think it bad average uh else bad and okay there we go so now this is going to give us and then we want to cast this as um a score class and uh that's going to be the first case is like if it's uh um if the season is not null but then there's going to be an else here right and in this case what we're going to do is we're just going to pull in last year's scoring class so like whatever their most recent season if they're retired the scoring class will just be pulled in from their most recent season and that's going to be how this kind of scoring class works I'm going to put this as scoring class so this is an enumeration right that's what's up here you can only only have these values that's the whole idea behind how enumerations work right so let's go ahead and uh got to change these back right so yesterday this is 1995 and then this is 1996 so we have our now we can run this right let me uh need comment this out so we can just hit the hit the Run button cool so now this will run okay so that ran and then we want to change this to 96 to 97 run all right then 97 98 when missed the when out okay that's fine I think that's everything then so it's really it's very much adding logic similarly to what you would in um in other like coding languages so I'm not sure why not the season is not null but then there's going to be an else here right and in this case what we're going to do is we're just going to pull in last year's scoring class so like whatever their most recent season if they're retired the scoring class will just be pulled in from their most recent season and that's going to be how this kind of scoring class works I'm going to put this as scoring class so this is an enumeration right that's what's up here you can only have these values that's the whole idea behind how enumerations work right so let's go ahead and uh got to change these back right so yesterday this is 1995 and then this is 1996 so we have our now we can run this right let me uh I need to comment this out so we can just hit the hit the Run button cool so now this will run okay so that ran and then we want to change this to 96 to 97 run right then 97 98 miss something say I'm just getting so this is an enumeration right that's what's up here you can only have these values that's the whole idea behind how enumerations work right so let's go ahead and uh got to change these back right so yesterday this is 1995 and then this is 1996 so we have our now we can run this right let me uh need comment this out so we can just hit the hit the Run button cool so now this will run okay so that ran and then so this is an enumeration right that's what's up here you can only have these values that's the whole idea behind how enumerations work right so let's go ahead and yes I am good thank you P how how are you that's fine interesting um um okay I need to move this yeah same with this yep cool then I'm going to do6 S I'm just going to 2 200 uh got to change these back right so yesterday this is 1995 and then this is 19 okay so now let's go ahead and say are from players where current season equals 2001 so this will give us so you'll see now we get all these columns we have the draft year we have season stats and then we have like scoring class and then years since last season but you'll see that like most of these people are going to be okay so see here we go see there are people here like so this guy he was in 97 but like he hasn't played in four years right so there's going to be a lot of people in here who like have like one or two years in the data set and then that's all they have right so anyways uh you'll see here we have our um current season and then if we say like and um we'll say player name equals Michael Jordan right so you'll see in this case let's just filter down to this guy right but you'll see years since last season makes sense right and you'll see how he's a star because in um 2001 you see he got 22 he averaged 22.9 points so makes sense that he's a star right so but if we go back one right so we say well instead of let's go to 2000 you see then he has years since last season three so like because like he hasn't played for three years and he took three years off and so like you'll see that like you can build these things up incrementally and you can really have some really powerful ways of doing cool queries so one of the things I wanted to show here though is let's um change this back to 2001 what I want to show is let's do some analytics to see which player had the has had had the biggest improvement from their first season to the the most to their most recent season so what we can do right is we can say season stats you know one is going to be this is as first season and then we have um season stats at cardinality of season stats and this is as latest season so and then we can put player name in here so this you'll see now this gives everyone and you have both their first season and their latest season so if we say this case okay we got to do the season stats we do do or then you have to put it in pen because it's obnoxious then you see do dot points and put this in pens then cast it to season stats dot points interesting this so now this is going to give us uh okay there we go so now we have our first season points and our latest season points so what we can do right is we can actually just divide here right or or we want latest season on top though right um and then we want to divide so that we can see like how much people improved I think this might give us divide by zero because of oh yeah we get division by zero so what we want to do is um if um equals zero um one else oh no just don't you just put like put this it's interesting CU you can create some really ugly looking SE queries if you want to so now that will like we can just have that that if block there right does it not like do I did I mess something up that oh is no that that looks right okay fine we'll just do it we'll do it the case when way there now that will work okay awesome okay this is not else I am missing something here I don't know what it is is it I haven't yeah there we go end okay so now we have uh you'll see we have we we we have a um an order here right this is going to give us our um our latest our currenty our first season and our latest season so if we say order by two descending this will give us who was the most improved player right so see a lot of w a lot of people have exactly the same so if we go all the way up okay so it looks like we have Don mle he like he did a 13x so like between his first season and his latest season he did 13 times better right and so that can be uh very powerful but you see one of the things that I I want to really emphasize with y'all here is this query doesn't this qu doesn't have a group by it's like there's no group bu but it seems like you like normally you would have a group by here because you would have to have both of those and you need to subtract them or do an aggregation of some kind where you do like a Min and a Max and then you group buy right that's how you would do this right but this query has nothing right no group buy so this query is actually insanely fast right it's a very very very fast quy the order by part is the slowest part of this query but without the this is like you can do this query with no Shuffle very very fast so like that's one of the things I'm trying to really emphasize to y'all is this query is like or this cumulative pattern that you can use is very powerful it incrementally builds up history it gives you access to historical analysis right like and for example like we say we just put in one thing in here let's put n scoring class equals star so that we only look at like because there might be players who like uh like okay there we go so these are okay these names are looking a little bit more common like so Tracy McGrady Derk Nowitzki right oh there's Kobe Bryant so he did like um 60% better and then like you see all sorts of players there where you can kind of filter it down to like the current players you these names all are like looking very familiar now but like that's the idea here right is you can easily do historical analyses on cumulative tables and you don't have to um do any Shuffle any group buys and this this career like spark or Presto will destroy this query in half a second right that can and that's like it it will just be so fast cuz the thing is is like if you don't have a group buy and everything can happen just in the map step and there's no reduce step then uh this can this can be as paralyzable as you want it to be like you this can be like what I call infinitely paralyzable we're like so this can this query can be as fast as you want it to be essentially right and so that is one of the things that can be really really powerful in creating these queries um I I hope yall um had um a good lecture here \[Music\] today okay so that's day one completed um yeah there's a lot of information there but it was really interesting to do and um the only thing is that I noticed that I had some slight differences in the like data that I got back versus the data that he got so um one of the things that I might investigate is I might went through all of this again in my own time just to see if I missed something out as I was following through the lecture or whether it might just be that um yeah I don't know I think I've missed something um maybe I missed like one of these when I was creating the inser to players um I think he started from 19 95 so that could be why um so and maybe I start from 1996 so I'm going to do an investigation of that after like ending this um but I am going to have my like dinner fast um because it's dinner time and yeah no I thought that was a really good session I think I learned a lot um about data modeling and yeah that is everything today if this was let me know if this is actually useful because I'm happy just to go through this on my own instead of streaming it I just thought it'd be nice to talk to people when I'm streaming it um but yeah let me know it's been great to speak with people as I've been going through this and thank you for those who have listened and watched the lecture and followed me um yeah it's been it's been fun so I'm going to end that here um and I will see you the next lecture hopefully thank you bye


 > [!info]
> - **Knowing Your Data Customer (15:03):** Understand who will be using the data (analysts, engineers, models, executives) to tailor the data model appropriately.
> - **OLTP vs. OLAP (19:50):** Differentiate between transactional (OLTP) and analytical (OLAP) processing needs when modeling data.
> - **Cumulative Table Design (28:07):** Use cumulative table design to hold onto historical data for analysis.
> - **Compactness vs. Usability Tradeoff (37:34):** Balance data compactness with usability based on the downstream consumer (online systems vs. analytics).
> - **Temporal Card Expion (41:19):**  Manage dimensions with a time component, considering how joins and sorting impact compression and run-length encoding (RLE).
> - **Run-Length Encoding (42:21):** Compress data by nullifying duplicate values in a column, especially effective with sorted data in Parquet format.
> - **Importance of Data Modeling Key (46:01):** The way to do data modeling impacts if you're having to shuffle data.
> - **Arrays of Structs (53:07):** Structs allows you to group your data to avoid duplication.
> - **Seed Query (1:13:52):** Seed query is the first step to building your cumlative data to look at what's to come in the future.
> - **Historical Analysis (1:57:30):** Historical Analysis can be done on cumlative tables without having to do any group buys.