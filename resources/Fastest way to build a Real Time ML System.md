---
title: "Fastest way to build a Real Time ML System"
author:
  - "Real-World ML by Pau Labarta Bajo"
published: 2025-03-18
source: "https://www.youtube.com/watch?v=LtYKld5zVd0"
image: "https://i.ytimg.com/vi/LtYKld5zVd0/maxresdefault.jpg"
created: 2025-03-20
tags:
  - "youtube"
  - "Real_Time_ML"
  - "Machine_Learning"
  - "Fraud_Detection"
summary: "Learn the fastest way to build a real-time ML system for fraud detection using feature pipelines, online retraining, and model versioning with TurboML."
---
# Fastest way to build a Real Time ML System

![Fastest way to build a Real Time ML System](https://www.youtube.com/embed/LtYKld5zVd0)

> [!summary]- Description
> Github repo with the source code
> ↳ https://github.com/Paulescu/end-2-end-real-time-ml
> 
> Sign up for FREE at 
> ↳ https://turboml.com/

> [!note]- Transcript (Youtube)
> what's up guys such a long time today I'm gonna show you the fastest way I know to build undeploy and manage a real time machine learning system let's get to it hi guys this is PAU yeah the same guy who was talking to you like 2 seconds ago it's the same okay it's me like quick let's go let me show you the git repository that I put together so you understand how to build a real time ML system on top of the TurboML platform so first of all let's go this is the repository let me scroll up here it is it's under Paulescu which is my Github username don't ask where this thing is coming from and then end to end real time m L so here's the ripple uh let's clone it so I'm gonna go here s s H um URL here it is so I'm gonna go to the terminal gate clone here it is okay is it there yes CD N to N and then I'm gonna open it with Visual Studio code I'm using Visual Studio Code because we're going to use one very useful development tool called dev containers which not all ideas support Visual Studio Code supports so this is it here we go okay so I'm gonna close this one here is the project okay so here's the same rhythm now in order to ease the development I'm gonna use a dev container what's a dev container a dev container is like a sandbox environment it's essentially a Docker container inside of which you have all the development tools that we you need while you are developing your project and dev containers are super useful because they allow developers working with different environments inside the same team to share the same tooling so instead of me working with my tools inside my MacBook and you for example using your Windows tools and each of us setting up all the development environment alone we're gonna define one common environment one dev container that both you and me can spin up locally and for that you're gonna need first the Docker engine installing your system and don't worry if you don't have it if you go to the readme you're gonna find the instructions and then inside this dev engine we're going to spin up this dev container with all the tools we need to develop for this project in this case the dev container you can find in dev container dot Jason and here we're essentially using we're building on top the unofficial image that the Turbo ML has prepared for us so this is a an image that has all the development tools that you're gonna need to locally develop and run your scripts against the Turbo ML platform so in order to start using the Dave container you need to open that container so for that you can go up here on the lower left corner you're gonna click and you're gonna click on reopening container if you don't see this option it means that you don't have the dev containers extension installed in your system and again don't panic if you don't have it in the read me you can find instructions to install it it's super easy you just add it like a normal VS code extension so we're reopening container here's it and now basically you can see it's saying connecting to that container you can check here the locks and here we are now if you observe here on the on the lower left corner it says that container Python so now we are running this thing is connected inside the that container how do you know that here below on the terminal if you just check you name hey basically I'm gonna see this is a Linux Linux Linux blah blah blah so my MacBook I'm using a MacBook but this terminal is actually connected this SSH into the container where we are developing and the container is based on Linux so it doesn't matter if you use Windows MacBook we all use the same environment long life death containers long life or long live god bless death containers um okay so now how do we build our system our system is composed of a feature pipeline which is responsible to transform this incoming transaction data into features then the modeling step so we're gonna need to define what kind of model we want to use and then we're going to generate live data to check that the whole system is working so the three comments you can find them super simplified in this makefile so in this makefile I created three comments you can run so you can define your future pipeline with make Future Pipeline with make model you're gonna set up the model and push this model definition to Turbo Mel and with make live data we're going to simulate actual production data being generated and being ingested into the Turbo Mel platform so that we can check that the model works so before doing that you will need to get a free account at Turbo ML so you can get a free account you can play with it for 24 hours and then if that's not enough for you you just sign up again and you're gonna get 24 more hours so you can play an experiment for free for that super simple let's go to Turbo ML so here it is let me go let me start so you go to Turbo ML and then you go try now you'll need to log in so I'm gonna use my Google credentials and here we are so here I'm on my inside my Turbo ML dashboard so to start using to communicate with the Turbo ML platform we're gonna use the Python SDK Turbo ML Python SDK in order to authenticate and use it we need to get two important parameters which are secrets don't share them with anyone the back end URL and the API key so these two parameters you will find in the report I created a dot m example so you need you just need to copy this thing and rename it as dot m and then here you need to copy these two parameters so the back and U R L it's here let me copy it here so back and U R L let me increase here the size so that you don't end up uh going to the ophthalmologist after this video and here is the a P I Q so here we are super so now in terms of authentication everything should be good to go so now let's first define our future pipeline for that I created the make set a feature pipeline and to run it it's super simple just to make a feature pipeline and now this thing while this thing is running let me show you the inner of this Python script so to define the future pipeline this follows like a declarative approach meaning that you define the transformation logic and then you say Turbomel you send it to the Turbomel platform and then you let Turbomel handle all the low level details necessary to bring this logic into life why is that important in real time emel uh performing transformations doing transformations in real time requires at least two important pieces of infrastructure one of them is a message bus typically a Kafka red Panda whatever that basically moves data as synchronously in real time between services and this is basically the data transport now this element Apache Cafka helps you move the data but to do feature engineering you need to apart from moving data you need to transform it on the fly and this is when a real time data processing engine enters into the game which means that you also need to add something like Apache cli flink a patch is Spark Streaming also there are Python options but in any way you need to bring uh not so easy to handle pieces of infrastructure to bring your logic to life and this is what torque and Mel does for you out of the box you define your future engineering logic using Python you submit it to the platform and you're done from that moment turuwamel takes care of executing so spinning up the necessary resources to compute these features in real time as new data comes into the platform so in terms of feature engineering in this case uh I just created one feature which is it's called a window feature so based on all the transactions so whenever a new transaction enters into the ML platform one feature that might make a lot of sense for me to compute is how much uh money was spent using that card in the last 24 hours so this is this is one example of a window feature and they are very common in real time and ML systems right so this is called stateful calculations they're not so easy to implement they're not so easy to handle at scale but this is something that you don't need to worry here you just define the logic and then doing turkmenel will bring in the necessary resources for this thing to actually happen fast enough for your use case so in this case uh we're using the Python SDK that Toru Mel has it's super easy to use and if I scroll down I'll see that yes the features have been already created so now if you go to the dashboard if you go to features you will see that there are two feature groups we have one for the labels the labels are binary numbers 0 or 1 0 meaning the transaction is non fraudulent one means the transaction is fraudulent and then here the other feature group are the features which means all the information we get from a transaction plus the features that we engineer on top of it so in this case we have the transaction time stamp the origin several more raw features plus the one that I engineer here which is the total amount spent with this car in the last 24 hours so this is our future pipeline it's there and it works so now you can move to the next step right you have the features in your feature store inside turuwell now you want to start creating some predictive models so you want to use them to try to find correlations between the features and the labels right and this is the part where we move to the second pipeline of the system which I call setup model call it set up model because we are not really training inside our computer here we are leveraging the Torrey model platform how what we're doing is we are defining how we want our model to work meaning what are the input features and the labels we want the model to output the model type which in this case I'm using a hoffding tree classifier this is a very standard real time online ML algorithm there are many of them you can play with you can use out of the box algorithms provided by the torch platform or you can create yours for example using a Python library an open source one called River uh then you define as I said that what are the features divided to numerical or categorical and that's important because this way uh you tell the model what features what columns needs to be encoded so that they can be used and that's it and you push the model to the platform right so when when we run this script which is what I'm gonna do in a second we're gonna submit this model definition to torque and torque will use all the data we push so far regarding previous transactions to train an initial version of this model and deploy it and that's not the end of the story why because some real war machine learning problems require what is called online retraining and fraud detection is one of them fraudsters are constantly trying new strategies new ways to try to commit fraud right and use money illegally so if you build a machine learning system that is supposed to predict fraud you need your machine learning model to continuously adjust to new fraud patterns that you are able to detect and for that you need to have a system that continuously and incrementally adjusts your model so that it doesn't get old this is something that is not easy to implement in practice it's not easy to monitor it's not easy to diagnose when things go wrong so it's something that you would typically spend a lot of time doing it on your own and this is when Turuwa Mel comes into the game again so with Turuwa Mel we've pushed the model the model is trained with the initial historical batch of data and then Turuwa Mel takes care of updating incrementally and versioning the model and that's super important because if at some point you check the monitoring metrics and things do not work well you can always roll back right so this process of online retraining and model versioning is something that Turbo ML brings here and it's super super useful for this problem okay enough talking let me let me try to run this thing up so let's do make uh what call make model I think I call it let me check yes make model here it is uh huh you know what happened here I already deployed this model before so I cannot really blow it with the exact exact same name but that is fine then I'm just gonna check the model that is there let me go there models models models here it is super here the model super okay so now the last thing that I want us to do let's generate some actual data and check that the whole thing is working so for that in general in a real world setting what you would do is you would connect turruamel to your data source which can be an ancestry bucket can be your internal Kafka topic whatever uh to simplify things here I'm just going to manually send data to the Twitter platform and I'm going to do that by simply taking historical data and resampling it okay this is a very common way to basically perform uh integration test and test that thing test that your system actually works so for that in the make file I have make live data and this script which is doing this sampling and pushing data uh into Turbo ML so let me clear this thing and let's go make live data okay I see transactions being pushed into the Turkmen platform I mean a lot of them I mean this is this script is so it's like a long running job so it's gonna it's gonna run for ages but the point here is that we are pushing data to the into the Tour WML platform so now what I want to see here is first is the model generating predictions and also how good they are so for that let me go to the dashboard and to check on models here's the model it's under status running super now if you check outputs here it is super so this thing is actually scoring data and then something that is very important that I said versioning so any model that is pushing to a platform needs to be updated continuously this is not a hard requirement this depends on your problem but for example for front detection that's a must so you here you need to think about how necessary that thing is for your problem how relevant how important is the online incremental training here and if it is then here's the it's the easiest way to implement it so for example this is the mall that I already pushed so here you can see the model versions you can see the first version that I push 1 hour ago and then that it's been updated and version so here you see the model version so if something is not good you can always roll back so this is the part where Turk Mel does the heavy lifting implements online retraining to your model and model virginity so that you can actually diagnose so if something does not look well in terms of monitoring so in the outputs in the metric section sorry here it is you can always fetch the the exact model given the version and then do some kind of analysis to understand okay so adding incremental retraining is something that you can do you can do step by step and you can do with safety using torque mouse so this is a very very powerful tool and finally what else I wanted to show you so here's a written it also model monitoring and model comparison yes that's also a very let's say real world problem right what we did here is that I'll show you how to create your first model push it to the platform and see that it works in terms of is it scoring data and what are the error metrics now two weeks later it might happen that you got new features you got new data that you want to use to create a second version of the model and the thing is that you don't want to break what's running in production so the safest way for you to do that is to keep the current model running and then deploy a second model and keep both models scoring in parallel for a certain amount of time so that you can compare predictions Model a so the current model you have in production versus Model B the model that you just developed and you are testing if it's better than the one you already had so with model comparison is something you can do super simple when you show the metrics you can compare models here so if you have a second model you can compare it and you can see errors one next to the other and if things look good means that you are ready to promote this model that is in the development stage you can promote it to the production stage so model comparison something very useful very day to day operations that you need you need to do well once you have a system running in production so that's it everything that I talked everything it's on the Gita repo and if you didn't understand because my English is too Spanish for you which I understand I mean my wife tells me that don't worry uh you can read it yourself and then read me okay so now I'm gonna go back to where I was now it's your turn sign up for free at turboml.com and start building your own real time machine learning system you can either improve the one that I provided make it better with feature engineering with hyper parameter tuning etc or build your own real time ML system for the problem that you care about and if you want to get more hands on content on real world no bullshit are you still learning subscribe for free the real war machine learning this letter every Saturday morning for free did I say for free again I'm so boring did I say that no okay see you soon


 > [!info]
> - **Fastest Way to Build a Real-Time ML System**
> - Use a dev container for consistent development environments (1:30)
> - Define a feature pipeline for data transformation (4:07)
> - Utilize a real-time data processing engine like Apache Flink for feature engineering (7:31)
> - Leverage a platform like TurboML to handle low-level infrastructure details (7:50)
> - Implement online retraining to continuously adjust to new patterns (11:24)
> - Use model versioning for easy rollbacks if issues arise (12:24)
> - Compare models to ensure new versions are better before deploying to production (17:11)
> 
> 